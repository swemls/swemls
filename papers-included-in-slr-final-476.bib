@inproceedings{Gaur2018753,
abstract = {Social media platforms are increasingly being used to share and seek advice on mental health issues. In particular, Reddit users freely discuss such issues on various subreddits, whose structure and content can be leveraged to formally interpret and relate subreddits and their posts in terms of mental health diagnostic categories. There is prior research on the extraction of mental health-related information, including symptoms, diagnosis, and treatments from social media; however, our approach can additionally provide actionable information to clinicians about the mental health of a patient in diagnostic terms for web-based intervention. Specifically, we provide a detailed analysis of the nature of subreddit content from domain expert's perspective and introduce a novel approach to map each subreddit to the best matching DSM-5 (Diagnostic and Statistical Manual of Mental Disorders - 5th Edition) category using multiclass classifier. Our classification algorithm analyzes all the posts of a subreddit by adapting topic modeling and word-embedding techniques, and utilizing curated medical knowledge bases to quantify relationship to DSM-5 categories. Our semantic encoding-decoding optimization approach reduces the false-alarm-rate from 30% to 2.5% over a comparable heuristic baseline, and our mapping results have been verified by domain experts achieving a kappa score of 0.84. {\textcopyright} 2018 Association for Computing Machinery.},
annote = {cited By 9; Conference of 27th ACM International Conference on Information and Knowledge Management, CIKM 2018 ; Conference Date: 22 October 2018 Through 26 October 2018; Conference Code:142310},
author = {Gaur, M and Sheth, A and Kursuncu, U and Daniulaityte, R and Pathak, J and Alambo, A and Thirunarayan, K},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3269206.3271732},
editor = {{Paton N. Candan S.}, Wang H Allan J Agrawal R Labrinidis A Cuzzocrea A Zaki M Srivastava D Broder A Schuster A},
isbn = {9781450360142},
keywords = {Decoding; Encoding (symbols); Knowledge management,Diagnosis,Drug abuse; DSM-5; Encoding and decoding; Medical},
pages = {753--762},
publisher = {Association for Computing Machinery},
title = {{"Let me tell you about your mental health!" Contextualized classification of reddit posts to DSM-5 for web-based intervention}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058009561&doi=10.1145%2F3269206.3271732&partnerID=40&md5=44a09fc083869f93b1fcc7955afb6f54},
year = {2018}
}
@article{Nakawala2019685,
abstract = {Purpose: Surgical workflow recognition and context-aware systems could allow better decision making and surgical planning by providing the focused information, which may eventually enhance surgical outcomes. While current developments in computer-assisted surgical systems are mostly focused on recognizing surgical phases, they lack recognition of surgical workflow sequence and other contextual element, e.g., “Instruments.” Our study proposes a hybrid approach, i.e., using deep learning and knowledge representation, to facilitate recognition of the surgical workflow. Methods: We implemented “Deep-Onto” network, which is an ensemble of deep learning models and knowledge management tools, ontology and production rules. As a prototypical scenario, we chose robot-assisted partial nephrectomy (RAPN). We annotated RAPN videos with surgical entities, e.g., “Step” and so forth. We performed different experiments, including the inter-subject variability, to recognize surgical steps. The corresponding subsequent steps along with other surgical contexts, i.e., “Actions,” “Phase” and “Instruments,” were also recognized. Results: The system was able to recognize 10 RAPN steps with the prevalence-weighted macro-average (PWMA) recall of 0.83, PWMA precision of 0.74, PWMA F1 score of 0.76, and the accuracy of 74.29% on 9 videos of RAPN. Conclusion: We found that the combined use of deep learning and knowledge representation techniques is a promising approach for the multi-level recognition of RAPN surgical workflow. {\textcopyright} 2018, CARS.},
annote = {cited By 10},
author = {Nakawala, H and Bianchi, R and Pescatori, L E and {De Cobelli}, O and Ferrigno, G and {De Momi}, E},
doi = {10.1007/s11548-018-1882-8},
issn = {18616410},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {Deep Learning; Humans; Kidney Neoplasms; Nephrect,article; controlled study; human; knowledge manage},
number = {4},
pages = {685--696},
publisher = {Springer Verlag},
title = {{“Deep-Onto” network for surgical workflow and context recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056702167&doi=10.1007%2Fs11548-018-1882-8&partnerID=40&md5=6561b1ca8014fbab57233808ff623b1a},
volume = {14},
year = {2019}
}
@article{8375980,
abstract = {As an important and challenging problem, knowledge representation and inference are typically carried out in a knowledge embedding framework over a multi-relational knowledge graph, and thus have a wide range of applications such as semantic retrieval and question answering. In this paper, we propose a bilinear learning framework which performs cross-entity knowledge relation analysis in the continuous vector space (derived from knowledge embedding). In the framework, we effectively model the intrinsic correlations among different types of knowledge relations within a max-margin multi-relational ranking scheme, which jointly optimizes the tasks of entity embedding and cross-entity relation prediction in terms of multi-relational structures of the knowledge graph. Specifically, we devise a bilinear scoring function that aims to evaluate the confidence degree of semantic relation prediction for entity pairs through a multi-relational learning-to-rank pipeline. In essence, the pipeline formulates the problem of relation prediction for entity pairs as that of learning relation-specific ranking functions by max-margin optimization. Experimental results demonstrate the effectiveness of the proposed framework on two common benchmark datasets.},
author = {Yu, S and Li, X and Zhao, X and Zhang, Z and Wu, F and Wang, J and Zhuang, Y and Li, X},
doi = {10.1109/TBDATA.2018.2843766},
issn = {2332-7790},
journal = {IEEE Transactions on Big Data},
keywords = {entity-relationship modelling;graph theory;learnin},
month = {dec},
number = {4},
pages = {588--600},
title = {{A Bilinear Ranking SVM for Knowledge Based Relation Prediction and Classification}},
volume = {5},
year = {2019}
}
@inproceedings{Karim201735,
abstract = {The understanding of variations in genome sequences assists us in identifying people who are predisposed to common diseases, solving rare diseases, and finding corresponding population group of the individuals from a larger population group. Although classical machine learning techniques allow the researchers to identify groups or clusters of related variables, accuracies, and effectiveness of these methods diminish for large and hyperdimensional datasets such as whole human genome. On the other hand, deep learning (DL) can make better representations of large-scale datasets to build models to learn these representations very extensively. Furthermore, Semantic Web (SW) technologies already acted as useful adaptors in life science research for large-scale data integration and querying. Thus the standardized public data created using SW plays an increasingly important role in life sciences research. In this paper, we propose a novel and scalable genomic data analysis towards population scale clustering and predicting geographic ethnicity using SW and DL-based technique. We used genotypes data from the 1000 Genome Project resulting from the whole genomes sequencing extracted from the 2504 individuals consisting of 84 million variants with 26 ethnic origins. Experimental results in terms accuracy and scalability show the effectiveness and superiority compared to the state-of-the-art. Particularly, our deep-learning-based analytics technique using classification and clustering algorithms can predict and group targeted populations with a prediction accuracy of 98% and an ARI of 0.92 respectively.},
annote = {cited By 0; Conference of 2017 Workshop on Semantic Web Solutions for Large-Scale Biomedical Data Analytics, SeWeBMeDA 2017 ; Conference Date: 28 May 2017; Conference Code:131047},
author = {Karim, R and Zappa, A and Sahay, R and Rebholz-Schuhmann, D},
booktitle = {CEUR Workshop Proceedings},
editor = {{Hasnain A. Sheth A.}, Rebholz-Schuhmann D Dumontier M},
issn = {16130073},
keywords = {Classification and clustering; Ethnicity predicti,Clustering algorithms; Data integration; Data mini,Population statistics},
pages = {35--49},
publisher = {CEUR-WS},
title = {{A deep learning approach to genomics data for population scale clustering and ethnicity prediction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032587277&partnerID=40&md5=68e4f150d2fc5ed315d58ead34900144},
volume = {1948},
year = {2017}
}
@article{Tang2020235,
abstract = {The Knowledge Graph Question Answering (KGQA) task is useful for information retrieval systems, intelligent customer service systems, etc., which has attracted the attention of a large number of researchers. Although the performance of KGQA has been further improved by introducing the Deep Learning models, there are still some difficulties to be solved, such as the representation of questions and answers, the efficient construction way of candidate path set, etc. In this paper, we propose a complete approach for KGQA task. Firstly, we devise a novel candidate path generation process, which effectively improves computation performance by reducing the number of candidate paths corresponding to a question and at the same time guarantees the accuracy of results. Secondly, considering the textual expression diversity of questions and stochastic of candidate paths, we present four models to learn semantic features of Chinese sequence with different focuses. Finally, in order to combine the advantages of each presented model, we propose a dedicated fusion policy which can get the most suitable path from the path set predicted by our presented models. We conduct experiments on Chinese Knowledge Base Question Answering (CKBQA) dataset. Experiment results show that our approach achieves better performance than the best one published in CCKS2019 competition. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 13th International Conference on Knowledge Science, Engineering and Management, KSEM 2020 ; Conference Date: 28 August 2020 Through 30 August 2020; Conference Code:243999},
author = {Tang, M and Xiong, H and Wang, L and Lin, X},
doi = {10.1007/978-3-030-55130-8_21},
editor = {{Li G. Shen H.T.}, Yuan Y Wang X Liu H Zhao X},
isbn = {9783030551292},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Computation performance; Customer service systems,Deep learning; Information retrieval systems; Know,Stochastic models},
pages = {235--246},
publisher = {Springer},
title = {{A dynamic answering path based fusion model for KGQA}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090096385&doi=10.1007%2F978-3-030-55130-8_21&partnerID=40&md5=7dbb65c0661c3b9dc2990596b7d0c77e},
volume = {12274 LNAI},
year = {2020}
}
@inproceedings{Edris20181,
abstract = {Through the success of deep convolutional neural networks (CNN) for image classification and semantic concepts detection, the multimedia retrieval community provides interesting image analysing approaches and tools in order to deliver accurate semantic interpretation. Never the less, such approaches still focusing only on explicit information and objects that exist in content. Considering that implicit information could enhance the semantic interpretation, we are interested in a knowledge based framework to detect the semantic context. In this paper, we discuss a fuzzy ontology based approach for understanding image content through the detection of the contained context. We conducted preliminary experiments on the ImageNet2017 dataset. While the obtained results still not impressive, many open research direction could be tackled. Indeed, we think that a deep based knowledge management (in particular knowledge extraction and reasoning) could be considered as interesting and promising. {\textcopyright} 2017 IEEE.},
annote = {cited By 1; Conference of 2017 Sudan Conference on Computer Science and Information Technology, SCCSIT 2017 ; Conference Date: 17 November 2017 Through 19 November 2017; Conference Code:134674},
author = {Edris, S S and Zarka, M and Ouarda, W and Alimi, A M},
booktitle = {Proceedings of: 2017 Sudan Conference on Computer Science and Information Technology, SCCSIT 2017},
doi = {10.1109/SCCSIT.2017.8293055},
editor = {{Abushama H.}, Babikir S F},
isbn = {9781538606674},
keywords = {Context classification; Deep convolutional neural,Data mining; Deep neural networks; Image recogniti,Image classification},
pages = {1--9},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A fuzzy ontology driven context classification system using large-scale image recognition based on deep CNN}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050463149&doi=10.1109%2FSCCSIT.2017.8293055&partnerID=40&md5=3f5b70cff3d7edb517fea30f0d3db473},
volume = {2017-Novem},
year = {2018}
}
@article{Han2018247,
abstract = {Due to the ubiquity of graphs, machine learning on graphs facilitates many AI systems. In order to incorporate the rich information of graphs into machine learning models, graph embedding has been developed, which seeks to preserve the graphs into low dimensional embeddings. Recently, researchers try to conduct graph embedding via generalizing neural networks on graphs. However, most existing approaches focus on node embedding, ignoring the heterogeneity of edges. Besides, the similarity relationship among random walk sequences has been rarely discussed. In this paper, we propose a generalization of Recurrent Neural Networks on Graphs (G-RNN) for graph embedding. More specifically, first we propose to utilize edge embedding and node embedding jointly to preserve graphs, which is of great significance in multi-relational graphs with heterogeneous edges. Then we propose the definition of subgraph level high-order proximity to preserve the inter-sequence proximity into the embeddings. To verify the generalization of G-RNN, we apply it to the embedding of knowledge graph, a typical multi-relational graph. Empirically we evaluate the resulting embeddings on the tasks of link prediction and node classification. The results show that the embeddings learned by G-RNN are powerful on both tasks, producing better performance than the baselines. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
annote = {cited By 4; Conference of 22nd Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2018 ; Conference Date: 3 June 2018 Through 6 June 2018; Conference Code:214589},
author = {Xiao Han and Chunhong Zhang and Chenchen Guo and Yang Ji},
doi = {10.1007/978-3-319-93037-4_20},
editor = {{Ho B. Phung D.}, Webb G I Tseng V S Ganji M Rashidi L},
isbn = {9783319930367},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining; Graphic methods; Learning systems,Graph embeddings; High-order; Knowledge graphs; L,Recurrent neural networks},
pages = {247--259},
publisher = {Springer Verlag},
title = {{A generalization of recurrent neural networks for graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049371910&doi=10.1007%2F978-3-319-93037-4_20&partnerID=40&md5=9d40f922fa2a4917431975538301f9f6},
volume = {10938 LNAI},
year = {2018}
}
@article{Wang2019,
abstract = {The paradigm of Smart product-service systems (Smart PSS) has emerged recently owing to the edge-cutting Information and Communication Technology (ICT) and artificial intelligence (AI) techniques. The unique features of Smart PSS including smartness and connectedness, value co-creation and data-driven design manner, enable the collection and analysis of large volume and heterogeneous contextual data to extract useful knowledge. Therefore, requirement elicitation, as a critical process for new solution (i.e. product-service) design, can be conducted in a rather context-aware manner, assured by those massive user-generated data and product-sensed data during the usage stage. Nevertheless, despite a few works on semantic modelling, scarcely any reports on such mechanism in today's smart, connected environment. Aiming to fill this gap, for the first time, a graph-based context-aware requirement elicitation approach considering contextual information within the Smart PSS is proposed. It leverages the pre-defined product, service, and condition ontologies together with Deepwalk technique, to formulate those concepts as nodes and their relationships as the edge of the proposed requirement graph. Implicit stakeholder requirements within a specific context can be further derived based on such interrelationships in a data-driven manner. To demonstrate its feasibility and effectiveness, an example of smart bike share system is addressed to illustrate the requirement elicitation process. It is hoped that this explorative study can offer valuable insights for the service providers who would like to extract requirements not only from the voice of customers but also from the user-generated data and product-sensed data. {\textcopyright} 2019, {\textcopyright} 2019 Informa UK Limited, trading as Taylor \& Francis Group.},
annote = {cited By 6},
author = {Wang, Z and Chen, C.-H. and Zheng, P and Li, X and Khoo, L P},
doi = {10.1080/00207543.2019.1702227},
issn = {00207543},
journal = {International Journal of Production Research},
keywords = {Context- awareness; Graph embeddings; Information,Data mining; Graphic methods; Ontology; Requiremen,Product design},
publisher = {Taylor and Francis Ltd.},
title = {{A graph-based embeddings requirement elicitation approach in smart product-service systems}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076933237&doi=10.1080%2F00207543.2019.1702227&partnerID=40&md5=232ad3d6383dabd2f4602577070146e5},
year = {2019}
}
@article{Wallaart2019363,
abstract = {This work focuses on sentence-level aspect-based sentiment analysis for restaurant reviews. A two-stage sentiment analysis algorithm is proposed. In this method, first a lexicalized domain ontology is used to predict the sentiment and as a back-up algorithm a neural network with a rotatory attention mechanism (LCR-Rot) is utilized. Furthermore, two features are added to the backup algorithm. The first extension changes the order in which the rotatory attention mechanism operates (LCR-Rot-inv). The second extension runs over the rotatory attention mechanism for multiple iterations (LCR-Rot-hop). Using the SemEval-2015 and SemEval-2016 data, we conclude that the two-stage method outperforms the baseline methods, albeit with a small percentage. Moreover, we find that the method where we iterate multiple times over a rotatory attention mechanism has the best performance. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 7; Conference of 16th International Semantic Web Conference, ESWC 2019 ; Conference Date: 2 June 2019 Through 6 June 2019; Conference Code:226559},
author = {Wallaart, O and Frasincar, F},
doi = {10.1007/978-3-030-21348-0_24},
editor = {{Janowicz K. Hammar K.}, Gray A J G Lopez V Fernandez M Zaveri A Hitzler P Haller A},
isbn = {9783030213473},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attention mechanisms; Baseline methods; Domain on,Data mining; Fading (radio); Ontology; Sentiment a,Semantic Web},
pages = {363--378},
publisher = {Springer Verlag},
title = {{A hybrid approach for aspect-based sentiment analysis using a lexicalized domain ontology and attentional neural models}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066795581&doi=10.1007%2F978-3-030-21348-0_24&partnerID=40&md5=f77af386644efee781ab881396816a57},
volume = {11503 LNCS},
year = {2019}
}
@inproceedings{Wei201923,
abstract = {Test-question/answer retrieval task has raised higher requirements in terms of accuracy, coverage and semantic understanding. We design a cascade model with two-stage training processes: The first stage uses 41,532 user test-question click records and 207,660 unclick records, which are collected from a designed test-question-answer experimental platform, to generate 200,000 pairwise training dataset to train a deep learning model, which could improve generalization ability. The second stage combines the output of the first stage with structural knowledge as new features to train a logistic regression for selecting the results from the candidates with higher accuracy, the training dataset is generated by manually annotating 20,000 test-question samples. The structural knowledge is also manually extracted from the samples for generating a small knowledge graph, and on this condition, we design knowledge features. Experimental results show that the proposed model outperforms the state-of-the-art algorithms, among which the cascading model contributes 3% improvement and the knowledge features contribute 1% improvement. {\textcopyright} Multi Conference on Computer Science and Information Systems, MCCSIS 2019. All rights reserved.},
annote = {cited By 0; Conference of 4th International Conference on Big Data Analytics, Data Mining and Computational Intelligence 2019, BigDaCI 2019 and the 8th International Conference on Theory and Practice in Modern Computing 2019, TPMC 2019 ; Conference Date: 16 July 2019 Through 18 July 2019; Conference Code:151501},
author = {Wei, Y and Li, D and Madden, A D},
booktitle = {Multi Conference on Computer Science and Information Systems, MCCSIS 2019 - Proceedings of the International Conferences on Big Data Analytics, Data Mining and Computational Intelligence 2019 and Theory and Practice in Modern Computing 2019},
doi = {10.33965/bigdaci2019_201907l003},
editor = {{Abraham A.P. Roth J.}, Rodrigues L},
isbn = {9789898533920},
keywords = {Advanced Analytics; Artificial intelligence; Big d,CASCADE model; Experimental platform; Generalizat,Data mining},
pages = {23--30},
publisher = {IADIS Press},
title = {{A knowledge based two-stage cascade model for test-question/answer retrieval}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073225978&doi=10.33965%2Fbigdaci2019_201907l003&partnerID=40&md5=e3df599aef01296a0ea9c0e32072f0c8},
year = {2019}
}
@inproceedings{9172840,
abstract = {As the basis of many knowledge graph completion tasks, the embedding representation of entities and relations in knowledge graph (KG) is an important task in the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI). While most of the existing knowledge graph embedding (KGE) models based on convolutional neural network (CNN) can obtain abundant feature embedding, they may ignore an important fact that the triples in the KG come from the text, as they simply learn about the feature embedding of entities and relations without considering contextual information. Therefore, in this paper, we propose an effective KGE model based on neural network. First of all, we convert the triple (h,r,t) of the KG into a sentence [h r t]. Then, the LSTM neural network is used to learn the long-term dependence of sentences from the input feature vectors. Then, on this basis, the two-layer convolutional neural network with several different filters is used to extract different local features. Finally, the obtained feature vectors are connected together, and the inner product is carried out with the weight vectors to obtain the score of the triple, so as to judge the validity of the given triple. We evaluate our model on two benchmark datasets FB15k-237 and WN18RR, the experimental results show that the model can effectively improve the accuracy of link prediction, achieving better results compared with other baseline models.},
author = {Li, C and Li, A and Tu, H and Wang, Y and Wang, C},
booktitle = {2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)},
doi = {10.1109/DSC50466.2020.00057},
keywords = {graph theory;learning (artificial intelligence);na},
month = {jul},
pages = {326--332},
title = {{A Knowledge Graph Embedding Method Based on Neural Network}},
year = {2020}
}
@inproceedings{Yang20181368,
abstract = {Although many researchers of recommender systems have noted that encoding user-item interactions based on DNNs promotes the performance of collaborative filtering, they ignore that embedding the latent features collected from external sources, e.g., knowledge graphs (KGs), is able to produce more precise recommendation results. Furthermore, CF-based models are still vulnerable to the scenarios of sparse known user-item interactions. In this paper, towards movie recommendation, we propose a novel knowledge-enhanced deep recommendation framework incorporating GAN-based models to acquire robust performance. Specifically, our framework first imports various feature embeddings distilled not only from user-movie interactions, but also from KGs and tags, to constitute initial user/movie representations. Then, user/movie representations are fed into a generator and a discriminator simultaneously to learn final optimal representations through adversarial training, which are conducive to generating better recommendation results. The extensive experiments on a real Douban dataset demonstrate our framework's superiority over some state-of-the-art recommendation models, especially in the scenarios of sparse observed user-movie interactions. {\textcopyright} 2018 IEEE.},
annote = {cited By 9; Conference of 18th IEEE International Conference on Data Mining, ICDM 2018 ; Conference Date: 17 November 2018 Through 20 November 2018; Conference Code:144190},
author = {Yang, D and Guo, Z and Wang, Z and Jiang, J and Xiao, Y and Wang, W},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2018.00187},
isbn = {9781538691588},
issn = {15504786},
keywords = {Collaborative filtering; Data mining; Embeddings;,Embedding; External sources; GaN based; Knowledge,Recommender systems},
pages = {1368--1373},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Knowledge-Enhanced Deep Recommendation Framework Incorporating GAN-Based Models}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061359767&doi=10.1109%2FICDM.2018.00187&partnerID=40&md5=aaf3b55b2cda671a9a199790bf25eaaa},
volume = {2018-Novem},
year = {2018}
}
@article{Sosa2020463,
abstract = {Millions of Americans are affected by rare diseases, many of which have poor survival rates. However, the small market size of individual rare diseases, combined with the time and capital requirements of pharmaceutical R&D, have hindered the development of new drugs for these cases. A promising alternative is drug repurposing, whereby existing FDA-approved drugs might be used to treat diseases different from their original indications. In order to generate drug repurposing hypotheses in a systematic and comprehensive fashion, it is essential to integrate information from across the literature of pharmacology, genetics, and pathology. To this end, we leverage a newly developed knowledge graph, the Global Network of Biomedical Relationships (GNBR). GNBR is a large, heterogeneous knowledge graph comprising drug, disease, and gene (or protein) entities linked by a small set of semantic themes derived from the abstracts of biomedical literature. We apply a knowledge graph embedding method that explicitly models the uncertainty associated with literature-derived relationships and uses link prediction to generate drug repurposing hypotheses. This approach achieves high performance on a gold-standard test set of known drug indications (AUROC = 0.89) and is capable of generating novel repurposing hypotheses, which we independently validate using external literature sources and protein interaction networks. Finally, we demonstrate the ability of our model to produce explanations of its predictions. {\textcopyright} 2019 The Authors.},
annote = {cited By 1; Conference of 25th Pacific Symposium on Biocomputing, PSB 2020 ; Conference Date: 3 January 2020 Through 7 January 2020; Conference Code:160215},
author = {Sosa, D N and Derry, A and Guo, M and Wei, E and Brinton, C and Altman, R B},
issn = {23356928},
journal = {Pacific Symposium on Biocomputing},
keywords = {Biomedical literature; Capital requirements; Glob,Diseases,Embeddings; Knowledge management; Proteins; Semant},
number = {2020},
pages = {463--474},
publisher = {World Scientific Publishing Co. Pte Ltd},
title = {{A literature-based knowledge graph embedding method for identifying drug repurposing opportunities in rare diseases}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076151599&partnerID=40&md5=1a864e0c3cfec0bd0379000ac0ad5fd8},
volume = {25},
year = {2020}
}
@article{Lima2019142,
abstract = {Relation Extraction (RE), the task of detecting and characterizing semantic relations between entities in text, has gained much importance in the last two decades, mainly in the biomedical domain. Many papers have been published on Relation Extraction using supervised machine learning techniques. Most of these techniques rely on statistical methods, such as feature-based and tree-kernels-based methods. Such statistical learning techniques are usually based on a propositional hypothesis space for representing examples, i.e., they employ an attribute–value representation of features. This kind of representation has some drawbacks, particularly in the extraction of complex relations which demand more contextual information about the involving instances, i.e., it is not able to effectively capture structural information from parse trees without loss of information. In this work, we present OntoILPER, a logic-based relational learning approach to Relation Extraction that uses Inductive Logic Programming for generating extraction models in the form of symbolic extraction rules. OntoILPER takes profit of a rich relational representation of examples, which can alleviate the aforementioned drawbacks. The proposed relational approach seems to be more suitable for Relation Extraction than statistical ones for several reasons that we argue. Moreover, OntoILPER uses a domain ontology that guides the background knowledge generation process and is used for storing the extracted relation instances. The induced extraction rules were evaluated on three protein–protein interaction datasets from the biomedical domain. The performance of OntoILPER extraction models was compared with other state-of-the-art RE systems. The encouraging results seem to demonstrate the effectiveness of the proposed solution. {\textcopyright} 2018 Elsevier Ltd},
annote = {cited By 5},
author = {Lima, R and Espinasse, B and Freitas, F},
doi = {10.1016/j.engappai.2018.11.001},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Computer circuits; Forestry; Inductive logic progr,Contextual information; Relation extraction; Rela,Data mining},
pages = {142--157},
publisher = {Elsevier Ltd},
title = {{A logic-based relational learning approach to relation extraction: The OntoILPER system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057496358&doi=10.1016%2Fj.engappai.2018.11.001&partnerID=40&md5=46ddd00d613ed9d46426d943543fafd9},
volume = {78},
year = {2019}
}
@article{ISI:000546399900011,
abstract = {Background: Knowledge graph embedding is an effective semantic
representation method for entities and relations in knowledge graphs.
Several translation-based algorithms, including TransE, TransH, TransR,
TransD, and TranSparse, have been proposed to learn effective embedding
vectors from typical knowledge graphs in which the relations between
head and tail entities are deterministic. However, in medical knowledge
graphs, the relations between head and tail entities are inherently
probabilistic. This difference introduces a challenge in embedding
medical knowledge graphs.
Objective: We aimed to address the challenge of how to learn the
probability values of triplets into representation vectors by making
enhancements to existing TransX (where Xis E, H, R, D, or Sparse)
algorithms, including the following: (1) constructing a mapping function
between the score value and the probability, and (2) introducing
probability-based loss of triplets into the original margin-based loss
function.
Methods: We performed the proposed PrTransX algorithm on a medical
knowledge graph that we built from large-scale real-world electronic
medical records data. We evaluated the embeddings using link prediction
task.
Results: Compared with the corresponding TransX algorithms, the proposed
PrTransX performed better than the TransX model in all evaluation
indicators, achieving a higher proportion of corrected entities ranked
in the top 10 and normalized discounted cumulative gain of the top 10
predicted tail entities, and lower mean rank.
Conclusions: The proposed PrTransX successfully incorporated the
uncertainty of the knowledge triplets into the embedding vectors.},
address = {130 QUEENS QUAY E, STE 1102, TORONTO, ON M5A 0P6, CANADA},
author = {Li, Linfeng and Wang, Peng and Wang, Yao and Wang, Shenghui and Yan, Jun and Jiang, Jinpeng and Tang, Buzhou and Wang, Chengliang and Liu, Yuting},
doi = {10.2196/17645},
journal = {JMIR MEDICAL INFORMATICS},
keywords = {probabilistic medical knowledge graph; representat},
month = {may},
number = {5},
publisher = {JMIR PUBLICATIONS, INC},
title = {{A Method to Learn Embedding of a Probabilistic Medical Knowledge Graph: Algorithm Development}},
type = {Article},
volume = {8},
year = {2020}
}
@inproceedings{Kong20192929,
abstract = {Knowledge graphs such as DBPedia and Freebase contain sparse linkage connectivity, which poses severe challenge to link prediction between entities. In addressing this sparsity problem, our studies indicate that one needs to leverage model with low complexity to avoid overfitting the weak structural information in the graphs, requiring the simple models which can efficiently encode the entities and their description information and then effectively decode their relationships. In this paper, we present a simple and efficient model that can attain these two goals. Specifically, we use a bag-of-words model, where relevant words are aggregated using average pooling or a basic Graph Convolutional Network to encode entities into distributed embeddings. A factorization machine is then used to score the relationships between those embeddings to generate linkage predictions. Empirical studies on two real datasets confirms the efficiency of our proposed model and shows superior predictive performance over state-of-the-art approaches. {\textcopyright} 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.},
annote = {cited By 1; Conference of 2019 World Wide Web Conference, WWW 2019 ; Conference Date: 13 May 2019 Through 17 May 2019; Conference Code:147966},
author = {Kong, F and Mensah, S and Zhang, R and Hu, Z and Guo, H and Mao, Y},
booktitle = {The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019},
doi = {10.1145/3308558.3313550},
isbn = {9781450366748},
keywords = {Convolutional networks; Description information;,Embeddings; Encoding (symbols); Forecasting; Knowl,Information retrieval},
pages = {2929--2935},
publisher = {Association for Computing Machinery, Inc},
title = {{A neural bag-of-words modelling framework for link prediction in knowledge bases with sparse connectivity}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066882448&doi=10.1145%2F3308558.3313550&partnerID=40&md5=f2cd8437870ff64dcce8f0adae82b0f2},
year = {2019}
}
@inproceedings{6047848,
abstract = {The paper describes an unsupervised neural model for classifying the methods of Web APIs into a large number of classes specified by a domain ontology. As a result of the classification, each method of a Web service is associated to one ontology concept, the name of the concept being further used to semantically annotate the method. The ontology concepts define some functionalities to be offered by different API methods. The names of these concepts are linguistically denoted by verbs or verb phrases that define the action performed by a method. The framework is based on a model of hierarchical self-organizing maps. The methods of the web APIs are encoded in a bag-of-words style, by counting the words that occur in their javadoc documentation. We experimented this automatic semantic annotation model with a data set consisting of APIs of RDF storage tools. The ontology and the APIs to be classified in our experiments are collected from this dataset.},
author = {Chifu, E S and Letia, I A},
booktitle = {2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing},
doi = {10.1109/ICCP.2011.6047848},
keywords = {application program interfaces;data mining;Java;on},
month = {aug},
pages = {87--94},
title = {{A neural model for semantically enhancing Web APIs}},
year = {2011}
}
@article{Bin202014951,
abstract = {In attraction recommendation scenarios, how to model multifaceted tourism contexts so as to accurately learn tourist preferences and attraction tourism features is a keystone of generating personalized recommendations. However, most of existing works generally focused on modeling spatiotemporal contexts of historical travel trajectories to learn tourists' preferences, while neglected rich heterogeneous tourism side information, i.e., personal tourism constraints of tourists and tourism attributes of attractions. To this end, we propose a Neural Multi-context Modeling Framework (NMMF) to learn tourism feature representations of tourists and attractions by modeling multiple tourism contexts. Initially, we leverage a travel knowledge graph and massive original travelogues to construct the tourism attribute context of attractions and the travel trajectory context of tourists. Then, we design two context embedding models, named TKG2vec and Traj2vec, to model two kinds of context respectively. Both models learn feature vectors of tourist and attraction in contexts by elaborating neural networks to project each tourist and attraction into a uniform latent feature space. Finally, our framework integrates feature vectors derived from two models to acquire complete feature representations of tourists and attractions, and recommends personalized attractions by calculating the similarity between tourist and candidate attractions in the latent space. Experimental results on a real-world tourism dataset demonstrate our framework outperforms state-of-the-art methods in two personalized attraction recommendation tasks. {\textcopyright} 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
annote = {cited By 0},
author = {Bin, C and Gu, T and Jia, Z and Zhu, G and Xiao, C},
doi = {10.1007/s11042-019-08554-5},
issn = {13807501},
journal = {Multimedia Tools and Applications},
keywords = {Attraction recommendation; Context modeling; Feat,Knowledge management; Trajectories,Vector spaces},
number = {21-22},
pages = {14951--14979},
publisher = {Springer},
title = {{A neural multi-context modeling framework for personalized attraction recommendation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077619307&doi=10.1007%2Fs11042-019-08554-5&partnerID=40&md5=f20b081bc2f081d82d27373126884ce4},
volume = {79},
year = {2020}
}
@inproceedings{8545570,
abstract = {Modeling knowledge graph completion by encoding each entity and relation into a continuous tensor space becomes very hot. Meanwhile, many models including TransE, TransH, TransR, CTransR, TransD, TranSpare, TransDR, STransE, DT, FT and OrbitE are proposed for knowledge graph completion. However, all these previous works take less attention to the asymmetrical and the imbalance of many relations (some relations link a subject and many objects, and other relations link many subjects and many objects). Therefore, this paper proposes a novel asymmetrical embedding model(AEM) for knowledge graph completion. Because of the different properties of the head and tail entities in the triplets of the same relationship, every head entity vector and every tail entity vector are weighted by the corresponding head relation vector and the corresponding tail relation vector, respectively. And then new entity vector representations are obtained and the new entity vectors in the same triple are similar. Because the AEM weights each dimension of the entity vectors, it can accurately represent the latent attributes of entities and relationships. Moreover, the number of parameters of the AEM is so small that it is easier to train. Finally, compared with previous embedding models, the AEM obtains a better link prediction performance through two benchmark datasets FB15K and WN18.},
author = {Geng, Z and Li, Z and Han, Y},
booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8545570},
issn = {1051-4651},
keywords = {data handling;graph theory;inference mechanisms;le},
month = {aug},
pages = {290--295},
title = {{A Novel Asymmetric Embedding Model for Knowledge Graph Completion}},
year = {2018}
}
@article{Lei2020534,
abstract = {As Noncommunicable Diseases (NCDs) are affected or controlled by diverse factors such as age, regionalism, timeliness or seasonality, they are always challenging to be treated accurately, which has impacted on daily life and work of patients. Unfortunately, although a number of researchers have already made some achievements (including clinical or even computer-based) on certain diseases, current situation is eager to be improved via computer technologies such as data mining and Deep Learning. In addition, the progress of NCD research has been hampered by privacy of health and medical data. In this paper, a hierarchical idea has been proposed to study the effects of various factors on diseases, and a data-driven framework named d-DC with good extensibility is presented. d-DC is able to classify the disease according to the occupation on the premise where the disease is occurring in a certain region. During collecting data, we used a combination of personal or family medical records and traditional methods to build a data acquisition model. Not only can it realize automatic collection and replenishment of data, but it can also effectively tackle the cold start problem of the model with relatively few data effectively. The diversity of information gathering includes structured data and unstructured data (such as plain texts, images or videos), which contributes to improve the classification accuracy and new knowledge acquisition. Apart from adopting machine learning methods, d-DC has employed knowledge graph (KG) to classify diseases for the first time. The vectorization of medical texts by using knowledge embedding is a novel consideration in the classification of diseases. When results are singular, the medical expert system was proposed to address inconsistencies through knowledge bases or online experts. The results of d-DC are displayed by using a combination of KG and traditional methods, which intuitively provides a reasonable interpretation to the results (highly descriptive). Experiments show that d-DC achieved the improved accuracy than the other previous methods. Especially, a fusion method called RKRE based on both ResNet and the expert system attained an average correct proportion of 86.95%, which is a good feasibility study in the field of disease classification. {\textcopyright} 2019 Elsevier B.V.},
annote = {cited By 1},
author = {Lei, Z and Sun, Y and Nanehkaran, Y A and Yang, S and Islam, M S and Lei, H and Zhang, D},
doi = {10.1016/j.future.2019.08.030},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Classification (of information),Classification accuracy; Disease classification;,Data acquisition; Data fusion; Data mining; Deep l},
pages = {534--548},
publisher = {Elsevier B.V.},
title = {{A novel data-driven robust framework based on machine learning and knowledge graph for disease classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072215486&doi=10.1016%2Fj.future.2019.08.030&partnerID=40&md5=ece42282bdd032dcf61184e187efef15},
volume = {102},
year = {2020}
}
@article{Ghoniem2019,
abstract = {Ontologies are used to model knowledge in several domains of interest, such as the biomedical domain. Conceptualization is the basic task for ontology building. Concepts are identified, and then they are linked through their semantic relationships. Recently, ontologies have constituted a crucial part of modern semantic webs because they can convert a web of documents into a web of things. Although ontology learning generally occupies a large space in computer science, Arabic ontology learning, in particular, is underdeveloped due to the Arabic language's nature as well as the profundity required in this domain. The previously published research on Arabic ontology learning from text falls into three categories: developing manually hand-crafted rules, using ordinary supervised/unsupervised machine learning algorithms, or a hybrid of these two approaches. The model proposed in this work contributes to Arabic ontology learning in two ways. First, a text mining algorithm is proposed for extracting concepts and their semantic relations from text documents. The algorithm calculates the concept frequency weights using the term frequency weights. Then, it calculates the weights of concept similarity using the information of the ontology structure, involving (1) the concept's path distance, (2) the concept's distribution layer, and (3) the mutual parent concept's distribution layer. Then, feature mapping is performed by assigning the concepts' similarities to the concept features. Second, a hybrid genetic-whale optimization algorithm was proposed to optimize ontology learning from Arabic text. The operator of the G-WOA is a hybrid operator integrating GA's mutation, crossover, and selection processes with the WOA's processes (encircling prey, attacking of bubble-net, and searching for prey) to fulfill the balance between both exploitation and exploration, and to find the solutions that exhibit the highest fitness. For evaluating the performance of the ontology learning approach, extensive comparisons are conducted using different Arabic corpora and bio-inspired optimization algorithms. Furthermore, two publicly available non-Arabic corpora are used to compare the efficiency of the proposed approach with those of other languages. The results reveal that the proposed genetic-whale optimization algorithm outperforms the other compared algorithms across all the Arabic corpora in terms of precision, recall, and F-score measures. Moreover, the proposed approach outperforms the state-of-the-art methods of ontology learning from Arabic and non-Arabic texts in terms of these three measures. {\textcopyright} 2019 by the authors.},
annote = {cited By 1},
author = {Ghoniem, R M and Alhelwa, N and Shaalan, K},
doi = {10.3390/a12090182},
issn = {19994893},
journal = {Algorithms},
keywords = {Bio-inspired optimizations; Exploitation and expl,Biomimetics; Data mining; Genetic algorithms; Mach,Learning algorithms},
number = {9},
publisher = {MDPI AG},
title = {{A novel hybrid genetic-whale optimization model for ontology learning from Arabic text}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072769146&doi=10.3390%2Fa12090182&partnerID=40&md5=97cd6b695f6ce79428d50f314b7d4c65},
volume = {12},
year = {2019}
}
@inproceedings{6974831,
abstract = {Biomedical knowledge stored in the web is increasing significantly as most of the biomedical research papers are published online. Biomedical entity extraction is a crucial procedure for efficient text analysis and retrieval. PubMed is a very popular indexing engine, concerning life sciences and biomedical research. Being a free database, it accesses primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. In this work, we propose a metasearch engine over PubMed, which classifies PubMed results according to their specific topic and the extracted Biomedical entities. This method helps researchers to browse and search in the retrieved results. In order to provide more accurate clustering results, we utilize the biomedical ontology, named MeSH as well as RxNorm which is a tool for supporting semantic interoperation between drug terminologies and pharmacy knowledge base systems. Finally, we embed the proposed methodology in an online system.},
author = {Kanavos, A and Theodoridis, E and Tsakalidis, A},
booktitle = {2014 25th International Workshop on Database and Expert Systems Applications},
doi = {10.1109/DEXA.2014.32},
issn = {2378-3915},
keywords = {data mining;information retrieval;medical computin},
pages = {82--86},
title = {{A PubMed Meta Search Engine Based on Biomedical Entity Mining}},
year = {2014}
}
@article{9055229,
abstract = {Topic Modelling has been successfully applied in many text mining applications such as natural language processing, information retrieval, information filtering, etc. In information filtering systems (IFs), user interest representation is the core part which determines the success of the system. Topics in a topic model generated from a user's documents can be used to represent the user's information interest. However, the quality of a topic model generated from a document collection is not always accurate because the topics of the topic model might contain meaningless or ambiguous words. This ambiguity problem can affect the performance of IFs which use a topic model to represent user information interest. Hence, a topic evaluation method to assess the quality of topics in a topic model is important for ensuring the effectiveness of utilizing the topic model in text mining applications. One method in measuring the quality of a topic model is to match the topical words of the model to concepts in an ontology. However, a limitation of this method is that some topical words in an examined topic cannot be found in the mapping ontology. In this study, we propose a new model to evaluate the quality of topics by matching concepts in an ontology. In particular, word embedding technique is applied to dealing with the ambiguity problem by finding similar concept words based on word embeddings. The assessed topics are then used in an information filtering system for filtering relevant documents for a user. The proposed model was evaluated against some state-of-the-art baseline models in terms of term-based, phrase-based, and topic-based user interest representations, and also some topic evaluation models. The result of the evaluation shows that the new proposed model outperforms the state-of-the-art baseline models.},
author = {Xu, Y and Nguyen, H and Li, Y},
doi = {10.1109/ACCESS.2020.2985079},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {data mining;information filtering;ontologies (arti},
pages = {66977--66988},
title = {{A Semantic Based Approach for Topic Evaluation in Information Filtering}},
volume = {8},
year = {2020}
}
@inproceedings{9170315,
abstract = {Large-scale knowledge graphs are structured to represent real world facts, but they are far from completeness. A number of completion methods have been developed to fill missing facts. In this paper, a novel Sentence-RCNN embedding model is proposed for knowledge graph completion. This model represents knowledge facts as sentences, so that it can precisely capture long-term dependencies, local structure information and translational features simultaneously. In addition, we propose a new method to construct negative samples (CNS), which greatly reduces the number of false negative samples used in model training stage. The proposed model was validated by two challenging benchmark datasets without any extra information. Results showed Sentence-RCNN model had fairly good accuracy, robustness and convergence than the state-of-the-art embedding models. It improved MRR and H{\textcopyright}N metrics by an average of over 11.8%, and obtained extra 5.4% improvement with CNS method.},
author = {Ma, M and Teng, F and Zhong, W and MA, Z},
booktitle = {2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)},
doi = {10.1109/ISKE47853.2019.9170315},
keywords = {data mining;feature extraction;graph theory;learni},
month = {nov},
pages = {484--490},
title = {{A Sentence-RCNN embedding model for Knowledge Graph Completion}},
year = {2019}
}
@article{Gillies20131044,
abstract = {Gene expression profile classification is a pivotal research domain assisting in the transformation from traditional to personalized medicine. A major challenge associated with gene expression data classification is the small number of samples relative to the large number of genes. To address this problem, researchers have devised various feature selection algorithms to reduce the number of genes. Recent studies have been experimenting with the use of semantic similarity between genes in Gene Ontology (GO) as a method to improve feature selection. While there are few studies that discuss how to use GO for feature selection, there is no simulation study that addresses when to use GO-based feature selection. To investigate this, we developed a novel simulation, which generates binary class datasets, where the differentially expressed genes between two classes have some underlying relationship in GO. This allows us to investigate the effects of various factors such as the relative connectedness of the underlying genes in GO, the mean magnitude of separation between differentially expressed genes denoted by $\delta$, and the number of training samples. Our simulation results suggest that the connectedness in GO of the differentially expressed genes for a biological condition is the primary factor for determining the efficacy of GO-based feature selection. In particular, as the connectedness of differentially expressed genes increases, the classification accuracy improvement increases. To quantify this notion of connectedness, we defined a measure called Biological Condition Annotation Level BCAL( G), where G is a graph of differentially expressed genes. Our main conclusions with respect to GO-based feature selection are the following: (1) it increases classification accuracy when BCAL( G) ≥ 0.696; (2) it decreases classification accuracy when BCAL( G) ≤ 0.389; (3) it provides marginal accuracy improvement when 0.389 < BCAL(G) < 0.696 and $\delta$< 1; (4) as the number of genes in a biological condition increases beyond 50 and $\delta$≥ 0.7, the improvement from GO-based feature selection decreases; and (5) we recommend not using GO-based feature selection when a biological condition has less than ten genes. Our results are derived from datasets preprocessed using RMA (Robust Multi-array Average), cases where $\delta$ is between 0.3 and 2.5, and training sample sizes between 20 and 200, therefore our conclusions are limited to these specifications. Overall, this simulation is innovative and addresses the question of when SoFoCles-style feature selection should be used for classification instead of statistical-based ranking measures. {\textcopyright} 2013 Elsevier Inc.},
annote = {cited By 9},
author = {Gillies, C E and Siadat, M.-R. and Patel, N V and Wilson, G D},
doi = {10.1016/j.jbi.2013.07.008},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Algorithms; Gene Expression Profiling; Humans; In,Cancer classification; Data mining; Feature evalu,Cancer classification; Differentially expressed ge,Data mining; Sampling,Gene expression,accuracy; area under the curve; article; biologic},
number = {6},
pages = {1044--1059},
title = {{A simulation to analyze feature selection methods utilizing gene ontology for gene expression classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888206472&doi=10.1016%2Fj.jbi.2013.07.008&partnerID=40&md5=f316d2e190ad733f023e4b43eee733c8},
volume = {46},
year = {2013}
}
@inproceedings{Piao20182,
abstract = {The recent development of deep learning approaches provides a convenient way to learn entity embeddings from different aspects such as texts and a homogeneous or heterogeneous graph encoded in a knowledge base such as DBpedia. However, it is unclear to what extent domain-specific entity embeddings learned from different aspects of a knowledge base reflect their similarities, and the potential of leveraging those similarities for item recommendations in a specific domain has not been explored. In this work, we investigate domain-specific entity embeddings learned from different aspects of DBpedia with state-of-the-art embedding approaches, and the recommendation performance based on the similarities of these embeddings. The experimental results on two real-word datasets show that recommender systems based on the similarities of entity embeddings learned from a homogeneous graph via the dbo:wikiPageWikiLink property provides the best performance compared to the ones learned from other aspects. {\textcopyright} 2018, Springer Nature Switzerland AG.},
annote = {From Duplicate 1 (A study of the similarities of entity embeddings learned from different aspects of a knowledge base for item recommendations - Piao, G; Breslin, J G)

cited By 2; Conference of 15th Extended Semantic Web Conference, ESWC 2018 ; Conference Date: 3 June 2018 Through 7 June 2018; Conference Code:216719

From Duplicate 2 (A study of the similarities of entity embeddings learned from different aspects of a knowledge base for item recommendations - Piao, G; Breslin, J G)

cited By 0; Conference of 1st Workshop on Deep Learning for Knowledge Graphs and Semantic Technologies, DL4KGS 2018 ; Conference Date: 4 June 2018; Conference Code:136761},
author = {Piao, G and Breslin, J G},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-98192-5_52},
editor = {{Gangemi A. Gentile A.L.}, Paulheim H Maleshkova M Rudolph S Pan J Z Alam M Nuzzolese A G},
isbn = {9783319981918},
issn = {16130073},
keywords = {Deep learning,Embeddings,Heterogeneous graph,Knowledge base,Knowledge based systems,Knowledge graphs,Knowledge management,Learning approach,Natural language processing systems,Recommendation performance,Recommender systems,Semantic Web,Semantic similarity,State of the art},
pages = {345--359},
publisher = {Springer Verlag},
title = {{A study of the similarities of entity embeddings learned from different aspects of a knowledge base for item recommendations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051671762&doi=10.1007%2F978-3-319-98192-5_52&partnerID=40&md5=fa0dd5e4fb496328c8368796f02ee781 https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048317616&partnerID=40&md5=59bb34c655c8a91a593d5fcbe},
volume = {2106},
year = {2018}
}
@article{8552355,
abstract = {The distributed representation of knowledge graphs (KGs), which embeds the structured graphs into low-dimensional embedding spaces, is widely used to facilitate various applications of AI, such as information retrieval and question answering. The primary elements of KGs, the entities viewed as nodes and the relations regarded as links between entities, naturally make up the local embedding context for each other, which is called the multi-restriction property of KGs. However, this property is not fully explored by previous models, where either only part of the multi-restriction is captured, or the capability of the embedding model is limited to a fixed function without clear interpretation. To address this issue, we propose TBNN, a triple-branch neural network to learn the embeddings of KGs. In particular, the embedding of any element of a KG is determined by its multi-restriction via an interaction layer followed by parallel branched layers. Thus, the entities and relations can be treated equivalently in spite of their seeming differences in the original KG. We define the loss function of TBNN based on the confidence score of the three elements of each triple. In addition, we propose using the log-sum-exp pairwise loss to smooth the hinge loss, which results in better performance. Empirically, we evaluate our model on the tasks of link prediction and triple classification with the subsets of WordNet and Freebase. Experiment results show that our model performs better than the baselines, especially providing stable performance for relations with different mapping properties.},
author = {Han, X and Zhang, C and Sun, T and Ji, Y and Hu, Z},
doi = {10.1109/ACCESS.2018.2884012},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {graph theory;knowledge representation;learning (ar},
pages = {76606--76615},
title = {{A Triple-Branch Neural Network for Knowledge Graph Embedding}},
volume = {6},
year = {2018}
}
@article{Indhumathy201873,
abstract = {Background: Hepatitis C Virus causes the most severe form of chronic liver disease and nearly 200 million people worldwide are estimated to be infected with this virus. Much about the HCV pathogenesis process is still unknown. The study of interactions between HCV and human proteins will lead to deeper understanding of HCV mechanism. Objective: The objective of this paper is to predict potentially new HCV-Human protein interactions using a weighted association rule mining technique. Methods: A new computational method was developed for mining associations within a bipartite graph that was constructed from the HCV-human protein interactions dataset. A new mathematical model was applied to weigh the discovered association rules based on Gene Ontology annotations of viral and human proteins. HCVpro database was used to generate a human-viral bipartite graph that was then analyzed computationally to extract biclusters within the graph. Association rules were extracted from the bipartite graph and weighted using a mathematical model that incorporated information about proteins available from Gene Ontology knowledge base. Results: Forty two new interactions between HCV and human proteins were predicted. Some of these predicted interactions were validated through literature survey and enrichment studies such as Gene ontology-based analysis, pathway- based analysis and disease association based analysis. Conclusion: The methodology developed in this paper can also be used for various other kind of data analysis and hence it carries a wide scope. This will be useful to conduct similar kind of experiments for other disease databases. {\textcopyright} 2018 Bentham Science Publishers.},
annote = {cited By 3},
author = {Indhumathy, M and Nabhan, A R and Arumugam, S},
doi = {10.2174/1574893611666161123142425},
issn = {15748936},
journal = {Current Bioinformatics},
keywords = {Article; data mining; gene ontology; Hepatitis C,nonstructural protein 2; nonstructural protein 3;},
number = {1},
pages = {73--84},
publisher = {Bentham Science Publishers B.V.},
title = {{A weighted association rule mining method for predicting HCV-human protein interactions}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046945244&doi=10.2174%2F1574893611666161123142425&partnerID=40&md5=69ebd6bb4145fe18542e7adfacf2a373},
volume = {13},
year = {2018}
}
@article{noauthor_ann_2020,
abstract = {Identifying protein complexes is helpful for understanding cellular functions and designing drugs. In the last decades, many computational methods have been proposed based on detecting dense subgraphs or subnetworks in Protein-Protein Interaction Networks (PINs). However, the high rate of false positive/negative interactions in PINs prevents from the achievement of satisfactory detection results directly from PINs, because most of such existing methods exploit mainly topological information to do network partitioning. In this paper, we propose a new approach for protein complex detection by merging topological information of PINs and functional information of proteins. We first split proteins to a number of protein groups from the perspective of protein functions by using FunCat data. Then, for each of the resulting protein groups, we calculate two protein-protein similarity matrices: one is computed by using graph embedding over a PIN, the other is by using GO terms, and combine these two matrices to get an integrated similarity matrix. Following that, we cluster the proteins in each group based on the corresponding integrated similarity matrix, and obtain a number of small protein clusters. We map these clusters of proteins onto the PIN, and get a number of connected subgraphs. After a round of merging of overlapping subgraphs, finally we get the detected complexes. We conduct empirical evaluation on four PPI datasets (Collins, Gavin, Krogan, and Wiphi) with two complex benchmarks (CYC2008 and MIPS). Experimental results show that our method performs better than the state-of-the-art methods.},
author = {Yao, Heng and Shi, Yunjia and Guan, Jihong and Zhou, Shuigeng},
doi = {10.1109/TCBB.2019.2897769},
issn = {15579964},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
keywords = {Protein-protein interaction network (PIN),graph embedding,protein complex,similarity matrix},
number = {3},
pages = {777--787},
pmid = {30736004},
title = {{Accurately Detecting Protein Complexes by Graph Embedding and Combining Functions with Interactions}},
volume = {17},
year = {2020}
}
@article{Wan2020471,
abstract = {Knowledge graph (KG) embedding approaches are widely used to infer underlying missing facts based on intrinsic structure information. However, the presence of noisy facts in automatically extracted or crowdsourcing KGs significantly reduces the reliability of various embedding learners. In this paper, we thoroughly study the underlying reasons for the performance drop in dealing with noisy knowledge graphs, and we propose an ensemble framework, Adaptive Knowledge Subgraph Ensemble (AKSE), to enhance the robustness and trust of knowledge graph completion. By employing an effective knowledge subgraph extraction approach to re-sample the sub-components from the original knowledge graph, AKSE generates different representations for learning diversified base learners (e.g., TransE and DistMult), which substantially alleviates the noise effect of KG embedding. All embedding learners are integrated into a unified framework to reduce generalization errors via our simple or adaptive weighting schemes, where the weight is allocated based on each individual learner's prediction capacity. Experimental results show that the robustness of our ensemble framework outperforms exiting knowledge graph embedding approaches on manually injected noise as well as inherent noisy extracted KGs. {\textcopyright} 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
annote = {cited By 0},
author = {Wan, G and Du, B and Pan, S and Wu, J},
doi = {10.1007/s11280-019-00711-y},
issn = {1386145X},
journal = {World Wide Web},
keywords = {Adaptive weighting; Generalization Error; Intrins,Data mining,Embeddings},
number = {1},
pages = {471--490},
publisher = {Springer},
title = {{Adaptive knowledge subgraph ensemble for robust and trustworthy knowledge graph completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074459402&doi=10.1007%2Fs11280-019-00711-y&partnerID=40&md5=5e31fd9eca2f07b543b1234739dbf27e},
volume = {23},
year = {2020}
}
@inproceedings{Meškele20192489,
abstract = {Sentences containing several different polarity aspects cause one of the main problems in sentiment analysis. Depending on an aspect, the same context words can have different effects on its sentiment value. Additionally, the polarity can be influenced by the domain-specific knowledge, showing the necessity to incorporate it into the sentiment classification. In this paper we present a hybrid solution for sentence-level aspect-based sentiment analysis using A Lexicalised Domain Ontology and Neural Attention (ALDONA) model to handle the problems mentioned above. To measure the influence of each word in a given sentence on an aspect's polarity, we introduce the bidirectional context attention mechanism. Moreover, the classification module is designed to handle the sentence's complex structure. Finally, the manually created lexicalised domain ontology (represented in OWL) is integrated to exploit the field-specific knowledge. Computational results obtained on a benchmark data set based on Web reviews have shown ALDONA's ability to outperform several state-of-the-art models and stress its contribution to aspect-based sentiment classification. {\textcopyright} 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
annote = {cited By 5; Conference of 34th Annual ACM Symposium on Applied Computing, SAC 2019 ; Conference Date: 8 April 2019 Through 12 April 2019; Conference Code:147772},
author = {Me{\v{s}}kele, D and Frasincar, F},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3297280.3297525},
isbn = {9781450359337},
keywords = {Attention mechanisms; Bi-directional contexts; Co,Classification (of information); Data mining; Sent,Ontology},
pages = {2489--2496},
publisher = {Association for Computing Machinery},
title = {{ALDONA: A hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalised domain ontology and a neural attention model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065643259&doi=10.1145%2F3297280.3297525&partnerID=40&md5=88eb7b4e8e24a6dcf952d9503dd42e69},
volume = {Part F1477},
year = {2019}
}
@article{Wang2020342,
abstract = {Knowledge Graphs (KGs) have been applied to various application scenarios including Web searching, Q\&A, recommendation system, natural language processing and so on. However, the vast majority of Knowledge Bases (KBs) are incomplete, necessitating a demand for KB completion (KBC). Methods of KBC used in the mainstream current knowledge base include the latent factor model, the random walk model and recent popular methods based on reinforcement learning, which performs well in their respective areas of expertise. Recurrent neural network (RNN) and its variants model temporal data by remembering information for long periods, however, whether they also have the ability to use the information they have already remembered to achieve complex reasoning in the knowledge graph. In this paper, we produce a novel framework (ALSTM) based on the Attention mechanism and Long Short-Term Memory (LSTM), which associates structure learning with parameter learning of first-order logical rules in an end-to-end differentiable neural networks model. In this framework, we designed a memory system and employed a multi-head dot product attention (MHDPA) to interact and update the memories embedded in the memory system for reasoning purposes. This is also consistent with the process of human cognition and reasoning, looking for enlightenment for the future in historical memory. In addition, we explored the use of inductive bias in deep learning to facilitate learning of entities, relations, and rules. Experiments establish the efficiency and effectiveness of our model and show that our method achieves better performance in tasks which include fact prediction and link prediction than baseline models on several benchmark datasets such as WN18RR, FB15K-237 and NELL-995. {\textcopyright} 2020 Elsevier Ltd},
annote = {cited By 2},
author = {Wang, Q and Hao, Y},
doi = {10.1016/j.neucom.2020.02.065},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Attention; Knowledge base; Knowledge basis (KBs);,Benchmarking; Brain; Data storage equipment; Deep,Long short-term memory,article; attention; deep learning; human; human e},
pages = {342--351},
publisher = {Elsevier B.V.},
title = {{ALSTM: An attention-based long short-term memory framework for knowledge base reasoning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081931785&doi=10.1016%2Fj.neucom.2020.02.065&partnerID=40&md5=1cbeb4e9a1aa7b61a3f9f20e5f3cd26e},
volume = {399},
year = {2020}
}
@article{Chung20201387,
abstract = {Context computing is a branch of ambient intelligence (AmI) research, which has been rapidly emerging in the support of intelligent smart health platform solution. To develop reliable ambient computing using the hybrid peer-to-peer network and Internet of Things, machine learning, deep learning, artificial intelligence, and context awareness have been applied. This study proposes an ambient context-based modeling for a health risk assessment using deep neural network. In the proposed method, we collected medical information from chronic disease patients such as EMR, PHR, and medical histories, as well as environmental data from a health platform. Subsequently, heterogeneous data are integrated through selecting, cleaning, modeling, and evaluating the collected raw data and then the context is created. The structured input data such as a sensor data are normalized by transforming the time domain data to the frequency domain information. Using a deep neural network, the normalized data are applied to create an ambient context. A deep neural network is composed of the following three layers: input layers with treated and untreated data; hidden layers where connection strength is trained as a weight; and output layers of trained results. In the deep neural network layers, the control of the weight of training data enables repeated learning to create an ambient context pattern. Using an ontology inference engine, unstructured/structured data, including individual health data and environmental information, and their context is presented as ontology metadata. In the knowledge base, hidden association relationships are discovered through mining. To inform the individual health conditions exposed to the individual environmental contexts, a health risk assessment model is developed with a set of the ambient context pattern learned with metadata and a deep neural network. The Minkowski distance formula, which defines a normalized geometrical distance between two nodes, is used to measure the similarity between the patients with chronical disease and the individual user based on the context. In the proposed model, the risk is represented as a similarity-based index. The risk assessment model can be implemented into the individual risk alert/prevention system. The model may significantly impact the healthcare industry as well as ambient intelligence research, thus contributing to improve the quality of human life of the future society. {\textcopyright} 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
annote = {cited By 14},
author = {Chung, K and Yoo, H and Choe, D.-E.},
doi = {10.1007/s12652-018-1033-7},
issn = {18685137},
journal = {Journal of Ambient Intelligence and Humanized Computing},
keywords = {Ambient context; Context-based modeling; Environm,Ambient intelligence; Big data; Data mining; Deep,Multilayer neural networks},
number = {4},
pages = {1387--1395},
publisher = {Springer},
title = {{Ambient context-based modeling for health risk assessment using deep neural network}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053527910&doi=10.1007%2Fs12652-018-1033-7&partnerID=40&md5=674f484e51d755f4239bea46438d9513},
volume = {11},
year = {2020}
}
@article{Hakimov2017329,
abstract = {The task of answering natural language questions over RDF data has received wide interest in recent years, in particular in the context of the series of QALD benchmarks. The task consists of mapping a natural language question to an executable form, e.g. SPARQL, so that answers from a given KB can be extracted. So far, most systems proposed are (i) monolingual and (ii) rely on a set of hard-coded rules to interpret questions and map them into a SPARQL query. We present the first multilingual QALD pipeline that induces a model from training data for mapping a natural language question into logical form as probabilistic inference. In particular, our approach learns to map universal syntactic dependency representations to a language-independent logical form based on DUDES (Dependency-based Underspecified Discourse Representation Structures) that are then mapped to a SPARQL query as a deterministic second step. Our model builds on factor graphs that rely on features extracted from the dependency graph and corresponding semantic representations. We rely on approximate inference techniques, Markov Chain Monte Carlo methods in particular, as well as Sample Rank to update parameters using a ranking objective. Our focus lies on developing methods that overcome the lexical gap and present a novel combination of machine translation and word embedding approaches for this purpose. As a proof of concept for our approach, we evaluate our approach on the QALD-6 datasets for English, German \& Spanish. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 5; Conference of 16th International Semantic Web Conference, ISWC 2017 ; Conference Date: 21 October 2017 Through 25 October 2017; Conference Code:200299},
author = {Hakimov, S and Jebbara, S and Cimiano, P},
doi = {10.1007/978-3-319-68288-4_20},
editor = {{Cudre-Mauroux P. Lange C.}, d'Amato C Fernandez M Heflin J Lecue F Tamma V Sequeda J},
isbn = {9783319682877},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Factor graphs; Multilinguality; Probabilistic gra,Mapping; Markov processes; Monte Carlo methods; Na,Semantic Web},
pages = {329--346},
publisher = {Springer Verlag},
title = {{AMUSE: Multilingual semantic parsing for question answering over linked data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032216547&doi=10.1007%2F978-3-319-68288-4_20&partnerID=40&md5=f32e2d0aff706ef53cba2e41b9447b26},
volume = {10587 LNCS},
year = {2017}
}
@article{10.1016/j.websem.2009.11.002,
abstract = {Ontology mapping seeks to find semantic correspondences between similar elements of different ontologies. It is a key challenge to achieve semantic interoperability in building the Semantic Web. This paper proposes a new generic and adaptive ontology mapping approach, called the PRIOR+, based on propagation theory, information retrieval techniques and artificial intelligence. The approach consists of three major modules, i.e., the IR-based similarity generator, the adaptive similarity filter and weighted similarity aggregator, and the neural network based constraint satisfaction solver. The approach first measures both linguistic and structural similarity of ontologies in a vector space model, and then aggregates them using an adaptive method based on their harmonies, which is defined as an estimator of performance of similarity. Finally to improve mapping accuracy the interactive activation and competition neural network is activated, if necessary, to search for a solution that can satisfy ontology constraints. The experimental results show that harmony is a good estimator of f-measure; the harmony based adaptive aggregation outperforms other aggregation methods; neural network approach significantly boosts the performance in most cases. Our approach is competitive with top-ranked systems on benchmark tests at OAEI campaign 2007, and performs the best on real cases in OAEI benchmark tests.},
address = {NLD},
author = {Mao, Ming and Peng, Yefei and Spring, Michael},
doi = {10.1016/j.websem.2009.11.002},
issn = {1570-8268},
journal = {Web Semant.},
keywords = {Constraint satisfaction problem (CSP),D.2.12 [Software Engineering]: Interoperability-D,H.3.3 [Information Systems]: Information Search a,Harmony-based adaptive aggregation,I.2.6 [Artificial Intelligence]: Learning-Connect,Interactive activation and competition (IAC) netw,Ontology mapping,PRIOR+},
month = {mar},
number = {1},
pages = {14--25},
publisher = {Elsevier Science Publishers B. V.},
title = {{An Adaptive Ontology Mapping Approach with Neural Network Based Constraint Satisfaction}},
url = {https://doi.org/10.1016/j.websem.2009.11.002},
volume = {8},
year = {2010}
}
@article{Choi2020,
abstract = {Knowledge bases such as Freebase, YAGO, DBPedia, and Nell contain a number of facts with various entities and relations. Since they store many facts, they are regarded as core resources for many natural language processing tasks. Nevertheless, they are not normally complete and have many missing facts. Such missing facts keep them from being used in diverse applications in spite of their usefulness. Therefore, it is significant to complete knowledge bases. Knowledge graph embedding is one of the promising approaches to completing a knowledge base and thus many variants of knowledge graph embedding have been proposed. It maps all entities and relations in knowledge base onto a low dimensional vector space. Then, candidate facts that are plausible in the space are determined as missing facts. However, any single knowledge graph embedding is insufficient to complete a knowledge base. As a solution to this problem, this paper defines knowledge base completion as a ranking task and proposes a committee-based knowledge graph embedding model for improving the performance of knowledge base completion. Since each knowledge graph embedding has its own idiosyncrasy, we make up a committee of various knowledge graph embeddings to reflect various perspectives. After ranking all candidate facts according to their plausibility computed by the committee, the top-k facts are chosen as missing facts. Our experimental results on two data sets show that the proposed model achieves higher performance than any single knowledge graph embedding and shows robust performances regardless of k. These results prove that the proposed model considers various perspectives in measuring the plausibility of candidate facts. {\textcopyright} 2020 by the authors.},
annote = {cited By 0},
author = {Choi, S J and Song, H.-J. and Park, S.-B.},
doi = {10.3390/APP10082651},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
number = {8},
publisher = {MDPI AG},
title = {{An approach to knowledge base completion by a committee-based knowledge graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084443437&doi=10.3390%2FAPP10082651&partnerID=40&md5=de13a13a3ba9168c076ea1b10122f943},
volume = {10},
year = {2020}
}
@article{ISI:000501389000007,
abstract = {With the advancement of scientific and engineering research, a huge
number of academic literature are accumulated. Manually reviewing the
existing literature is the main way to explore embedded knowledge, and
the process is quite time-consuming and labor intensive. As the quantity
of literature is increasing exponentially, it would be more difficult to
cover all aspects of the literature using the traditional manual review
approach. To overcome this drawback, bibliometric analysis is used to
analyze the current situation and trend of a specific research field. In
the bibliometric analysis, only a few key phrases (e.g., authors,
publishers, journals, and citations) are usually used as the inputs for
analysis. Information other than those phrases is not extracted for
analysis, while that neglected information (e.g., abstract) might
provide more detailed knowledge in the article. To tackle with this
problem, this study proposed an automatic literature knowledge graph and
reasoning network modeling framework based on ontology and Natural
Language Processing (NLP), to facilitate the efficient knowledge
exploration from literature abstract. In this framework, a
representation ontology is proposed to characterize the literature
abstract data into four knowledge elements (background, objectives,
solutions, and findings), and NLP technology is used to extract the
ontology instances from the abstract automatically. Based on the
representation ontology, a four-space integrated knowledge graph is
built using NLP technology. Then, reasoning network is generated
according to the reasoning mechanism defined in the proposed ontology
model. To validate the proposed framework, a case study is conducted to
analyze the literature in the field of construction management. The case
study proves that the proposed ontology model can be used to represent
the knowledge embedded in the literatures' abstracts, and the ontology
elements can be automatically extracted by NLP models. The proposed
framework can be an enhancement for the bibliometric analysis to explore
more knowledge from the literature.},
address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
author = {Chen, Hainan and Luo, Xiaowei},
doi = {10.1016/j.aei.2019.100959},
issn = {1474-0346},
journal = {ADVANCED ENGINEERING INFORMATICS},
keywords = {Representation ontology; Natural language processi},
month = {oct},
publisher = {ELSEVIER SCI LTD},
title = {{An automatic literature knowledge graph and reasoning network modeling framework based on ontology and natural language processing}},
type = {Article},
volume = {42},
year = {2019}
}
@inproceedings{7019790,
abstract = {Rule acquisition acts as a heart of the Semantic Web applications. It is well known that the Semantic Web which implies inferential rules are inconsistent. To tackle this problem, this paper presents a new rule acquisition method based on type - 2 fuzzy rough ontology and self adaptive genetic algorithm to deal with inconsistent data on Web. In this research, we use type-2 fuzzy rough ontology to provide efficient inference mechanisms and reasoning capabilities, which deduce inconsistent and ambiguous data on the Web. Then self adaptive genetic algorithm is used for mining effective rules set. When the algorithm is processing, the user is allowed to set three evaluation parameter values of the rules: support confidence and coverage for specific application needs. The proposed algorithm can remove the rules which do not meet the requirements. The case study results show that the suggested method is suitable to provide efficient and consistent rules on Web.},
author = {Devadoss, N and Ramakrishnan, S},
booktitle = {2014 International Conference on Contemporary Computing and Informatics (IC3I)},
doi = {10.1109/IC3I.2014.7019790},
keywords = {data mining;fuzzy reasoning;fuzzy set theory;genet},
month = {nov},
pages = {1220--1225},
title = {{An efficient rule acquisition method using type - 2 fuzzy rough ontology}},
year = {2014}
}
@inproceedings{6121789,
abstract = {In this paper we present an approach for improving a specific class of semantic annotation, that relates a term of the document with a (sub)tree of the ontology, instead of linking a term with a single concept of the ontology. An important part of this class of annotation is filtering the relevant (sub)nodes and relations, because the returned graph should only contain relevant information, that is, nodes that are truly related with the topics of the document. In addition, we consider that the relevance of nodes vary depending on if the node is a branch or a leaf, that is, if the node has links to other nodes or it is a text-based description. This paper focuses on the relevance of branch nodes, which is calculated from the relevance of its links, since leaf nodes relevance is usually estimated by similarity metrics. Specifically, our approach incises in learning (through a genetic algorithm) and assigning the most appropriate weights to these links in order to reduce the precision/recall curve of the annotation process. The results show that our solution is viable and outperforms the state of the art approaches.},
author = {Vidal, J C and Lama, M and Otero-Garc{\'{i}}a, E and Bugar{\'{i}}n, A},
booktitle = {2011 11th International Conference on Intelligent Systems Design and Applications},
doi = {10.1109/ISDA.2011.6121789},
issn = {2164-7151},
keywords = {data mining;genetic algorithms;information filteri},
month = {nov},
pages = {1002--1007},
title = {{An evolutionary approach for learning the weight of relations in linked data}},
year = {2011}
}
@article{Chen20101208,
abstract = {With the rapid growth of text documents, document clustering has become one of the main techniques for organizing large amount of documents into a small number of meaningful clusters. However, there still exist several challenges for document clustering, such as high dimensionality, scalability, accuracy, meaningful cluster labels, overlapping clusters, and extracting semantics from texts. In order to improve the quality of document clustering results, we propose an effective Fuzzy-based Multi-label Document Clustering (FMDC) approach that integrates fuzzy association rule mining with an existing ontology WordNet to alleviate these problems. In our approach, the key terms will be extracted from the document set, and the initial representation of all documents is further enriched by using hypernyms of WordNet in order to exploit the semantic relations between terms. Then, a fuzzy association rule mining algorithm for texts is employed to discover a set of highly-related fuzzy frequent itemsets, which contain key terms to be regarded as the labels of the candidate clusters. Finally, each document is dispatched into more than one target cluster by referring to these candidate clusters, and then the highly similar target clusters are merged. We conducted experiments to evaluate the performance based on Classic, Re0, R8, and WebKB datasets. The experimental results proved that our approach outperforms the influential document clustering methods with higher accuracy. Therefore, our approach not only provides more general and meaningful labels for documents, but also effectively generates overlapping clusters. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
annote = {cited By 57},
author = {Chen, C.-L. and Tseng, F S C and Liang, T},
doi = {10.1016/j.datak.2010.08.003},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Association rules; Associative processing; Cluste,Cluster analysis,Document Clustering; Frequent Itemsets; Fuzzy asso},
number = {11},
pages = {1208--1226},
title = {{An integration of WordNet and fuzzy association rule mining for multi-label document clustering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957752478&doi=10.1016%2Fj.datak.2010.08.003&partnerID=40&md5=8937c5a95c04f2a45d2bcfb81a4d1665},
volume = {69},
year = {2010}
}
@article{Amador-Domínguez2020125,
abstract = {This paper introduces a new initialization method for knowledge graph (KG) embedding that can leverage ontological information in knowledge graph completion problems, such as link classification and link prediction. Although the initialization method is general and applicable to different KG embedding approaches in the literature, such as TransE or RESCAL, this paper experiments with deep learning and specifically with the neural tensor network (NTN) model. The experimental results show that the proposed method can improve link classification for a given relation by up{\^{A}} to 15%. In a second contribution, the proposed method allows for addressing a problem not studied in the literature and introduced here as “KG completion with fresh entities”. This is the use of KG embeddings for KG completion when one or several of the entities in a triple (head, relation, tail) has not been observed in the training phase. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 1; Conference of 16th International Conference on Distributed Computing and Artificial Intelligence, DCAI 2019 ; Conference Date: 26 June 2019 Through 28 June 2019; Conference Code:227849},
author = {Amador-Dom{\'{i}}nguez, E and Hohenecker, P and Lukasiewicz, T and Manrique, D and Serrano, E},
doi = {10.1007/978-3-030-23887-2_15},
editor = {{Herrera F. Matsui K.}, Rodriguez-Gonzalez S},
isbn = {9783030238865},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Artificial intelligence; Classification (of inform,Completion problem; Feature modeling; Initializat,Deep learning},
pages = {125--133},
publisher = {Springer Verlag},
title = {{An ontology-based deep learning approach for knowledge graph completion with fresh entities}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068593918&doi=10.1007%2F978-3-030-23887-2_15&partnerID=40&md5=06884f5efdc11de071f3242452a87bab},
volume = {1003},
year = {2020}
}
@article{10.1007/s11042-011-0936-5,
abstract = {This paper deals with information retrieval and semantic indexing of multimedia documents. We propose a generic scheme combining an ontology-based evidential framework and high-level multimodal fusion, aimed at recognising semantic concepts in videos. This work is represented on two stages: First, the adaptation of evidence theory to neural network, thus giving Neural Network based on Evidence Theory (NNET). This theory presents two important information for decision-making compared to the probabilistic methods: belief degree and system ignorance. The NNET is then improved further by incorporating the relationship between descriptors and concepts, modeled by a weight vector based on entropy and perplexity. The combination of this vector with the classifiers outputs, gives us a new model called Perplexity-based Evidential Neural Network (PENN). Secondly, an ontology-based concept is introduced via the influence representation of the relations between concepts and the ontological readjustment of the confidence values. To represent this relationship, three types of information are computed: low-level visual descriptors, concept co-occurrence and semantic similarities. The final system is called Ontological-PENN. A comparison between the main similarity construction methodologies are proposed. Experimental results using the TRECVid dataset are presented to support the effectiveness of our scheme.},
address = {USA},
author = {Benmokhtar, Rachid and Huet, Benoit},
doi = {10.1007/s11042-011-0936-5},
issn = {1380-7501},
journal = {Multimedia Tools Appl.},
keywords = {Classification,Classifier fusion,Inter-concepts similarity,LSCOM-lite,Ontology,Semantic gap,TRECVid,Video shots indexing},
month = {nov},
number = {2},
pages = {663--689},
publisher = {Kluwer Academic Publishers},
title = {{An Ontology-Based Evidential Framework for Video Indexing Using High-Level Multimodal Fusion}},
url = {https://doi.org/10.1007/s11042-011-0936-5},
volume = {73},
year = {2014}
}
@article{Ma2017185,
abstract = {Nowadays, online data shows an astonishing increase and the issue of semantic indexing remains an open question. Ontologies and knowledge bases have been widely used to optimize performance. However, researchers are placing increased emphasis on internal relations of ontologies but neglect latent semantic relations between ontologies and documents. They generally annotate instances mentioned in documents, which are related to concepts in ontologies. In this paper, we propose an Ontology-based Latent Semantic Indexing approach utilizing Long Short-Term Memory networks (LSTM-OLSI). We utilize an importance-aware topic model to extract document-level semantic features and leverage ontologies to extract word-level contextual features. Then we encode the above two levels of features and match their embedding vectors utilizing LSTM networks. Finally, the experimental results reveal that LSTM-OLSI outperforms existing techniques and demonstrates deep comprehension of instances and articles. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 2; Conference of 1st Asia-Pacific Web and Web-Age Information Management Joint Conference on Web and Big Data, APWeb-WAIM 2017 ; Conference Date: 7 July 2017 Through 9 July 2017; Conference Code:196129},
author = {Ma, N and Zheng, H.-T. and Xiao, X},
doi = {10.1007/978-3-319-63579-8_15},
editor = {{Shahabi C. Lian X.}, Jensen C S Yang X Chen L},
isbn = {9783319635781},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Big data; Brain; Data mining; Indexing (of informa,Contextual feature; Knowledge basis; Latent Seman,Long short-term memory},
pages = {185--199},
publisher = {Springer Verlag},
title = {{An ontology-based latent semantic indexing approach using long short-term memory networks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028455427&doi=10.1007%2F978-3-319-63579-8_15&partnerID=40&md5=7ffebe468218d324ac8d5ca3ddbcb605},
volume = {10366 LNCS},
year = {2017}
}
@inproceedings{Mouakher2019307,
abstract = {Given the France's rich wine heritage as well as its pioneering position as the world's second wine producer, the production of high quality wines plays a role of primary importance. The recent development of IOT and efficient big data processing has been shown to provide purposeful issue to permanent monitoring during the entire wine making process. Standing within this trend, we introduce in this paper an intelligent system for vineyards monitoring in the Burgundy region. The main trust of the proposed system relies on the use of the Swrl rules in WineCloud ontology. The design of the ontology is mainly based on information gathered from interviews with wine growers. In addition, sensor data is also collected and used to feed the ontology after being processed. The system is used in the aim to have better grape quality with an improved vineyard management. To do so, association rules are extracted from the collected data aiming to provide useful knowledge to forecast vine diseases. {\textcopyright} 2019 IEEE.},
annote = {cited By 2; Conference of 28th IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE 2019 ; Conference Date: 12 June 2019 Through 14 June 2019; Conference Code:150864},
author = {Mouakher, A and Belkaroui, R and Bertaux, A and Labbani, O and Hugol-Gential, C and Nicolle, C},
booktitle = {Proceedings - 2019 IEEE 28th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE 2019},
doi = {10.1109/WETICE.2019.00070},
isbn = {9781728106762},
keywords = {Association rules mining; High quality; Monitorin,Association rules; Data handling; Data mining; Int,Monitoring},
pages = {307--312},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{An Ontology-Based Monitoring System in Vineyards of the Burgundy Region}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071693202&doi=10.1109%2FWETICE.2019.00070&partnerID=40&md5=a442fca1fcd424d274f2f95248a7efb6},
year = {2019}
}
@inproceedings{ISI:000449466100112,
abstract = {Precisely understanding the value and perception of consumers has long
been recognized as essential elements of every market-oriented company's
core business strategy. For this reason, customers' affection, as the
basis for the formation of human values and judgment, should be
considered carefully to strengthen the product quality and
competitiveness. However, conventional product design places more
attention to functional attributes and requires survey process to
collect customers' evaluations, neglecting the in-depth study of the
underlying associations between design properties and consumers'
emotions based on the abundant online consumer response resources. To
improve the deficiency, this study was proposed to develop a product
affective properties identification approach. Particularly, data mining
techniques (e.g. web mining, text mining) are applied to capture online
product review resources. Considering the characteristics of
user/consumer responses and evaluations, ontology is utilized to assist
in the semantic analysis. With the help of product knowledge hierarchy
and electronic lexical database, product properties, which can evoke
consumers' affect, can be identified. Furthermore, the identified
product affective properties are prioritized to provide designers with
important reference for future improvement on the product. To illustrate
the proposed approach, a pilot study based on iPhone 7 was conducted, in
which the influential affective properties have been identified, and a
ranking of them has been mapped out.},
address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
annote = {24th ISPE Inc. International Conference on Transdisciplinary
Engineering, Nanyang Technol Univ, Singapore, SINGAPORE, JUL 10-14, 2017},
author = {Chang, Danni and Lin, Danping and Han, Ting},
booktitle = {TRANSDISCIPLINARY ENGINEERING: A PARADIGM SHIFT},
doi = {10.3233/978-1-61499-779-5-977},
editor = {{Chen, CH and Trappey, AC and Peruzzini, M and Stjepandic, J and Wognum, N}},
isbn = {978-1-61499-779-5; 978-1-61499-778-8},
issn = {2352-7528},
keywords = {Product affective property; ontology; data mining;},
organization = {Int Soc Product Enhancement Inc; Fraunhofer; IOS Press; PROSTEP AG},
pages = {977--984},
publisher = {IOS PRESS},
series = {Advances in Transdisciplinary Engineering},
title = {{An Ontology-Based Product Affective Properties Identification Approach}},
type = {Proceedings Paper},
volume = {5},
year = {2017}
}
@inproceedings{8959791,
abstract = {The paper approaches aspect based opinion mining, which uses an unsupervised neural network as the opinion classifier. The neural network is an extension of the Growing Hierarchical Self-organizing Maps (GHSOM). In the aspect based sentiment analysis, we exploit the fact that the binary relations of syntactic dependency, extracted from product reviews, very often express relations between an aspect of the reviewed product and an opinion towards that aspect. We use the Growing Hierarchical Self-organizing Maps to classify pairs built according to the dependency relations, more exactly to classify pairs of the form (aspect, opinion bearing word). With this classification, we discover whether the various text mentions of the aspects of the target entity (such as the aspects of a product) are opinionated with positive or negative sentiment in the text of a review. We classify these pairs against a domain specific taxonomy of aspects, which also includes (positive/ negative) opinions associated with the aspects. Since it is based on classification against an ontology, our approach is semantic oriented. We tested our system on a collection of reviews about photo cameras.},
author = {Chifu, E S and Chifu, V R},
booktitle = {2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP)},
doi = {10.1109/ICCP48234.2019.8959791},
keywords = {data mining;ontologies (artificial intelligence);p},
pages = {151--157},
title = {{An Unsupervised Neural Model for Aspect Based Opinion Mining}},
year = {2019}
}
@article{Kiefer2011460,
abstract = {Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting four sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for almost any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines even shows that our approach is superior in terms of classification accuracy. {\textcopyright} 2011 Springer-Verlag Berlin Heidelberg.},
address = {Galway},
annote = {cited By 2; Conference of 7th International Summer School 2011 on Reasoning Web: Semantic Technologies for the Web of Data ; Conference Date: 23 August 2011 Through 27 August 2011; Conference Code:86354},
author = {Kiefer, C and Bernstein, A},
doi = {10.1007/978-3-642-23032-5_10},
isbn = {9783642230318},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Analytical results; Bayesian classifier; Classific,Classification (of information); Learning systems,Semantic Web},
pages = {460--503},
title = {{Application and evaluation of inductive reasoning methods for the Semantic Web and software analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052403581&doi=10.1007%2F978-3-642-23032-5_10&partnerID=40&md5=ed05c3d79d026c9c58618a86f40d4573},
volume = {6848 LNCS},
year = {2011}
}
@inproceedings{Neubarth20127,
abstract = {This paper demonstrates how association rule mining can be applied to discover relations between two ontologies of folk music: a genre and a region ontology. Genreregion associations have been widely studied in folk music research but have been neglected in music information retrieval. We present a method of association rule mining with constraints consisting of rule templates and rule evaluation measures to identify different, musicologically motivated, categories of genre-region associations. The method is applied to a corpus of 1902 Basque folk tunes, and several interesting rules and rule sets are discovered. {\textcopyright} 2012 International Society for Music Information Retrieval.},
address = {Porto},
annote = {cited By 8; Conference of 13th International Society for Music Information Retrieval Conference, ISMIR 2012 ; Conference Date: 8 October 2012 Through 12 October 2012; Conference Code:95398},
author = {Neubarth, K and Goienetxea, I and Johnson, C G and Conklin, D},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012},
isbn = {9789727521449},
keywords = {Association mining; Interesting rules; Music genre,Information retrieval},
pages = {7--12},
title = {{Association mining of folk music genres and toponyms}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873438951&partnerID=40&md5=acf845aed4c4f24401d5982a4e8bb5ae},
year = {2012}
}
@inproceedings{7300289,
abstract = {Gene Ontology is one of the largest bioinformatics project that seeks to consolidate knowledge about genes through annotation of terms to three ontologies. In this work, we present a technique to find association relationships in the annotation terms for the Saccharomyces cerevisiae (SGD) genome. We first present a normalization algorithm to ensure that the annotation terms have a similar level of specificity. Association rule mining algorithms are used to find significant and non-trivial association rules in these normalized datasets. Metrics such as support, confidence, and lift can be used to evaluate the strength of found rules. We conducted experiments on the entire SGD annotation dataset and here we present the top 10 strongest rules for each of the three ontologies. We verify the found rules using evidence from the biomedical literature. The presented method has a number of advantages - it relies only on the structure of the gene ontology, has minimal memory and storage requirements, and can be easily scaled for large genomes, such as the human genome. There are many applications of this technique, such as predicting the GO annotations for new genes or those that have not been studied extensively.},
author = {Nagar, A and Hahsler, M and Al-Mubaid, H},
booktitle = {2015 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)},
doi = {10.1109/CIBCB.2015.7300289},
keywords = {bioinformatics;biomedical engineering;data mining;},
month = {aug},
pages = {1--7},
title = {{Association rule mining of gene ontology annotation terms for SGD}},
year = {2015}
}
@article{Bytyçi201667,
abstract = {Internet of Things (IoT) applications by means of wireless sensor networks (WSN) produce large amounts of raw data. These data might formally be defined by following a semantic IoT model that covers data, meta-data, as well as their relations, or might simply be stored in a database without any formal specification. In both cases, using association rules as a data mining technique may result into inferring interesting relations between data and/or metadata. In this paper we argue that the context has not been used extensively for added value to the mining process. Therefore, we propose a different approach when it comes to association rule mining by enriching it with a context-aware ontology. The approach is demonstrated by hand of an application to WSNs for water quality monitoring. Initially, new ontology, its concepts and relationships are introduced to model water quality monitoring through mobile sensors. Consequently, the ontology is populated with quality data generated by sensors, and enriched afterwards with context. Finally, the evaluation results of our approach of including context ontology in the mining process are promising: new association rules have been derived, providing thus new knowledge not inferable when applying association rule mining simply over raw data. {\textcopyright} Springer International Publishing AG 2016.},
annote = {cited By 7; Conference of 10th International Conference on Metadata and Semantics Research, MTSR 2016 ; Conference Date: 22 November 2016 Through 25 November 2016; Conference Code:186869},
author = {Byty{\c{c}}i, E and Ahmedi, L and Kurti, A},
doi = {10.1007/978-3-319-49157-8_6},
editor = {{Garoufallou E. Coll I.S.}, Stellato A Greenberg J},
isbn = {9783319491561},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Association rules; Data mining; Internet of things,Context; Context ontology; Evaluation results; In,Wireless sensor networks},
pages = {67--78},
publisher = {Springer Verlag},
title = {{Association rule mining with context ontologies: An application to mobile sensing of water quality}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000384218&doi=10.1007%2F978-3-319-49157-8_6&partnerID=40&md5=b9c2507955ff22860480306c0bf745a4},
volume = {672},
year = {2016}
}
@inproceedings{10.1145/3397271.3401428,
abstract = {Recommender system (RS) devotes to predicting user preference to a given item and has been widely deployed in most web-scale applications. Recently, knowledge graph (KG) attracts much attention in RS due to its abundant connective information. Existing methods either explore independent meta-paths for user-item pairs over KG, or employ graph neural network (GNN) on whole KG to produce representations for users and items separately. Despite effectiveness, the former type of methods fails to fully capture structural information implied in KG, while the latter ignores the mutual effect between target user and item during the embedding propagation. In this work, we propose a new framework named Adaptive Target-Behavior Relational Graph network (ATBRG for short) to effectively capture structural relations of target user-item pairs over KG. Specifically, to associate the given target item with user behaviors over KG, we propose the graph connect and graph prune techniques to construct adaptive target-behavior relational graph. To fully distill structural information from the sub-graph connected by rich relations in an end-to-end fashion, we elaborate on the model design of ATBRG, equipped with relation-aware extractor layer and representation activation layer. We perform extensive experiments on both industrial and benchmark datasets. Empirical results show that ATBRG consistently and significantly outperforms state-of-the-art methods. Moreover, ATBRG has also achieved a performance improvement of 5.1% on CTR metric after successful deployment in one popular recommendation scenario of Taobao APP.},
address = {New York, NY, USA},
author = {Feng, Yufei and Hu, Binbin and Lv, Fuyu and Liu, Qingwen and Zhang, Zhiqiang and Ou, Wenwu},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3397271.3401428},
isbn = {9781450380164},
keywords = {graph neural network,knowledge graph,recommender system},
pages = {2231--2240},
publisher = {Association for Computing Machinery},
series = {SIGIR '20},
title = {{ATBRG: Adaptive Target-Behavior Relational Graph Network for Effective Recommendation}},
url = {https://doi.org/10.1145/3397271.3401428},
year = {2020}
}
@article{Zhao2020542,
abstract = {Knowledge graph completion (KGC) aims to predict missing information in a knowledge graph. Many existing embedding-based KGC models solve the Out-of-knowledge-graph (OOKG) entity problem (also known as zero-shot entity problem) by utilizing textual information resources such as descriptions and types. However, few works utilize the extra structural information to generate embeddings. In this paper, we propose a new zero-shot scenario: how to acquire the embedding vector of a relation that is not observed at training time. Our work uses a convolutional transition and attention-based aggregation graph neural network to solve both the OOKG entity problem and the new OOKG relation problem without retraining, regarding the structural neighbors as the auxiliary information. The experimental results show the effectiveness of our proposed models in solving the OOKG relation problem. For the OOKG entity problem, our model performs better than the previous GNN-based model by 23.9% in NELL-995-Tail dataset. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2020 ; Conference Date: 11 May 2020 Through 14 May 2020; Conference Code:240129},
author = {Zhao, M and Jia, W and Huang, Y},
doi = {10.1007/978-3-030-47436-2_41},
editor = {{Lauw H.W. Lim E.-P.}, Wong R.C.-W. Ntoulas A Ng S.-K. Pan S J},
isbn = {9783030474355},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Auxiliary information; Graph networks; Graph neur,Convolutional neural networks; Data communication,Data mining},
pages = {542--554},
publisher = {Springer},
title = {{Attention-Based Aggregation Graph Networks for Knowledge Graph Information Transfer}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734369&doi=10.1007%2F978-3-030-47436-2_41&partnerID=40&md5=cd17f5b8de516d9c885f843fef355a79},
volume = {12085 LNAI},
year = {2020}
}
@inproceedings{Balaneshinkordan20181173,
abstract = {The problem of ad-hoc structured document retrieval arises in many information access scenarios, from Web to product search. Yet neither deep neural networks, which have been successfully applied to ad-hoc information retrieval and Web search, nor the attention mechanism, which has been shown to significantly improve the performance of deep neural networks on natural language processing tasks, have been explored in the context of this problem. In this paper, we propose a deep neural architecture for ad-hoc structured document retrieval, which utilizes attention mechanism to determine important phrases in keyword queries as well as the relative importance of matching those phrases in different fields of structured documents. Experimental evaluation on publicly available collections for Web document, product and entity retrieval from knowledge graphs indicates superior retrieval accuracy of the proposed neural architecture relative to both state-of-the-art neural architectures for ad-hoc document retrieval and probabilistic models for ad-hoc structured document retrieval. {\textcopyright} 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
annote = {cited By 1; Conference of 27th ACM International Conference on Information and Knowledge Management, CIKM 2018 ; Conference Date: 22 October 2018 Through 26 October 2018; Conference Code:142310},
author = {Balaneshinkordan, S and Kotov, A and Nikolaev, F},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3269206.3271801},
editor = {{Paton N. Candan S.}, Wang H Allan J Agrawal R Labrinidis A Cuzzocrea A Zaki M Srivastava D Broder A Schuster A},
isbn = {9781450360142},
keywords = {Architecture; Deep neural networks; Electronic doc,Attention; Attention mechanisms; Experimental eva,Information retrieval},
pages = {1173--1182},
publisher = {Association for Computing Machinery},
title = {{Attentive neural architecture for ad-hoc structured document retrieval}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058024140&doi=10.1145%2F3269206.3271801&partnerID=40&md5=0644b382e9fac7a6d9dac7b58562564f},
year = {2018}
}
@inproceedings{8258142,
abstract = {The technological advancements in biomedical domain has led to a tremendous growth of unstructured data; primarily a result of increased publication of findings. At the same time, a corresponding interest in the Natural Language Processing (NLP) community to develop scalable methodologies to exploit such massive unlabeled corpora for unsupervised language processing has resulted in new opportunities towards developing semantic sensitive models. Amongst them, the field of word embeddings has garnered significant attention due to its capability to understand implicit semantics. However such data driven models are largely agnostic of the rich explicit semantic knowledge available in the biomedical domain in the form of vocabularies and ontologies. This is problematic because it leads to a poor representation of words with little local context and its effect is acute in biomedical domain. In this paper, we propose a novel model (MeSH2Vec) that jointly exploits both contextual information and available explicit semantic knowledge to learn externally augmented word embeddings. Unlike existing approaches, the proposed methodology is more dexterous in its ability to handle relationships between indirectly related concepts. The 13% improvement in the correlation to experts, shown on experiments involving biomedical concept similarity and relatedness task validates the effectiveness of the proposed approach and demonstrates the importance of incorporating human curated knowledge in the process of generating word embeddings.},
author = {Jha, K and Xun, G and Gopalakrishnan, V and Zhang, A},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)},
doi = {10.1109/BigData.2017.8258142},
keywords = {bioinformatics;formal concept analysis;knowledge b},
month = {dec},
pages = {1965--1974},
title = {{Augmenting word embeddings through external knowledge-base for biomedical application}},
year = {2017}
}
@article{Braun2020,
abstract = {Natural language descriptions of plant phenotypes are a rich source of information for genetics and genomics research. We computationally translated descriptions of plant phenotypes into structured representations that can be analyzed to identify biologically meaningful associations. These representations include the entity–quality (EQ) formalism, which uses terms from biological ontologies to represent phenotypes in a standardized, semantically rich format, as well as numerical vector representations generated using natural language processing (NLP) methods (such as the bag-of-words approach and document embedding). We compared resulting phenotype similarity measures to those derived from manually curated data to determine the performance of each method. Computationally derived EQ and vector representations were comparably successful in recapitulating biological truth to representations created through manual EQ statement curation. Moreover, NLP methods for generating vector representations of phenotypes are scalable to large quantities of text because they require no human input. These results indicate that it is now possible to computationally and automatically produce and populate large-scale information resources that enable researchers to query phenotypic descriptions directly. {\textcopyright} Copyright {\textcopyright} 2020 Braun and Lawrence-Dill.},
annote = {cited By 0},
author = {Braun, I R and Lawrence-Dill, C J},
doi = {10.3389/fpls.2019.01629},
issn = {1664462X},
journal = {Frontiers in Plant Science},
publisher = {Frontiers Media S.A.},
title = {{Automated Methods Enable Direct Computation on Phenotypic Descriptions for Novel Candidate Gene Prediction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078186082&doi=10.3389%2Ffpls.2019.01629&partnerID=40&md5=9275113c8197cfcda05f84222872d361},
volume = {10},
year = {2020}
}
@article{Stork2019667,
abstract = {In this paper, scientific species names from images of handwritten species observations are automatically recognised and annotated with semantic concepts, so that they can be used for document retrieval and faceted search. Until now, automated semantic annotation of such named entities was only applied to printed or digital text. We employ a two-step approach. First, word images are classified, identifying elements of scientific species names; Genus, species, author, using (i) visual structural features, (ii) position, and (iii) context. Second, the identified species names are semantically annotated according to the NHC-Ontology, an ontology that describes species observations. Internationalised Resource Identifiers (IRIs) are assigned to the elements so that they can be linked and disambiguated at a later stage by individual researchers. For the identification of scientific species names, we achieve an average F1 score of 0.86. Moreover, we discuss how our method will function in a semi-automated annotation process, with a fruitful dialogue between system and user as the main objective. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 1; Conference of 41st European Conference on Information Retrieval, ECIR 2019 ; Conference Date: 14 April 2019 Through 18 April 2019; Conference Code:225189},
author = {Stork, L and Weber, A and van den Herik, J and Plaat, A and Verbeek, F and Wolstencroft, K},
doi = {10.1007/978-3-030-15712-8_43},
editor = {{Stein B. Hauff C.}, Mayr P Fuhr N Azzopardi L Hiemstra D},
isbn = {9783030157111},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automation; Biodiversity; Character recognition; D,Document Retrieval; Faceted search; Handwritten t,Semantics},
pages = {667--680},
publisher = {Springer Verlag},
title = {{Automated semantic annotation of species names in handwritten Texts}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064860066&doi=10.1007%2F978-3-030-15712-8_43&partnerID=40&md5=f6a8a008584fe5d463dc6124e0ef388a},
volume = {11437 LNCS},
year = {2019}
}
@article{Aydin2017,
abstract = {Information regarding the physical interactions among proteins is crucial, since protein-protein interactions (PPIs) are central for many biological processes. The experimental techniques used to verify PPIs are vital for characterizing and assessing the reliability of the identified PPIs. A lot of information about PPIs and the experimental methods are only available in the text of the scientific publications that report them. In this study, we approach the problem of identifying passages with experimental methods for physical interactions between proteins as an information retrieval search task. The baseline system is based on query matching, where the queries are generated by utilizing the names (including synonyms) of the experimental methods in the Proteomics Standard Initiative-Molecular Interactions (PSI-MI) ontology. We propose two methods, where the baseline queries are expanded by including additional relevant terms. The first method is a supervised approach, where the most salient terms for each experimental method are obtained by using the term frequency-relevance frequency (tf.rf) metric over 13 articles from our manually annotated data set of 30 full text articles, which is made publicly available. On the other hand, the second method is an unsupervised approach, where the queries for each experimental method are expanded by using the word embeddings of the names of the experimental methods in the PSI-MI ontology. The word embeddings are obtained by utilizing a large unlabeled full text corpus. The proposed methods are evaluated on the test set consisting of 17 articles. Both methods obtain higher recall scores compared with the baseline, with a loss in precision. Besides higher recall, the word embeddings based approach achieves higher F-measure than the baseline and the tf.rf based methods. We also show that incorporating gene name and interaction keyword identification leads to improved precision and F-measure scores for all three evaluated methods. The tf.rf based approach was developed as part of our participation in the Collaborative Biocurator Assistant Task of the BioCreative V challenge assessment, whereas the word embeddings based approach is a novel contribution of this article. VC The Author(s) 2017. Published by Oxford University Press.},
annote = {cited By 5},
author = {Aydin, F and H{\"{u}}s{\"{u}}nbeyi, Z M and {\"{O}}zg{\"{u}}r, A},
doi = {10.1093/database/baw166},
issn = {17580463},
journal = {Database},
keywords = {Biological Ontologies; Data Curation; Data Mining,Protein; Proteins,biological ontology; data mining; genetics; infor,protein},
number = {1},
publisher = {Oxford University Press},
title = {{Automatic query generation using word embeddings for retrieving passages describing experimental methods}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016080195&doi=10.1093%2Fdatabase%2Fbaw166&partnerID=40&md5=2b448d810a567bdb3f51cd97e92862aa},
volume = {2017},
year = {2017}
}
@inproceedings{Fauceglia202025,
abstract = {The Knowledge Graph Induction Service (KGIS) is an end-to-end knowledge induction system. One of its main capabilities is to automatically induce taxonomies1 from input documents using a hybrid approach that takes advantage of linguistic patterns, semantic web and neural networks. KGIS allows the user to semi-automatically curate and expand the induced taxonomy through a component called smart spreadsheet by exploiting distributional semantics. In this paper, we describe these taxonomy induction and expansion features of KGIS. A screencast video demonstrating the system is available in https://ibm.box.com/v/emnlp-2019-demo . {\textcopyright} 2019 Association for Computational Linguistics.},
annote = {cited By 1; Conference of 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019 ; Conference Date: 3 November 2019 Through 7 November 2019; Conference Code:160431},
author = {Fauceglia, N R and Gliozzo, A and Dash, S and Chowdhury, M F M and Mihindukulasooriya, N},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Proceedings of System Demonstrations},
isbn = {9781950737925},
keywords = {Distributional semantics; End to end; Hybrid appr,HTTP; Taxonomies,Natural language processing systems},
pages = {25--30},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Automatic taxonomy induction and expansion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087435953&partnerID=40&md5=e6a757718ea22a1913f24ed788ad3a26},
year = {2020}
}
@inproceedings{9031447,
abstract = {Extracting semantic information from web tables is a heavily researched area in natural language processing (NLP) and knowledge base augmentation. Identifying the kind of information each table column contains makes it easy for computers to read, understand and use the information in a table. Mapping the columns to concepts from an ontology solves this issue. This paper introduces Col2Pedia, a novel supervised learning based approach to map web table columns to concepts from a knowledge base, automatically. Col2Pedia leverages the behaviour of columns of a table when taken as a pair to infer mappings for these columns with high precision and recall.},
author = {Chamiran, K and Rukshan, A and Thayasivam, U},
booktitle = {2020 IEEE 14th International Conference on Semantic Computing (ICSC)},
doi = {10.1109/ICSC.2020.00029},
issn = {2325-6516},
keywords = {dbpedia,knowledge based systems;learning (artificial intel,transe,translation embedding},
month = {feb},
pages = {150--153},
title = {{Automating web Table Columns to Knowledge Base Mapping using Translation Embedding}},
year = {2020}
}
@article{Wei2020580,
abstract = {With the growth of knowledge graphs, entity descriptions are becoming extremely lengthy. Entity summarization task, aiming to generate diverse, comprehensive and representative summaries for entities, has received an increasing interest recently. In most previous methods, features are usually extracted by the hand-crafted templates. Then the feature selection and multi-user preference simulation take place, depending too much on human expertise. In this paper, a novel integration method called AutoSUM is proposed for automatic feature extraction and multi-user preference simulation to overcome the drawbacks of previous methods. There are two modules in AutoSUM: extractor and simulator. The extractor module operates automatic feature extraction based on a BiLSTM with a combined input representation including word embeddings and graph embeddings. Meanwhile, the simulator module automates multi-user preference simulation based on a well-designed two-phase attention mechanism (i.e., entity-phase attention and user-phase attention). Experimental results demonstrate that AutoSUM produces the state-of-the-art performance on two widely used datasets (i.e., DBpedia and LinkedMDB) in both F-measure and MAP. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 1; Conference of 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2020 ; Conference Date: 11 May 2020 Through 14 May 2020; Conference Code:240129},
author = {Wei, D and Liu, Y and Zhu, F and Zang, L and Zhou, W and Lu, Y and Hu, S},
doi = {10.1007/978-3-030-47436-2_44},
editor = {{Lauw H.W. Lim E.-P.}, Wong R.C.-W. Ntoulas A Ng S.-K. Pan S J},
isbn = {9783030474355},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attention mechanisms; Automatic feature extractio,Data mining; Embeddings; Extraction,Feature extraction},
pages = {580--592},
publisher = {Springer},
title = {{AutoSUM: Automating Feature Extraction and Multi-user Preference Simulation for Entity Summarization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085738272&doi=10.1007%2F978-3-030-47436-2_44&partnerID=40&md5=69afb7caba6f3d494c2b8763ad07b197},
volume = {12085 LNAI},
year = {2020}
}
@article{10.1007/s10844-018-0535-2,
abstract = {Knowledge representation learning (KRL), exploited by various applications such as question answering and information retrieval, aims to embed the entities and relations contained by the knowledge graph into points of a vector space such that the semantic and structure information of the graph is well preserved in the representing space. However, the previous works mainly learned the embedding representations by treating each entity and relation equally which tends to ignore the inherent imbalance and heterogeneous properties existing in knowledge graph. By visualizing the representation results obtained from classic algorithm TransE in detail, we reveal the disadvantages caused by this homogeneous learning strategy and gain insight of designing policy for the homogeneous representation learning. In this paper, we propose a novel margin-based pairwise representation learning framework to be incorporated into many KRL approaches, with the method of introducing adaptivity according to the degree of knowledge heterogeneity. More specially, an adaptive margin appropriate to separate the real samples from fake samples in the embedding space is first proposed based on the sample's distribution density, and then an adaptive weight is suggested to explicitly address the trade-off between the different contributions coming from the real and fake samples respectively. The experiments show that our Adaptive Weighted Margin Learning (AWML) framework can help the previous work achieve a better performance on real-world Knowledge Graphs Freebase and WordNet in the tasks of both link prediction and triplet classification.},
address = {USA},
author = {Guo, Chenchen and Zhang, Chunhong and Han, Xiao and Ji, Yang},
doi = {10.1007/s10844-018-0535-2},
issn = {0925-9902},
journal = {J. Intell. Inf. Syst.},
keywords = {Adaptive importance weight,Adaptive margin,Knowledge graph,Knowledge representation learning},
month = {aug},
number = {1},
pages = {167--197},
publisher = {Kluwer Academic Publishers},
title = {{AWML: Adaptive Weighted Margin Learning for Knowledge Graph Embedding}},
url = {https://doi.org/10.1007/s10844-018-0535-2},
volume = {53},
year = {2019}
}
@inproceedings{9194505,
abstract = {Short text classification is an important task in the area of natural language processing. Recent studies attempt to employ external knowledge to improve classification performance, but they ignore the correlation between external knowledge and have poor interpretability. This paper proposes a novel Background Knowledge Graph based method for Short Text Classification called BaKGraSTeC for short, which can not only employ external knowledge from a knowledge graph to enrich text information, but also utilize its structural information through a graph neural network to promote the understanding of texts. Specifically, we construct a background knowledge graph based on training data, then we propose a novel architecture that integrates background knowledge graph into a graph neural network to model and capture implicit interactions between its concepts and classes. Besides, we propose an attention mechanism considering both similarity and co-occurrence between concepts and classes to identify the informative concepts in texts. Our experimental results demonstrate the effectiveness with good interpretability of BaKGraSTeC through using external knowledge and their structural information for short text classification.},
author = {Jiang, X and Shen, Y and Wang, Y and Jin, X and Cheng, X},
booktitle = {2020 IEEE International Conference on Knowledge Graph (ICKG)},
doi = {10.1109/ICBK50248.2020.00058},
keywords = {graph theory;learning (artificial intelligence);na},
month = {aug},
pages = {360--366},
title = {{BaKGraSTeC: A Background Knowledge Graph Based Method for Short Text Classification}},
year = {2020}
}
@inproceedings{10.1145/3357384.3358014,
abstract = {Low-dimensional embeddings of knowledge graphs and behavior graphs have proved remarkably powerful in varieties of tasks, from predicting unobserved edges between entities to content recommendation. The two types of graphs can contain distinct and complementary information for the same entities/nodes. However, previous works focus either on knowledge graph embedding or behavior graph embedding while few works consider both in a unified way. Here we present BEM, a Bayesian framework that incorporates the information from knowledge graphs and behavior graphs. To be more specific, BEM takes as prior the pre-trained embeddings from the knowledge graph, and integrates them with the pre-trained embeddings from the behavior graphs via a Bayesian generative model. BEM is able to mutually refine the embeddings from both sides while preserving their own topological structures. To show the superiority of our method, we conduct a range of experiments on three benchmark datasets: node classification, link prediction, triplet classification on two small datasets related to Freebase, and item recommendation on a large-scale e-commerce dataset.},
address = {New York, NY, USA},
author = {Ye, Yuting and Wang, Xuwu and Yao, Jiangchao and Jia, Kunyang and Zhou, Jingren and Xiao, Yanghua and Yang, Hongxia},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3357384.3358014},
isbn = {9781450369763},
keywords = {bayesian model,graph embedding,knowledge graph},
pages = {679--688},
publisher = {Association for Computing Machinery},
series = {CIKM '19},
title = {{Bayes EMbedding (BEM): Refining Representation by Integrating Knowledge Graphs and Behavior-Specific Networks}},
url = {https://doi.org/10.1145/3357384.3358014},
year = {2019}
}
@inproceedings{ISI:000563403600018,
abstract = {Much of biomedical and healthcare data is encoded in discrete, symbolic
form such as text and medical codes. There is a wealth of expert-curated
biomedical domain knowledge stored in knowledge bases and ontologies,
but the lack of reliable methods for learning knowledge representation
has limited their usefulness in machine learning applications. While
text-based representation learning has significantly improved in recent
years through advances in natural language processing, attempts to learn
biomedical concept embeddings so far have been lacking. A recent family
of models called knowledge graph embeddings have shown promising results
on general domain knowledge graphs, and we explore their capabilities in
the biomedical domain. We train several state-of-the-art knowledge graph
embedding models on the SNOMED-CT knowledge graph, provide a benchmark
with comparison to existing methods and in-depth discussion on best
practices, and make a case for the importance of leveraging the
multi-relational nature of knowledge graphs for learning biomedical
knowledge representation. The embeddings, code, and materials will be
made available to the community(1).},
address = {209 N EIGHTH STREET, STROUDSBURG, PA 18360 USA},
annote = {19th SIGBioMed Workshop on Biomedical Language Processing (BioNLP),
ELECTR NETWORK, JUL09, 2020},
author = {Chang, David and Balazevic, Ivana and Allen, Carl and Chawla, Daniel and Brandt, Cynthia and Taylor, Richard Andrew},
booktitle = {19TH SIGBIOMED WORKSHOP ON BIOMEDICAL LANGUAGE PROCESSING (BIONLP 2020)},
isbn = {978-1-952148-09-5},
organization = {SIGBioMed},
pages = {167--176},
publisher = {ASSOC COMPUTATIONAL LINGUISTICS-ACL},
title = {{Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings}},
type = {Proceedings Paper},
year = {2020}
}
@article{Phan2020480,
abstract = {A question answering (QA) system based on natural language processing and deep learning is a prominent area and is being researched widely. The Long Short-Term Memory (LSTM) model that is a variety of Recurrent Neural Network (RNN) used to be popular in machine translation, and question answering system. However, that model still has certainly limited capabilities, so a new model named Bidirectional Encoder Representation from Transformer (BERT) emerged to solve these restrictions. BERT has more advanced features than LSTM and shows state-of-the-art results in many tasks, especially in multilingual question answering system over the past few years. Nevertheless, we tried applying multilingual BERT model for a Vietnamese QA system and found that BERT model still has certainly limitation in term of time and precision to return a Vietnamese answer. The purpose of this study is to propose a method that solved above restriction of multilingual BERT and applied for question answering system about tourism in Vietnam. Our method combined BERT and knowledge graph to enhance accurately and find quickly for an answer. We experimented our crafted QA data about Vietnam tourism on three models such as LSTM, BERT fine-tuned multilingual for QA (BERT for QA), and BERT+vnKG. As a result, our model outperformed two previous models in terms of accuracy and time. This research can also be applied to other fields such as finance, e-commerce, and so on. {\textcopyright} 2020 Science and Information Organization.},
annote = {cited By 0},
author = {Phan, T H V and Do, P},
doi = {10.14569/IJACSA.2020.0110761},
issn = {2158107X},
journal = {International Journal of Advanced Computer Science and Applications},
number = {7},
pages = {480--487},
publisher = {Science and Information Organization},
title = {{BERT+vnKG: Using deep learning and knowledge graph to improve vietnamese question answering system}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088985283&doi=10.14569%2FIJACSA.2020.0110761&partnerID=40&md5=a42476a896ca180e281da94e02c20014},
volume = {11},
year = {2020}
}
@inproceedings{Jakobs2017,
abstract = {Machine learning on linked data is strongly dependent on the selection of high quality data features to achieve good results and build reusable and gen-eralizable models. In this work, we explore the problem of representing multi-valued relations in a suitable form for machine learning while keeping the human comprehensibility of the resulting model. Specifically, we propose the use of a binary vector representation and compare it to two state of the art approaches. Our evaluation shows that the binary vector representation achieves mostly higher accuracy in comparison to standard propositionalization techniques. It also achieves comparable accuracy to a recently presented graph embeddings ap-proach, while retaining the human comprehensibility.},
annote = {cited By 0; Conference of 2017 ISWC Posters and Demonstrations and Industry Tracks, ISWC-P and D-Industry 2017 ; Conference Date: 23 October 2017 Through 25 October 2017; Conference Code:131274},
author = {Jakobs, F and Terziev, Y and Gruhn, V},
booktitle = {CEUR Workshop Proceedings},
editor = {{Song D. Nikitina N.}, Fokoue A Haase P},
issn = {16130073},
keywords = {Artificial intelligence; Data handling; Data minin,Binary vectors; Graph embeddings; High quality da,Bins},
publisher = {CEUR-WS},
title = {{Binary vector based propositionalization strategy for multivalued relations in linked data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033474590&partnerID=40&md5=1edae87c56828e2ec5898b76e10e74f0},
volume = {1963},
year = {2017}
}
@article{Sousa2020367,
abstract = {Successful biomedical relation extraction can provide evidence to researchers and clinicians about possible unknown associations between biomedical entities, advancing the current knowledge we have about those entities and their inherent mechanisms. Most biomedical relation extraction systems do not resort to external sources of knowledge, such as domain-specific ontologies. However, using deep learning methods, along with biomedical ontologies, has been recently shown to effectively advance the biomedical relation extraction field. To perform relation extraction, our deep learning system, BiOnt, employs four types of biomedical ontologies, namely, the Gene Ontology, the Human Phenotype Ontology, the Human Disease Ontology, and the Chemical Entities of Biological Interest, regarding gene-products, phenotypes, diseases, and chemical compounds, respectively. We tested our system with three data sets that represent three different types of relations of biomedical entities. BiOnt achieved, in F-score, an improvement of 4.93% points for drug-drug interactions (DDI corpus), 4.99% points for phenotype-gene relations (PGR corpus), and 2.21% points for chemical-induced disease relations (BC5CDR corpus), relatively to the state-of-the-art. The code supporting this system is available at https://github.com/lasigeBioTM/BiONT. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 42nd European Conference on IR Research, ECIR 2020 ; Conference Date: 14 April 2020 Through 17 April 2020; Conference Code:239129},
author = {Sousa, D and Couto, F M},
doi = {10.1007/978-3-030-45442-5_46},
editor = {{Jose J.M. Yilmaz E.}, Magalhaes J Martins F Castells P Ferro N Silva M J},
isbn = {9783030454418},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biomedical ontologies; Domain-specific ontologies,Data mining; Drug interactions; Extraction; Genes;,Deep learning},
pages = {367--374},
publisher = {Springer},
title = {{BiOnt: Deep learning using multiple biomedical ontologies for relation extraction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084182686&doi=10.1007%2F978-3-030-45442-5_46&partnerID=40&md5=c48db6ae693850647ab3380c7edc5a4b},
volume = {12036 LNCS},
year = {2020}
}
@article{Zhang2019,
abstract = {Distributed word representations have become an essential foundation for biomedical natural language processing (BioNLP), text mining and information retrieval. Word embeddings are traditionally computed at the word level from a large corpus of unlabeled text, ignoring the information present in the internal structure of words or any information available in domain specific structured resources such as ontologies. However, such information holds potentials for greatly improving the quality of the word representation, as suggested in some recent studies in the general domain. Here we present BioWordVec: an open set of biomedical word vectors/embeddings that combines subword information from unlabeled biomedical text with a widely-used biomedical controlled vocabulary called Medical Subject Headings (MeSH). We assess both the validity and utility of our generated word embeddings over multiple NLP tasks in the biomedical domain. Our benchmarking results demonstrate that our word embeddings can result in significantly improved performance over the previous state of the art in those challenging tasks. {\textcopyright} 2019, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.},
annote = {cited By 21},
author = {Zhang, Y and Chen, Q and Yang, Z and Lin, H and Lu, Z},
doi = {10.1038/s41597-019-0055-0},
issn = {20524463},
journal = {Scientific Data},
keywords = {Algorithms; Data Mining; Medical Subject Headings,algorithm; data mining; Medical Subject Headings;},
number = {1},
publisher = {Nature Research},
title = {{BioWordVec, improving biomedical word embeddings with subword information and MeSH}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065955637&doi=10.1038%2Fs41597-019-0055-0&partnerID=40&md5=4d7e81e321204764de17db847646af08},
volume = {6},
year = {2019}
}
@article{Lamurias2019,
abstract = {Background: Recent studies have proposed deep learning techniques, namely recurrent neural networks, to improve biomedical text mining tasks. However, these techniques rarely take advantage of existing domain-specific resources, such as ontologies. In Life and Health Sciences there is a vast and valuable set of such resources publicly available, which are continuously being updated. Biomedical ontologies are nowadays a mainstream approach to formalize existing knowledge about entities, such as genes, chemicals, phenotypes, and disorders. These resources contain supplementary information that may not be yet encoded in training data, particularly in domains with limited labeled data. Results: We propose a new model to detect and classify relations in text, BO-LSTM, that takes advantage of domain-specific ontologies, by representing each entity as the sequence of its ancestors in the ontology. We implemented BO-LSTM as a recurrent neural network with long short-term memory units and using open biomedical ontologies, specifically Chemical Entities of Biological Interest (ChEBI), Human Phenotype, and Gene Ontology. We assessed the performance of BO-LSTM with drug-drug interactions mentioned in a publicly available corpus from an international challenge, composed of 792 drug descriptions and 233 scientific abstracts. By using the domain-specific ontology in addition to word embeddings and WordNet, BO-LSTM improved the F1-score of both the detection and classification of drug-drug interactions, particularly in a document set with a limited number of annotations. We adapted an existing DDI extraction model with our ontology-based method, obtaining a higher F1 score than the original model. Furthermore, we developed and made available a corpus of 228 abstracts annotated with relations between genes and phenotypes, and demonstrated how BO-LSTM can be applied to other types of relations. Conclusions: Our findings demonstrate that besides the high performance of current deep learning techniques, domain-specific ontologies can still be useful to mitigate the lack of labeled data. {\textcopyright} 2019 The Author(s).},
annote = {cited By 10},
author = {Lamurias, A and Sousa, D and Clarke, L A and Couto, F M},
doi = {10.1186/s12859-018-2584-5},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Abstracting; Brain; Data mining; Deep learning; Ex,Biological Ontologies; Data Mining; Databases,Biomedical ontologies; Biomedical text minings; D,Drug interactions,Factual; Deep Learning; Drug Interactions; Gene O,Short-Term; Natural Language Processing; Neural N,artificial neural network; biological ontology; d},
number = {1},
publisher = {BioMed Central Ltd.},
title = {{BO-LSTM: Classifying relations via long short-term memory networks along biomedical ontologies}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059594576&doi=10.1186%2Fs12859-018-2584-5&partnerID=40&md5=23a545dee0c29702af8ddb6e18a00a52},
volume = {20},
year = {2019}
}
@article{Xu2020206,
abstract = {Multilingual knowledge graph (KG) embeddings have attracted many researchers, and benefit lots of cross-lingual tasks. The cross-lingual entity alignment task is to match equivalent entities in different languages, which can largely enrich the multilingual KGs. Many previous methods consider solely the use of structures to encode entities. However, lots of multilingual KGs provide rich entity descriptions. In this paper, we mainly focus on how to utilize these descriptions to boost the cross-lingual entity alignment. Specifically, we propose two textual embedding models called Cross-TextGCN and Cross-TextMatch to embed description for each entity. Our experiments on DBP15K show that these two textual embedding model can indeed boost the structure based cross-lingual entity alignment model. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 9th CCF International Conference on Natural Language Processing and Chinese Computing, NLPCC 2020 ; Conference Date: 14 October 2020 Through 18 October 2020; Conference Code:249929},
author = {Xu, W and Chen, C and Jia, C and Shen, Y and Ma, X and Lu, W},
doi = {10.1007/978-3-030-60457-8_17},
editor = {{Zhu X. Zhang M.}, Hong Y He R},
isbn = {9783030604561},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Alignment; Embeddings; Knowledge representation,Cross-lingual; Knowledge graphs; Structure-based,Natural language processing systems},
pages = {206--218},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Boosting Cross-lingual Entity Alignment with Textual Embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093088831&doi=10.1007%2F978-3-030-60457-8_17&partnerID=40&md5=abcaf66a96d6e9c91ae9a52fb09ce612},
volume = {12431 LNAI},
year = {2020}
}

@article{9031710,
abstract = {Pedagogical (Tutor or Tutoring) Models are an important element of Intelligent Tutoring Systems (ITS) and they can be described by sets of (tutoring) rules. The implementation of a Tutoring Model includes both the formal representation of the aforementioned rules and a mechanism able to interpret such representation and execute the rules. One of the most suitable approaches to formally represent pedagogical rules is to construct semantic web ontologies that are highly interoperable and can be integrated with other models in an ITS like the subject domain and the student model. However, the main drawback of semantic web-based approaches is that they require a considerable human effort to prepare and build relevant ontologies. This paper proposes a novel approach to maintain the benefits of the semantic web-based approach in representing pedagogical rules for an ITS, while overcoming its main drawback by employing a data mining technique to automatically extract rules from real-world tutoring sessions and represent them by means of Web Ontology Language (OWL).},
author = {Chang, M and D'Aniello, G and Gaeta, M and Orciuoli, F and Sampson, D and Simonelli, C},
doi = {10.1109/ACCESS.2020.2979281},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {data mining;intelligent tutoring systems;knowledge},
pages = {48151--48162},
title = {{Building Ontology-Driven Tutoring Models for Intelligent Tutoring Systems Using Data Mining}},
volume = {8},
year = {2020}
}
@article{ISI:000488929400004,
abstract = {Modern data mining algorithms frequently need to address the task of
learning from heterogeneous data, including various sources of
background knowledge. A data mining task where ontologies are used as
background knowledge in data analysis is referred to as semantic data
mining. A specific semantic data mining task is semantic subgroup
discovery: a rule learning approach enabling ontology terms to be used
in subgroup descriptions learned from class labeled data. This paper
presents Community-Based Semantic Subgroup Discovery (CBSSD), a novel
approach that advances ontology-based subgroup identification by
exploiting the structural properties of induced complex networks related
to the studied phenomenon. Following the idea of multi-view learning,
using different sources of information to obtain better models, the
CBSSD approach can leverage different types of nodes of the induced
complex network, simultaneously using information from multiple levels
of a biological system. The approach was tested on ten data sets
consisting of genes related to complex diseases, as well as core
metabolic processes. The experimental results demonstrate that the CBSSD
approach is scalable, applicable to large complex networks, and that it
can be used to identify significant combinations of terms, which can not
be uncovered by contemporary term enrichment analysis approaches.},
address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
author = {Skrlj, Blaz and Kralj, Jan and Lavrac, Nada},
doi = {10.1007/s10844-019-00545-0},
issn = {0925-9902},
journal = {JOURNAL OF INTELLIGENT INFORMATION SYSTEMS},
keywords = {Semantic data mining; Ontologies; Community detect},
month = {oct},
number = {2},
pages = {265--304},
publisher = {SPRINGER},
title = {{CBSSD: community-based semantic subgroup discovery}},
type = {Article},
volume = {53},
year = {2019}
}
@inproceedings{9067631,
abstract = {Chinese text classification is an important task in data mining, which extracts category features from unstructured contents. Conventional Chinese text classification models only leverage the surface features in the original text, which omits the potential extensional knowledge of each word. To capture the semantic features of each word more comprehensive, this paper proposed a Chinese news text classification algorithm based on an online knowledge extension and convolutional neural network (OKE-CNN), which leverages both knowledge graph to extend latent semantic information and CNN to obtain the category. Compared with other baseline methods, OKE-CNN can utilize the surface and latent features, simultaneously, which can be adapted to complex scenes, e.g., sparse data and unclear topics. In our experiment, OKE-CNN exhibits superior performance and achieves 97.94% and 87.03% on THUCNews and TouTiao datasets, separately, over SOTA competitors.},
author = {HE, C and ZHANG, C and HU, S and TAN, Z and ZHU, H and GE, B},
booktitle = {2019 16th International Computer Conference on Wavelet Active Media Technology and Information Processing},
doi = {10.1109/ICCWAMTIP47768.2019.9067631},
issn = {2576-8964},
keywords = {convolutional neural nets;data mining;pattern clas},
month = {dec},
pages = {204--211},
title = {{Chinese News Text Classification Algorithm Based on Online Knowledge Extension and Convolutional Neural Network}},
year = {2019}
}
@inproceedings{Buscaldi201750,
abstract = {The ScienceIE task at SemEval-2017 introduced an epistemological classification of keyphrases in scientific publications, suggesting that research activities revolve around the key concepts of process (methods and systems), material (data and physical resources) and task. In this paper we present a method for the classification of keyphrases according to the ScienceIE classification, using WordNet and word embeddings derived features. The method outperforms the best system at SemEval-2017, although our experiments highlighted some issues with the collection.},
annote = {cited By 0; Conference of Actes du 1er atelier Valorisation et Analyse des Donnees de la Recherche, VADOR 2017 - 1st Workshop on Valorization and Analysis of Research Data, VADOR 2017 ; Conference Date: 31 May 2017; Conference Code:128247},
author = {Buscaldi, D and Hernandez, S D and Charnois, T},
booktitle = {CEUR Workshop Proceedings},
editor = {{Kergosien E. Bessagnet M.-N.}, Schopfel J Cabanac G},
issn = {16130073},
keywords = {Data mining,Derived features; Embeddings; Keyphrase extractio,Information retrieval; Ontology},
pages = {50--57},
publisher = {CEUR-WS},
title = {{Classification of keyphrases from scientific publications using wordnet and word embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022091034&partnerID=40&md5=034e0df01b158febaae285443e4659c9},
volume = {1860},
year = {2017}
}
@article{Valdez201830,
abstract = {Scientific reproducibility is key to the advancement of science as researchers can build on sound and validated results to design new research studies. However, recent studies in biomedical research have highlighted key challenges in scientific reproducibility as more than 70% of researchers in a survey of more than 1500 participants were not able to reproduce results from other groups and 50% of researchers were not able to reproduce their own experiments. Provenance metadata is a key component of scientific reproducibility and as part of the Provenance for Clinical and Health Research (ProvCaRe) project, we have: (1) identified and modeled important provenance terms associated with a biomedical research study in the S3 model (formalized in the ProvCaRe ontology); (2) developed a new natural language processing (NLP) workflow to identify and extract provenance metadata from published articles describing biomedical research studies; and (3) developed the ProvCaRe knowledge repository to enable users to query and explore provenance of research studies using the S3 model. However, a key challenge in this project is the automated classification of provenance metadata extracted by the NLP workflow according to the S3 model and its subsequent querying in the ProvCaRe knowledge repository. In this paper, we describe the development and comparative evaluation of deep learning techniques for multi-class classification of structured provenance metadata extracted from biomedical literature using 12 different categories of provenance terms represented in the S3 model. We describe the application of the Long Term Short Memory (LSTM) network, which has the highest classification accuracy of 86% in our evaluation, to classify more than 48 million provenance triples in the ProvCaRe knowledge repository (available at: https://provcare.case.edu/). {\textcopyright} Springer Nature Switzerland AG 2018.},
annote = {cited By 1; Conference of 7th International Provenance and Annotation Workshop, IPAW 2018 ; Conference Date: 9 July 2018 Through 10 July 2018; Conference Code:218129},
author = {Valdez, J and Kim, M and Rueschman, M and Redline, S and Sahoo, S S},
doi = {10.1007/978-3-319-98379-0_3},
editor = {{Belhajjame K. Gehani A.}, Alper P},
isbn = {9783319983783},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automated classification; Biomedical literature;,Clinical research; Long short-term memory; Metadat,Deep learning},
pages = {30--41},
publisher = {Springer Verlag},
title = {{Classification of provenance triples for scientific reproducibility: A comparative evaluation of deep learning models in the provcare project}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053837321&doi=10.1007%2F978-3-319-98379-0_3&partnerID=40&md5=edccfb1c8374928a2f7934dc5e296cde},
volume = {11017 LNCS},
year = {2018}
}
@inproceedings{8952314,
abstract = {Coding convention plays an important role in guaranteeing software quality. However, coding conventions are usually informally presented and inconvenient for programmers to use. In this paper, we present CocoQa, a system that answers programmer's questions about coding conventions. CocoQa answers questions by querying a knowledge graph for coding conventions. It employs 1) a subgraph matching algorithm that parses the question into a SPARQL query, and 2) a machine comprehension algorithm that uses an end-to-end neural network to detect answers from searched paragraphs. We have implemented CocoQa, and evaluated it on a coding convention QA dataset. The results show that CocoQa can answer questions about coding conventions precisely. In particular, CocoQa can achieve a precision of 82.92% and a recall of 91.10%. Repository: https://github.com/14dtj/CocoQa/ Video: https://youtu.be/VQaXi1WydAU.},
author = {Du, T and Cao, J and Wu, Q and Li, W and Shen, B and Chen, Y},
booktitle = {2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
doi = {10.1109/ASE.2019.00108},
issn = {2643-1572},
keywords = {graph theory;neural nets;query processing;question},
month = {nov},
pages = {1086--1089},
title = {{CocoQa: Question Answering for Coding Conventions Over Knowledge Graphs}},
year = {2019}
}
@article{Jia2020,
abstract = {Entity linking is a fundamental task in natural language processing. The task of entity linking with knowledge graphs aims at linking mentions in text to their correct entities in a knowledge graph like DBpedia or YAGO2. Most of existing methods rely on hand-designed features to model the contexts of mentions and entities, which are sparse and hard to calibrate. In this paper, we present a neural model that first combines co-attention mechanism with graph convolutional network for entity linking with knowledge graphs, which extracts features of mentions and entities from their contexts automatically. Specifically, given the context of a mention and one of its candidate entities' context, we introduce the co-attention mechanism to learn the relatedness between the mention context and the candidate entity context, and build the mention representation in consideration of such relatedness. Moreover, we propose a context-aware graph convolutional network for entity representation, which takes both the graph structure of the candidate entity and its relatedness with the mention context into consideration. Experimental results show that our model consistently outperforms the baseline methods on five widely used datasets. {\textcopyright} 2020 John Wiley \& Sons Ltd},
annote = {cited By 0},
author = {Jia, N and Cheng, X and Su, S and Ding, L},
doi = {10.1111/exsy.12606},
issn = {02664720},
journal = {Expert Systems},
keywords = {Attention mechanisms; Baseline methods; Context-A,Convolution; Graph structures; Knowledge represent,Convolutional neural networks},
publisher = {Blackwell Publishing Ltd},
title = {{CoGCN: Combining co-attention with graph convolutional network for entity linking with knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089248180&doi=10.1111%2Fexsy.12606&partnerID=40&md5=a1fae4689af755a2ea63a664542f010a},
year = {2020}
}
@inproceedings{Zhang2016353,
abstract = {Among different recommendation techniques, collaborative filtering usually suffer from limited performance due to the sparsity of user-item interactions. To address the issues, auxiliary information is usually used to boost the performance. Due to the rapid collection of information on the web, the knowledge base provides heterogeneous information including both structured and unstructured data with different semantics, which can be consumed by various applications. In this paper, we investigate how to leverage the heterogeneous information in a knowledge base to improve the quality of recommender systems. First, by exploiting the knowledge base, we design three components to extract items' semantic representations from structural content, textual content and visual content, respectively. To be specific, we adopt a heterogeneous network embedding method, termed as TransR, to extract items' structural representations by considering the heterogeneity of both nodes and relationships. We apply stacked denoising auto-encoders and stacked convolutional auto-encoders, which are two types of deep learning based embedding techniques, to extract items' textual representations and visual representations, respectively. Finally, we propose our final integrated framework, which is termed as Collaborative Knowledge Base Embedding (CKE), to jointly learn the latent representations in collaborative filtering as well as items' semantic representations from the knowledge base. To evaluate the performance of each embedding component as well as the whole system, we conduct extensive experiments with two realworld datasets from different scenarios. The results reveal that our approaches outperform several widely adopted state-of-the-art recommendation methods. {\textcopyright} 2016 ACM.},
annote = {cited By 334; Conference of 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2016 ; Conference Date: 13 August 2016 Through 17 August 2016; Conference Code:123286},
author = {Zhang, F and Yuan, N J and Lian, D and Xie, X and Ma, W.-Y.},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2939672.2939673},
isbn = {9781450342322},
keywords = {Collaborative filtering; Heterogeneous networks; K,Collaborative knowledge; Heterogeneous informatio,Data mining},
pages = {353--362},
publisher = {Association for Computing Machinery},
title = {{Collaborative knowledge base embedding for recommender systems}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984981976&doi=10.1145%2F2939672.2939673&partnerID=40&md5=cd078596b074245b1105e2d6dac204ae},
volume = {13-17-Augu},
year = {2016}
}
@inproceedings{10.1145/2682571.2797067,
abstract = {Digital Humanities make more and more structured and richly annotated corpora available. Most of this data rely on well known and established standards, such as TEI, which especially enable scientists to edit and publish their work. However, one of the remaining problems is to give adequate access to this rich data, in order to produce higher-order knowledge.In this paper, we present an integrated environment combining an advanced search engine and text-mining techniques for hermeneutics in Digital Humanities. Relying on semantic web technologies, the search engine uses full text as well as complex embedding structures and offers a single interface to access rich and heterogeneous data and meta-data. Text-mining possibilities enable scholars to exhibit regularities in corpora. Results obtained on the Cartesian corpus illustrate these principles and tools.},
address = {New York, NY, USA},
author = {Widlocher, Antoine and Bechet, Nicolas and Lecarpentier, Jean-Marc and Mathet, Yann and Roger, Julia},
booktitle = {Proceedings of the 2015 ACM Symposium on Document Engineering},
doi = {10.1145/2682571.2797067},
isbn = {9781450333078},
keywords = {digital humanities,information retrieval,text-mining},
pages = {157--166},
publisher = {Association for Computing Machinery},
series = {DocEng '15},
title = {{Combining Advanced Information Retrieval and Text-Mining for Digital Humanities}},
url = {https://doi.org/10.1145/2682571.2797067},
year = {2015}
}
@article{Li2020256,
abstract = {With the popularity of mobile devices, large amounts of mobile applications (a.k.a.“app”) have been developed and published. Detecting similar apps from a large pool of apps is a fundamental and important task because it has many benefits for various purposes. There exist several works that try to combine different metadata of apps for measuring the similarity between apps. However, few of them pay attention to the roles of this service. Besides, existing methods do not distinguish the characters of contents in the metadata. Therefore, it is hard to obtain accurate semantic representations of apps and capture their fine-grained correlations. In this paper, we propose a novel framework by knowledge graph (KG) techniques and a hybrid embedding strategy to fill above gaps. For the construction of KG, we design a lightweight ontology tailored for the service of cybersecurity analysts. Benefited from a defined schema, more linkages can be shared among apps. To detect similar apps, we divide the relations in KG into structured and unstructured ones according to their related content. Then, TextRank algorithm is employed to extract important tokens from unstructured texts and transform them into structured triples. In this way, the representations of apps in our framework can be iteratively learned by combining KG embedding methods and network embedding models for improving the performance of similar apps detection. Preliminary results indicate the effectiveness of our method comparing to existing models in terms of reciprocal ranking and minimum ranking. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 9th CCF International Conference on Natural Language Processing and Chinese Computing, NLPCC 2020 ; Conference Date: 14 October 2020 Through 18 October 2020; Conference Code:249929},
author = {Li, W and Zhang, B and Xu, L and Wang, M and Luo, A and Niu, Y},
doi = {10.1007/978-3-030-60450-9_21},
editor = {{Zhu X. Zhang M.}, Hong Y He R},
isbn = {9783030604493},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Embedding method; Embedding strategies; Knowledge,Embeddings,Iterative methods; Knowledge representation; Metad},
pages = {256--269},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Combining Knowledge Graph Embedding and Network Embedding for Detecting Similar Mobile Applications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093101846&doi=10.1007%2F978-3-030-60450-9_21&partnerID=40&md5=d5671a86298ad1ac1128a9f544edc26e},
volume = {12430 LNAI},
year = {2020}
}
@inproceedings{Nikolov2018977,
abstract = {Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications. {\textcopyright} 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License.},
annote = {cited By 1; Conference of 27th International World Wide Web, WWW 2018 ; Conference Date: 23 April 2018 Through 27 April 2018; Conference Code:159682},
author = {Nikolov, A and Haase, P and Herzig, D M and Trame, J and Kozlov, A},
booktitle = {The Web Conference 2018 - Companion of the World Wide Web Conference, WWW 2018},
doi = {10.1145/3184558.3191527},
isbn = {9781450356404},
keywords = {Data mining,Embeddings; Knowledge based systems; Vector spaces,Explicit information; Implicit informations; Know},
pages = {977--980},
publisher = {Association for Computing Machinery, Inc},
title = {{Combining RDF Graph Data and Embedding Models for an Augmented Knowledge Graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074265515&doi=10.1145%2F3184558.3191527&partnerID=40&md5=8b6cd4f786b1db8b16ac72daa7badbbf},
year = {2018}
}
@inproceedings{Ferré20193443,
abstract = {In this paper, we propose a two-step method to normalize multi-word terms with concepts from a domain-specific ontology. Normalization is a critical step of information extraction. The method uses vector representations of terms computed with word embedding information and hierarchical information among ontology concepts. A training dataset and a first result dataset with high precision and low recall are generated by using the ToMap unsupervised normalization method. It is based on the similarities between the form of the term to normalize and the form of concept labels. Then, a projection of the space of terms towards the space of concepts is learned by globally minimizing the distances between vectors of terms and vectors of concepts. It applies multivariate linear regression using the previously generated training dataset. Finally, a distance calculation is carried out between the projections of term vectors and the concept vectors, providing a prediction of normalization by a concept for each term. This method was evaluated through the categorization task of bacterial habitats of BioNLP Shared Task 2016. Our results largely outperform all existing systems on this task, opening up very encouraging prospects. {\textcopyright} LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.},
annote = {cited By 1; Conference of 11th International Conference on Language Resources and Evaluation, LREC 2018 ; Conference Date: 7 May 2018 Through 12 May 2018; Conference Code:143414},
author = {Ferr{\'{e}}, A and Del{\'{e}}ger, L and Zweigenbaum, P and N{\'{e}}dellec, C},
booktitle = {LREC 2018 - 11th International Conference on Language Resources and Evaluation},
editor = {{Isahara H. Maegaard B.}, Piperidis S Cieri C Declerck T Hasida K Mazo H Choukri K Goggi S Mariani J Moreno A Calzolari N Odijk J Tokunaga T},
isbn = {9791095546009},
keywords = {Data mining; Ontology; Semantics; Vectors,Distributional semantics; Domain-specific ontolog,Vector spaces},
pages = {3443--3447},
publisher = {European Language Resources Association (ELRA)},
title = {{Combining rule-based and embedding-based approaches to normalize textual entities with an ontology}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059879298&partnerID=40&md5=4e5ee37b2727b2f7fa929d13a9ed6c45},
year = {2019}
}
@inproceedings{8334467,
abstract = {Word embedding is becoming more popular in the Semantic Web community as an effective approach for capturing semantics in various contexts. In this paper, we combine word embedding and topic modeling to model RDF data for the entity summarization task. In our model, ES-LDAext, which is the extended version of our previous model, we utilize the word embedding to supplement the RDF data before applying entity summarization. In addition, in the model presented here, we use RDF literals as a very good source of information to create more reliable and representative summaries for entities. To do that, we use the Named Entity Recognition approach to extract entities within literals before feeding them into the word embedding model to enrich the RDF data. Experimental results demonstrate the effectiveness of the proposed model.},
author = {Pouriyeh, S and Allahyari, M and Kochut, K and Cheng, G and Arabnia, H R},
booktitle = {2018 IEEE 12th International Conference on Semantic Computing (ICSC)},
doi = {10.1109/ICSC.2018.00044},
keywords = {-topic-model,-word-embedding,knowledge based systems;semantic Web;text analysis},
month = {jan},
pages = {252--255},
title = {{Combining Word Embedding and Knowledge-Based Topic Modeling for Entity Summarization}},
year = {2018}
}
@article{9179107,
abstract = {Stance classification aims at identifying, in the text, the attitude toward the given targets as favorable, negative, or unrelated. In existing models for stance classification, only textual representation is leveraged, while commonsense knowledge is ignored. In order to better incorporate commonsense knowledge into stance classification, we propose a novel model named commonsense knowledge enhanced memory network, which jointly represents textual and commonsense knowledge representation of given target and text. The textual memory module in our model treats the textual representation as memory vectors, and uses attention mechanism to embody the important parts. For commonsense knowledge memory module, we jointly leverage the entity and relation embeddings learned by TransE model to take full advantage of constraints of the knowledge graph. Experimental results on the SemEval dataset show that the combination of the commonsense knowledge memory and textual memory can improve stance classification.},
author = {Du, J and Gui, L and Xu, R and Xia, Y and Wang, X},
doi = {10.1109/MIS.2020.2983497},
issn = {1941-1294},
journal = {IEEE Intelligent Systems},
keywords = {common-sense reasoning;graph theory;knowledge repr},
month = {jul},
number = {4},
pages = {102--109},
title = {{Commonsense Knowledge Enhanced Memory Network for Stance Classification}},
volume = {35},
year = {2020}
}
@inproceedings{10.1007/978-3-319-34129-3_14,
abstract = {When modeling Linked Open Data LOD, reusing appropriate vocabulary terms to represent the data is difficult, because there are many vocabularies to choose from. Vocabulary term recommendations could alleviate this situation. We present a user study evaluating a vocabulary term recommendation service that is based on how other data providers have used RDF classes and properties in the LOD cloud. Our study compares the machine learning technique Learning to Rank L2R, the classical data mining approach Association Rule mining AR, and a baseline that does not provide any recommendations. Results show that utilizing AR, participants needed less time and less effort to model the data, which in the end resulted in models of better quality.},
address = {Berlin, Heidelberg},
author = {Schaible, Johann and Szekely, Pedro and Scherp, Ansgar},
booktitle = {Proceedings of the 13th International Conference on The Semantic Web. Latest Advances and New Domains - Volume 9678},
doi = {10.1007/978-3-319-34129-3_14},
isbn = {9783319341286},
pages = {214--230},
publisher = {Springer-Verlag},
title = {{Comparing Vocabulary Term Recommendations Using Association Rules and Learning to Rank: A User Study}},
url = {https://doi.org/10.1007/978-3-319-34129-3_14},
year = {2016}
}
@inproceedings{10.1145/3394486.3403123,
abstract = {Clinical trials play important roles in drug development but often suffer from expensive, inaccurate and insufficient patient recruitment. The availability of massive electronic health records (EHR) data and trial eligibility criteria (EC) bring a new opportunity to data driven patient recruitment. One key task named patient-trial matching is to find qualified patients for clinical trials given structured EHR and unstructured EC text (both inclusion and exclusion criteria). How to match complex EC text with longitudinal patient EHRs? How to embed many-to-many relationships between patients and trials? How to explicitly handle the difference between inclusion and exclusion criteria? In this paper, we proposed CrOss-Modal PseudO-SiamEse network (COMPOSE) to address these challenges for patient-trial matching. One path of the network encodes EC using convolutional highway network. The other path processes EHR with multi-granularity memory network that encodes structured patient records into multiple levels based on medical ontology. Using the EC embedding as query, COMPOSE performs attentional record alignment and thus enables dynamic patient-trial matching. COMPOSE also introduces a composite loss term to maximize the similarity between patient records and inclusion criteria while minimize the similarity to the exclusion criteria. Experiment results show COMPOSE can reach 98.0% AUC on patient-criteria matching and 83.7% accuracy on patient-trial matching, which leads 24.3% improvement over the best baseline on real-world patient-trial matching tasks.},
address = {New York, NY, USA},
author = {Gao, Junyi and Xiao, Cao and Glass, Lucas M and Sun, Jimeng},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3394486.3403123},
isbn = {9781450379984},
keywords = {Cross-Modal learning,Pseudo-Siamese network,trial recruitment},
pages = {803--812},
publisher = {Association for Computing Machinery},
series = {KDD '20},
title = {{COMPOSE: Cross-Modal Pseudo-Siamese Network for Patient Trial Matching}},
url = {https://doi.org/10.1145/3394486.3403123},
year = {2020}
}
@inproceedings{Khandelwal2020204,
abstract = {In today's world, data or information is increasing at an exponential rate, and so is the fake news. Traditional fact-checking methods like fake news detection by experts, analysts, or some organizations do not match with the volume of information available. This is where the problem of computational fact-checking or validation becomes relevant. Given a Knowledge Graph, a knowledge corpus, and a fact (triple statement), the goal of fact-checking is to decide whether the fact or knowledge is correct or not. Existing approaches extensively used several structural features of the input Knowledge Graph to address the mentioned problem. In this work, our primary focus would be to leverage the unstructured information along with the structured ones. Our approach considers finding evidence from Wikipedia and structured information from Wikidata, which helps in determining the validity of the input facts. As features from the structured domain, we have used TransE embedding considering components of the input fact. The similarity of input fact with elements of relevant Wikipedia pages has been used as unstructured features. The experiments with a dataset consisting of nine relations of Wikidata has established the advantage of combining unstructured features with structured features for the given task. {\textcopyright} 2020 Association for Computing Machinery.},
annote = {cited By 0; Conference of ACM India Joint 7th ACM IKDD Conference on Data Science and 25th International Conference on Management of Data, CoDS-COMAD 2020 ; Conference Date: 5 January 2020 Through 7 January 2020; Conference Code:156814},
author = {Khandelwal, S and Kumar, D},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3371158.3371187},
isbn = {9781450377386},
keywords = {Data mining,Exponential rates; Knowledge graphs; RDF triples;,Knowledge management; Knowledge representation; Su},
pages = {204--208},
publisher = {Association for Computing Machinery},
title = {{Computational fact validation from knowledge graph using structured and unstructured information}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078415141&doi=10.1145%2F3371158.3371187&partnerID=40&md5=d30dd2b7121485455d7b9a4174496f30},
year = {2020}
}
@inproceedings{Wajnberg2019,
abstract = {Linked data (LD) is a rich format increasingly exploited in knowledge discovery from data (KDD). To that end, LD is typically structured as graph, but can also fit the multi-relational data mining (MRDM) paradigm, e.g. as multiple types and object properties may be used in the dataset. Formal concept analysis (FCA) has been successfully used as theoretical framework for KDD in a variety of applications, primely in clustering and association rule mining (ARM) tasks. As FCA applicability to LD is limited by its single data table input format, relational concept analysis (RCA) was introduced as a MRDM extension that successfully deals with links in the data, including cyclic ones. While RCA has been mainly adapted for conceptual clustering in the past, we present here an RCA-based ARM method. It exploits the iterative nature of pattern generation to cut cyclic references with a minimal loss of information. The utility of the rules discovered by our method has been validated by an application as a decision support in the aluminum die casting industry. Copyright {\textcopyright} 2019 for this paper by its authors.},
annote = {cited By 0; Conference of 2019 Joint Ontology Workshops Episode V: The Styrian Autumn of Ontology, JOWO 2019 ; Conference Date: 23 September 2019 Through 25 September 2019; Conference Code:155856},
author = {Wajnberg, M and Valtchev, P and Lezoche, M and {Blondin Mass{\'{e}}}, A and Panetto, H},
booktitle = {CEUR Workshop Proceedings},
editor = {{Barton A. Seppala S.}, Porello D},
issn = {16130073},
keywords = {Association rule minings (ARM); Conceptual cluste,Association rules; Data mining; Decision making; D,Formal concept analysis},
publisher = {CEUR-WS},
title = {{Concept analysis-based association mining from linked data: A case in industrial decision making}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077703960&partnerID=40&md5=5cb12dbcdb07c33142b9d95febb03282},
volume = {2518},
year = {2019}
}
@article{Zhao2020198,
abstract = {Protein-protein interactions (PPIs) are pivotal for cellular functions and biological processes. In the past years, computational methods using amino acid sequences and gene ontology (GO) annotations of proteins for prioritizing PPIs have provided important references for biological experiments in the wet lab. Despite the current success, sequence information and ontological annotation in semantic representation have not been integrated into current methods. We propose a deep-learning-based PPI prediction methodology conjointly featuring sequence information and GO annotation. First, we adopt a word-embedding tool, the NCBI-blueBERT model pre-trained on PubMed, to map the GO terms into their semantic vectors. Then, the GO semantic vectors and protein sequence vector serve as the input of the proposed inception recurrent neural network (RNN) attention network (IRAN). The IRAN captures the spatial relationship and the potential sequential feature of the protein sequence and ontological annotation semantics. The extensive experimental results on 12 benchmarks demonstrate that our method achieves superiority over state-of-the-art baselines. In the yeast dataset of a binary PPI prediction, our method improved the performance with the Matthews correlation coefficient increasing from 94.2% to 98.2% and the accuracy from 97.1% to 98.2%. The analogous results were also obtained in other comparison evaluations. {\textcopyright} 2020 The Authors
The identification and characterization of protein-protein interactions (PPIs) are considered pivotal to understand the mechanisms of biological processes. This study attempts to infer PPIs by fusing gene ontology annotation and raw sequences of protein based on deep learning and natural language processing techniques, providing better PPI prediction performance. {\textcopyright} 2020 The Authors},
annote = {cited By 0},
author = {Zhao, L and Wang, J and Hu, Y and Cheng, L},
doi = {10.1016/j.omtn.2020.08.025},
issn = {21622531},
journal = {Molecular Therapy - Nucleic Acids},
keywords = {amino acid sequence; article; attention network; c},
pages = {198--208},
publisher = {Cell Press},
title = {{Conjoint Feature Representation of GO and Protein Sequence for PPI Prediction Based on an Inception RNN Attention Network}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091088704&doi=10.1016%2Fj.omtn.2020.08.025&partnerID=40&md5=85d5dc4c5081ddc991053aa18176ba83},
volume = {22},
year = {2020}
}
@inproceedings{Zhang20171223,
abstract = {Knowledge graphs (KGs) have been widely used to represent relationships among entities, while KGs cannot capture new relationships between entities emerging along time. Since news often provides the latest information regarding the new entities and relationships, there is an opportunity to connect emerging relationships from news timely. However, it is a challenging task due to the source heterogeneity of structured KGs and unstructured news texts. In order to address the issue, we propose a tensor-based framework to capture the complex interactions among multiple types of relations, entities and text descriptions. We further develop an efficient Text-Aware MUlti-RElational learning method (TAMURE) that can learn the embedding representations of entities and relation types from both KGs and news, by jointly factorizing the interaction parameters. Furthermore, the complexity of TAMURE is linear in the number of parameters, which makes it suitable to large-scale KGs and news texts. Extensive experiments via TensorFlow demonstrate the effectiveness of the proposed TAMURE model compared with nine state-of-the-art methods on real-world datasets. {\textcopyright} 2017 IEEE.},
annote = {cited By 2; Conference of 5th IEEE International Conference on Big Data, Big Data 2017 ; Conference Date: 11 December 2017 Through 14 December 2017; Conference Code:134260},
author = {Zhang, J and Lu, C.-T. and Cao, B and Chang, Y and Yu, P S},
booktitle = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
doi = {10.1109/BigData.2017.8258048},
editor = {{Nie J.-Y. Obradovic Z.}, Suzumura T Ghosh R Nambiar R Wang C Zang H Baeza-Yates R Baeza-Yates R Hu X Kepner J Cuzzocrea A Tang J Toyoda M},
isbn = {9781538627143},
keywords = {Big data; Embeddings; Factorization; Information r,Embedding; Emerging Relationships; Interaction pa,Learning systems},
pages = {1223--1232},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Connecting emerging relationships from news via tensor factorization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047845832&doi=10.1109%2FBigData.2017.8258048&partnerID=40&md5=16fe5690b127ff248566d686ec4f739f},
volume = {2018-Janua},
year = {2017}
}
@inproceedings{Fu2019,
abstract = {Representation learning aims to encode knowledge graphs into a low-dimensional vector space. However, since both the out-degree and in-degree of the entities in the knowledge graph follow the power-law distribution, only a small number of entities with higher frequency play a key role in training process, while the others have less effect. This leads to a more serious problem of data sparsity. In this paper, we propose two knowledge graph representation learning models for sub-graph structure fusion. Based on the original model, our models are trained by incorporating the sub-graph structure during training process. By incorporating the sub-graph information into the representation learning model, the structural associations between different relations can be incorporated while considering the semantic association between entities. So that the entities and relations can be modeled more accurately, and the data sparsity problem of the knowledge graph representation learning model can be alleviated effectively. {\textcopyright} Published under licence by IOP Publishing Ltd.},
annote = {cited By 0; Conference of 2nd International Conference on Computer Information Science and Application Technology, CISAT 2019 ; Conference Date: 30 August 2019 Through 1 September 2019; Conference Code:156052},
author = {Fu, W and Yang, X and Sun, X},
booktitle = {Journal of Physics: Conference Series},
doi = {10.1088/1742-6596/1345/2/022011},
issn = {17426588},
keywords = {Data mining; Graphic methods; Semantics; Vector sp,Data sparsity problems; Higher frequencies; Knowl,Learning systems},
number = {2},
publisher = {Institute of Physics Publishing},
title = {{Considering Sub-graph Structure in Representation Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077323603&doi=10.1088%2F1742-6596%2F1345%2F2%2F022011&partnerID=40&md5=6657904422d4f2b9aed61a5fd97eb08d},
volume = {1345},
year = {2019}
}
@article{Restrepo-Arango2017184,
abstract = {News content usually refers to specific facts, people or situations. Understanding the whole relationships and context about the actual text may require the user to manually connect, search and filter other sources, with a considerable effort. This work considers the use of deep learning techniques to analyze the news content to automatically build context and ultimately to provide a valuable solution for news readers and news editors using a real dataset from the most important online newspaper. Using a news article as a seed, we relate and add valuable information to news articles, providing understanding and comprehensiveness to put information into users' perspective, based on semantic, unobvious and time changing relationships. Context is constructed by modeling news and ontological information using deep learning. Ontological information is extracted from knowledge base sources. Content In-context is a complete solution applying this approach to a Colombian real, online news dataset, produced in Spanish. Tests and results are performed considering new articles using unknown data. Results prove to be interesting compared to classical machine learning methods. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 0; Conference of 12th Colombian Conference on Computing, CCC 2017 ; Conference Date: 19 September 2017 Through 22 September 2017; Conference Code:196549},
author = {Restrepo-Arango, C and Jim{\'{e}}nez-Guar{\'{i}}n, C},
doi = {10.1007/978-3-319-66562-7_14},
editor = {{Solano A.}, Ordonez H},
isbn = {9783319665610},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Complete solutions; Contextualization; Knowledge,Deep learning; Knowledge based systems; Learning a,Learning systems},
pages = {184--198},
publisher = {Springer Verlag},
title = {{Content in-context: Automatic news contextualization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028801561&doi=10.1007%2F978-3-319-66562-7_14&partnerID=40&md5=825519bb5f9a67311a95414b0b5123f2},
volume = {735},
year = {2017}
}
@inproceedings{Jia201924,
abstract = {Entity recommendation, providing search users with an improved experience via assisting them in finding related entities for a given query, has become an indispensable feature of today's search engines. Existing studies typically only consider the queries with explicit entities. They usually fail to handle complex queries that without entities, such as "what food is good for cold weather", because their models could not infer the underlying meaning of the input text. In this work, we believe that contexts convey valuable evidence that could facilitate the semantic modeling of queries, and take them into consideration for entity recommendation. In order to better model the semantics of queries and entities, we learn the representation of queries and entities jointly with attentive deep neural networks. We evaluate our approach using large-scale, real-world search logs from a widely used commercial Chinese search engine. Our system has been deployed in ShenMa Search Engine 1and you can fetch it in UC Browser of Alibaba. Results from online A/B test suggest that the impression efficiency of click-through rate increased by 5.1% and page view increased by 5.5%. Copyright {\^{A}}ľ 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
annote = {cited By 0; Conference of 2nd International Workshop on EntitY REtrieval, EYRE 2019 ; Conference Date: 3 November 2019; Conference Code:151820},
author = {Jia, Q and Zhang, N and Hua, N},
booktitle = {CEUR Workshop Proceedings},
editor = {{Cheng G. Gunaratna K.}, Wang J},
issn = {16130073},
keywords = {Chinese search engines; Click-through rate; Conce,Deep neural networks; Knowledge management; Search,Recommender systems},
pages = {24--31},
publisher = {CEUR-WS},
title = {{Context-aware deep model for entity recommendation system in search engine at alibaba}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072723127&partnerID=40&md5=4fdaaee70a0d01326b386838a3205e57},
volume = {2446},
year = {2019}
}
@inproceedings{4QP5XAGX,
abstract = {Automatic art analysis aims to classify and retrieve artistic representations from a collection of images by using computer vision and machine learning techniques. In this work, we propose to enhance visual representations from neural networks with contextual artistic information. Whereas visual representations are able to capture information about the content and the style of an artwork, our proposed context-aware embeddings additionally encode relationships between different artistic attributes, such as author, school, or historical period. We design two different approaches for using context in automatic art analysis. In the first one, contextual data is obtained through a multi-task learning model, in which several attributes are trained together to find visual relationships between elements. In the second approach, context is obtained through an art-specific knowledge graph, which encodes relationships between artistic attributes. An exhaustive evaluation of both of our models in several art analysis problems, such as author identification, type classification, or cross-modal retrieval, show that performance is improved by up to 7.3% in art classification and 37.24% in retrieval when context-aware embeddings are used. {\textcopyright} 2019 Association for Computing Machinery.},
annote = {cited By 4; Conference of 2019 ACM International Conference on Multimedia Retrieval, ICMR 2019 ; Conference Date: 10 June 2019 Through 13 June 2019; Conference Code:148567},
author = {Garcia, Noa and Renoust, Benjamin and Nakashima, Yuta},
booktitle = {Proceedings of the 2019 on International Conference on Multimedia Retrieval},
isbn = {9781450367653},
keywords = {Arts computing; Embeddings; Encoding (symbols); Le,Author identification; Knowledge graphs; Machine,Modal analysis},
pages = {25--33},
title = {{Context-aware embeddings for automatic art analysis}},
year = {2019}
}
@article{Gao2019249,
abstract = {Schematic knowledge, as a critical ingredient of knowledge graphs, defines logical axioms based on concepts to support for eliminating heterogeneity, integration, and reasoning over knowledge graphs (KGs). Although some well-known KGs contain large scale schematic knowledge, they are far from complete, especially schematic knowledge stating that two concepts have subclassOf relations (also called subclassOf axioms) and schematic knowledge stating that two concepts are logically disjoint (also called disjointWith axioms). One of the most important characters of these axioms is their logical properties such as transitivity and symmetry. Current KG embedding models focus on encoding factual knowledge (i.e., triples) in a KG and cannot directly be applied to further schematic knowledge (i.e., axioms) completion. The main reason is that they ignore these logical properties. To solve this issue, we propose a novel model named CosE for schematic knowledge. More precisely, CosE projects each concept into two semantic spaces. One is an angle-based semantic space that is utilized to preserve transitivity or symmetry of an axiom. The other is a translation-based semantic space utilized to measure the confidence score of an axiom. Moreover, two score functions tailored for subclassOf and disjointWith are designed to learn the representation of concepts with these two relations sufficiently. We conduct extensive experiments on link prediction on benchmark datasets like YAGO and FMA ontologies. The results indicate that CosE outperforms state-of-the-art methods and successfully preserve the transitivity and symmetry of axioms. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 8th CCF International Conference on Natural Language Processing and Chinese Computing, NLPCC 2019 ; Conference Date: 9 October 2019 Through 14 October 2019; Conference Code:232669},
author = {Gao, H and Zheng, X and Li, W and Qi, G and Wang, M},
doi = {10.1007/978-3-030-32233-5_20},
editor = {{Tang J. Kan M.-Y.}, Zhao D Li S Zan H},
isbn = {9783030322328},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Benchmark datasets; Confidence score; Embedding;,Embeddings; Semantics,Natural language processing systems},
pages = {249--261},
publisher = {Springer},
title = {{Cosine-Based Embedding for Completing Schematic Knowledge}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075572131&doi=10.1007%2F978-3-030-32233-5_20&partnerID=40&md5=73412078d93286f5ca07cc813b7d01ce},
volume = {11838 LNAI},
year = {2019}
}
@article{10.5555/1672957.1672980,
abstract = {Domain ontologies play an important role in supporting knowledge-based applications in the Semantic Web. To facilitate the building of ontologies, text mining techniques have been used to perform ontology learning from texts. However, traditional systems employ shallow natural language processing techniques and focus only on concept and taxonomic relation extraction. In this paper we present a system, known as Concept-Relation-Concept Tuple-based Ontology Learning (CRCTOL), for mining ontologies automatically from domain-specific documents. Specifically, CRCTOL adopts a full text parsing technique and employs a combination of statistical and lexico-syntactic methods, including a statistical algorithm that extracts key concepts from a document collection, a word sense disambiguation algorithm that disambiguates words in the key concepts, a rule-based algorithm that extracts relations between the key concepts, and a modified generalized association rule mining algorithm that prunes unimportant relations for ontology learning. As a result, the ontologies learned by CRCTOL are more concise and contain a richer semantics in terms of the range and number of semantic relations compared with alternative systems. We present two case studies where CRCTOL is used to build a terrorism domain ontology and a sport event domain ontology. At the component level, quantitative evaluation by comparing with Text-To-Onto and its successor Text2Onto has shown that CRCTOL is able to extract concepts and semantic relations with a significantly higher level of accuracy. At the ontology level, the quality of the learned ontologies is evaluated by either employing a set of quantitative and qualitative methods including analyzing the graph structural property, comparison to WordNet, and expert rating, or directly comparing with a human-edited benchmark ontology, demonstrating the high quality of the ontologies learned. {\textcopyright} 2010 Wiley Periodicals, Inc.},
address = {USA},
author = {Jiang, Xing and Tan, Ah-Hwee},
issn = {1532-2882},
journal = {J. Am. Soc. Inf. Sci. Technol.},
keywords = {automatic taxonomy generation,knowledge acquisition,ontologies,semantic relationships,text mining},
month = {jan},
number = {1},
pages = {150--168},
publisher = {John Wiley \& Sons, Inc.},
title = {{CRCTOL: A Semantic-Based Domain Ontology Learning System}},
volume = {61},
year = {2010}
}
@inproceedings{Wang2020349,
abstract = {Multilingual knowledge graphs (KGs) such as DBpedia and YAGO contain structured knowledge of entities in several distinct languages, and they are useful resources for cross-lingual AI and NLP applications. Cross-lingual KG alignment is the task of matching entities with their counterparts in different languages, which is an important way to enrich the cross-lingual links in multilingual KGs. In this paper, we propose a novel approach for cross-lingual KG alignment via graph convolutional networks (GCNs). Given a set of pre-aligned entities, our approach trains GCNs to embed entities of each language into a unified vector space. Entity alignments are discovered based on the distances between entities in the embedding space. Embeddings can be learned from both the structural and attribute information of entities, and the results of structure embedding and attribute embedding are combined to get accurate alignments. In the experiments on aligning real multilingual KGs, our approach gets the best performance compared with other embedding-based KG alignment approaches. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 40; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Wang, Z and Lv, Q and Lan, X and Zhang, Y},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {Alignment,Attribute information; Convolutional networks; Cr,Convolution; Embeddings; Natural language processi},
pages = {349--357},
publisher = {Association for Computational Linguistics},
title = {{Cross-lingual knowledge graph alignment via graph convolutional networks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081733471&partnerID=40&md5=53c089964bcd0e41beab22b875bba29c},
year = {2020}
}
@inproceedings{10.1145/3148011.3148026,
abstract = {Capturing knowledge via learned latent vector representations of words, images and knowledge graph (KG) entities has shown state-of-the-art performance in computer vision, computational linguistics and KG tasks. Recent results demonstrate that the learning of such representations across modalities can be beneficial, since each modality captures complementary information. However, those approaches are limited to concepts with cross-modal alignments in the training data which are only available for just a few concepts. Especially for visual objects exist far fewer embeddings than for words or KG entities. We investigate whether a word embedding (e.g., for "apple") can still capture information from other modalities even if there is no matching concept within the other modalities (i.e., no images or KG entities of apples but of oranges as pictured in the title analogy). The empirical results of our knowledge transfer approach demonstrate that word embeddings do benefit from extrapolating information across modalities even for concepts that are not represented in the other modalities. Interestingly, this applies most to concrete concepts (e.g., dragonfly) while abstract concepts (e.g., animal) benefit most if aligned concepts are available in the other modalities.},
address = {New York, NY, USA},
author = {Both, Fabian and Thoma, Steffen and Rettinger, Achim},
booktitle = {Proceedings of the Knowledge Capture Conference},
doi = {10.1145/3148011.3148026},
isbn = {9781450355537},
keywords = {Knowledge Transfer,Multi-Modality,Word Similarity},
publisher = {Association for Computing Machinery},
series = {K-CAP 2017},
title = {{Cross-Modal Knowledge Transfer: Improving the Word Embedding of Apple by Looking at Oranges}},
url = {https://doi.org/10.1145/3148011.3148026},
year = {2017}
}
@inproceedings{9194501,
abstract = {As the largest source code repository, GitHub has played a vital role in modern social coding ecosystem to generate production software. Despite the apparent benefits of such social coding paradigm, its potential security risks have been largely overlooked (e.g., malicious codes or repositories could be easily embedded and distributed). To address this imminent issue, in this paper, we propose a novel framework (named GitCyber) to automate malicious repository detection in GitHub at the first attempt. In GitCyber, we first extract code contents from the repositories hosted in GitHub as the inputs for deep neural network (DNN), and then we incorporate cybersecurity domain knowledge modeled by heterogeneous information network (HIN) to design cyber-guided loss function in the learning objective of the DNN to assure the classification performance while preserving consistency with the observational domain knowledge. Comprehensive experiments based on the large-scale data collected from GitHub demonstrate that our proposed GitCyber outperforms the state-of-the-arts in malicious repository detection.},
author = {Zhang, Y and Fan, Y and Hou, S and Ye, Y and Xiao, X and Li, P and Shi, C and Zhao, L and Xu, S},
booktitle = {2020 IEEE International Conference on Knowledge Graph (ICKG)},
doi = {10.1109/ICBK50248.2020.00071},
keywords = {information networks;neural nets;pattern classific},
month = {aug},
pages = {458--465},
title = {{Cyber-guided Deep Neural Network for Malicious Repository Detection in GitHub}},
year = {2020}
}
@article{Romanov2020262,
abstract = {This article discusses the algorithms and methods of data representation for knowledge graphs. The proposed algorithms make it possible to automate the process of extracting and processing data from users' requests within the mixed learning process and reduce the role of an expert in the preparation of question-answering data sets necessary for training models of dialogue systems. The results show that the method of enrichment of the knowledge graph leads to an increase in the number of links and the accuracy of vector representation models. {\textcopyright} 2020, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 8th World Conference on Information Systems and Technologies, WorldCIST 2020 ; Conference Date: 7 April 2020 Through 10 April 2020; Conference Code:240259},
author = {Romanov, A and Volchek, D and Mouromtsev, D},
doi = {10.1007/978-3-030-45688-7_27},
editor = {{Rocha A. Adeli H.}, Reis L P Costanzo S Orovic I Moreira F},
isbn = {9783030456870},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Data extraction; Data representations; Dialogue s,Data handling; Graph algorithms; Information syste,Data mining},
pages = {262--270},
publisher = {Springer},
title = {{Data Extraction and Preprocessing for Automated Question Answering Based on Knowledge Graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085527354&doi=10.1007%2F978-3-030-45688-7_27&partnerID=40&md5=f21321724870c2f3337aed2bc0b438c3},
volume = {1159 AISC},
year = {2020}
}
@article{Gruca2017,
abstract = {Background: High-throughput methods in molecular biology provided researchers with abundance of experimental data that need to be interpreted in order to understand the experimental results. Manual methods of functional gene/protein group interpretation are expensive and time-consuming; therefore, there is a need to develop new efficient data mining methods and bioinformatics tools that could support the expert in the process of functional analysis of experimental results. Results: In this study, we propose a comprehensive framework for the induction of logical rules in the form of combinations of Gene Ontology (GO) terms for functional interpretation of gene sets. Within the framework, we present four approaches: the fully automated method of rule induction without filtering, rule induction method with filtering, expert-driven rule filtering method based on additive utility functions, and expert-driven rule induction method based on the so-called seed or expert terms - the GO terms of special interest which should be included into the description. These GO terms usually describe some processes or pathways of particular interest, which are related to the experiment that is being performed. During the rule induction and filtering processes such seed terms are used as a base on which the description is build. Conclusion: We compare the descriptions obtained with different algorithms of rule induction and filtering and show that a filtering step is required to reduce the number of rules in the output set so that they could be analyzed by a human expert. However, filtering may remove information from the output rule set which is potentially interesting for the expert. Therefore, in the study, we present two methods that involve interaction with the expert during the process of rule induction. Both of them are able to reduce the number of rules, but only in the case of the method based on seed terms, each of the created rule includes expert terms in combination with the other terms. Further analysis of such combinations may provide new knowledge about biological processes and their combination with other pathways related to genes described by the rules. A suite of Matlab scripts that provide the functionality of a comprehensive framework for the rule induction and filtering presented in this study is available free of charge at: http://rulego.polsl.pl/framework. {\textcopyright} 2017 The Author(s).},
annote = {cited By 3},
author = {Gruca, A and Sikora, M},
doi = {10.1186/s13326-017-0129-x},
issn = {20411480},
journal = {Journal of Biomedical Semantics},
keywords = {Data Mining; Databases,Genetic; Gene Ontology; Humans,data mining; gene ontology; genetic database; huma},
number = {1},
publisher = {BioMed Central Ltd.},
title = {{Data- and expert-driven rule induction and filtering framework for functional interpretation and description of gene sets}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021193897&doi=10.1186%2Fs13326-017-0129-x&partnerID=40&md5=4862f0515ec568630e1d3477cbb2ad78},
volume = {8},
year = {2017}
}
@inproceedings{10.1145/2783258.2783365,
abstract = {We apply deep learning to the problem of discovery and detection of characteristic patterns of physiology in clinical time series data. We propose two novel modifications to standard neural net training that address challenges and exploit properties that are peculiar, if not exclusive, to medical data. First, we examine a general framework for using prior knowledge to regularize parameters in the topmost layers. This framework can leverage priors of any form, ranging from formal ontologies (e.g., ICD9 codes) to data-derived similarity. Second, we describe a scalable procedure for training a collection of neural networks of different sizes but with partially shared architectures. Both of these innovations are well-suited to medical applications, where available data are not yet Internet scale and have many sparse outputs (e.g., rare diagnoses) but which have exploitable structure (e.g., temporal order and relationships between labels). However, both techniques are sufficiently general to be applied to other problems and domains. We demonstrate the empirical efficacy of both techniques on two real-world hospital data sets and show that the resulting neural nets learn interpretable and clinically relevant features.},
address = {New York, NY, USA},
author = {Che, Zhengping and Kale, David and Li, Wenzhe and Bahadori, Mohammad Taha and Liu, Yan},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2783258.2783365},
isbn = {9781450336642},
keywords = {deep learning,healthcare,medical informatics,multi-label classification,multivari- ate time series,phenotyping},
pages = {507--516},
publisher = {Association for Computing Machinery},
series = {KDD '15},
title = {{Deep Computational Phenotyping}},
url = {https://doi.org/10.1145/2783258.2783365},
year = {2015}
}
@article{Soliman2020,
abstract = {The Internet is a remarkably complex technical system. Its rapid growth has also brought technical issues such as problems to information retrieval. Search engines retrieve requested information based on the provided keywords. Consequently, it is difficult to accurately find the required information without understanding the syntax and semantics of the content. Multiple approaches are proposed to resolve this problem by employing the semantic web and linked data techniques. Such approaches serialize the content using the Resource Description Framework (RDF) and execute the queries using SPARQL to resolve the problem. However, an exact match between RDF content and query structure is required. Although, it improves the keyword-based search; however, it does not provide probabilistic reasoning to find the semantic relationship between the queries and their results. From this perspective, in this paper, we propose a deep learning-based approach for searching RDF graphs. The proposed approach treats document requests as a classification problem. First, we preprocess the RDF graphs to convert them into N-Triples format. Second, bag-of-words (BOW) and word2vec feature modeling techniques are combined for a novel deep representation of RDF graphs. The attention mechanism enables the proposed approach to understand the semantic between RDF graphs. Third, we train a convolutional neural network for the accurate retrieval of RDF graphs using the deep representation. We employ 10-fold cross-validation to evaluate the proposed approach. The results show that the proposed approach is accurate and surpasses the state-of-the-art. The average accuracy, precision, recall, and f-measure are up to 97.12%, 98.17%, 95.56%, and 96.85%, respectively. {\textcopyright} 2020 Hatem Soliman. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
annote = {cited By 0},
author = {Soliman, H},
doi = {10.1371/journal.pone.0230500},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Data Mining; Databases,Factual; Deep Learning; Software,article; attention; convolutional neural network;},
number = {3},
publisher = {Public Library of Science},
title = {{Deep learning based searching approach for RDF graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082176386&doi=10.1371%2Fjournal.pone.0230500&partnerID=40&md5=ffaa256835d5390dfbd4d4de0eb04551},
volume = {15},
year = {2020}
}
@article{Deng201952,
abstract = {Time series prediction with data stream has been widely studied. Current deep learning methods e.g., Long Short-Term Memory (LSTM) perform well in learning feature representations from raw data. However, most of these models can narrowly learn semantic information behind the data. In this paper, we revisit LSTM from the perspective of Semantic Web, where streaming data are represented as ontology sequences. We propose a novel semantic-based neural network (STBNet) that (i) enriches the semantics of data stream with external text, and (ii) exploits the underlying semantics with background knowledge for time series prediction. Previous models mainly rely on numerical representation of values in raw data, while the proposed STBNet model creatively integrates semantic embedding into a hybrid neural network. We develop a new attention mechanism based on similarity among semantic embedding of ontology stream, and then we combine ontology stream and numerical analysis in the deep learning model. Furthermore, we also enrich ontology stream in STBNet, where Convolutional Neural Networks (CNNs) are incorporated in learning lexical representations of words in the text. The experiments show that STBNet outperforms state-of-the-art methods on stock price prediction. {\textcopyright} 2019, Springer Nature Singapore Pte Ltd.},
annote = {cited By 0; Conference of 3rd China Conference on Knowledge Graph and Semantic Computing, CCKS 2018 ; Conference Date: 14 August 2018 Through 17 August 2018; Conference Code:221949},
author = {Deng, S and Pan, J Z and Chen, J and Chen, H},
doi = {10.1007/978-981-13-3146-6_5},
editor = {{Li X. Harmelen F.V.}, Tang J Han X Wang Q Zhao J},
isbn = {9789811331459},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Back-ground knowledge; Convolutional neural netwo,Data mining; Deep learning; Electronic trading; Fo,Long short-term memory},
pages = {52--64},
publisher = {Springer Verlag},
title = {{Deep learning for knowledge-driven ontology stream prediction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058510112&doi=10.1007%2F978-981-13-3146-6_5&partnerID=40&md5=9d32a96a5c73436e2e447592155550bc},
volume = {957},
year = {2019}
}
@inproceedings{ArguelloCasteleiro2016,
abstract = {Background: Automatic identification of gene and protein names from biomedical publications can help curators and researchers to keep up with the findings published in the scientific literature. As of today, this is a challenging task related to information retrieval, and in the realm of Big Data Analytics. Objectives: To investigate the feasibility of using word embeddings (i.e. distributed word representations) from Deep Learning algorithms together with terms from the Cardiovascular Disease Ontology (CVDO) as a step to identifying omics information encoded in the biomedical literature. Methods: Word embeddings were generated using the neural language models CBOW and Skip-gram with an input of more than 14 million PubMed citations (titles and abstracts) corresponding to articles published between 2000 and 2016. Then the abstracts of selected papers from the sysVASC systematic review were manually annotated with gene/protein names. We set up two experiments that used the word embeddings to produce term variants for gene/protein names: the first experiment used the terms manually annotated from the papers; the second experiment enriched/expanded the annotated terms using terms from the human-readable labels of key classes (gene/proteins) from the CVDO ontology. CVDO is formalised in the W3C Web Ontology Language (OWL) and contains 172,121 UniProt Knowledgebase protein classes related to human and 86,792 UniProtKB protein classes related to mouse. The hypothesis is that by enriching the original annotated terms, a better context is provided, and therefore, it is easier to obtain suitable (full and/or partial) term variants for gene/protein names from word embeddings. Results: From the papers manually annotated, a list of 107 terms (gene/protein names) was acquired. As part of the word embeddings generated from CBOW and Skip-gram, a lexicon with more than 9 million terms was created. Using the cosine similarity metric, a list of the 12 top-ranked terms was generated from word embeddings for query terms present in the generated lexicon. Domain experts evaluated a total of 1968 pairs of terms and classified the retrieved terms as: TV (term variant); PTV (partial term variant); and NTV (non term variant, meaning none of the previous two categories). In experiment I, Skip-gram finds the double amount of (full and/or partial) term variants for gene/protein names as compared with CBOW. Using Skip-gram, the weighted Cohen's Kappa inter-annotator agreement for two domain experts was 0.80 for the first experiment and 0.74 for the second experiment. In the first experiment, suitable (full and/or partial) term variants were found for 65 of the 107 terms. In the second experiment, the number increased to 100. Conclusion: This study demonstrates the benefits of using terms from the CVDO ontology classes to obtain more pertinent term variants for gene/protein names from word embeddings generated from an unannotated corpus with more than 14 million PubMed citations. As the terms variants are induced from the biomedical literature, they can facilitate data tagging and semantic indexing tasks. Overall, our study explores the feasibility of obtaining methods that scale when dealing with big data, and which enable automation of deep semantic analysis and markup of textual information from unannotated biomedical literature.},
annote = {cited By 1; Conference of 7th Workshop on Ontologies and Data in Life Sciences, ODLS 2016 ; Conference Date: 29 September 2016 Through 30 September 2016; Conference Code:124174},
author = {{Arguello Casteleiro}, M and Demetriou, G and Read, W and Fernandez-Prieto, M J and Maseda-Fernandez, D and Nenadic, G and Klein, J and Keane, J and Stevens, R},
booktitle = {CEUR Workshop Proceedings},
editor = {{Herre H. Jansen L.}, Jansen L Schober D Loebe F Boeker M},
issn = {16130073},
keywords = {Abstracting; Automation; Bioinformatics; Cardiolog,Automatic identification; Biomedical literature;,Big data},
publisher = {CEUR-WS},
title = {{Deep learning meets semantic web: A feasibility study with the cardiovascular disease ontology and PubMed citations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992398755&partnerID=40&md5=e95975136cd1fcc477d293e2589327d5},
volume = {1692},
year = {2016}
}
@article{Zong20172337,
abstract = {Motivation: A heterogeneous network topology possessing abundant interactions between biomedical entities has yet to be utilized in similarity-based methods for predicting drug-target associations based on the array of varying features of drugs and their targets. Deep learning reveals features of vertices of a large network that can be adapted in accommodating the similarity-based solutions to provide a flexible method of drug-target prediction. Results: We propose a similarity-based drug-target prediction method that enhances existing association discovery methods by using a topology-based similarity measure. DeepWalk, a deep learning method, is adopted in this study to calculate the similarities within Linked Tripartite Network (LTN), a heterogeneous network generated from biomedical linked datasets. This proposed method shows promising results for drug-target association prediction: 98.96% AUC ROC score with a 10-fold cross-validation and 99.25% AUC ROC score with a Monte Carlo crossvalidation with LTN. By utilizing DeepWalk, we demonstrate that: (i) this method outperforms other existing topology-based similarity computation methods, (ii) the performance is better for tripartite than with bipartite networks and (iii) the measure of similarity using network topology outperforms the ones derived from chemical structure (drugs) or genomic sequence (targets). Our proposed methodology proves to be capable of providing a promising solution for drug-target prediction based on topological similarity with a heterogeneous network, and may be readily re-purposed and adapted in the existing of similarity-based methodologies. {\textcopyright} The Author 2017. Published by Oxford University Press. All rights reserved.},
annote = {cited By 54},
author = {Zong, N and Kim, H and Ngo, V and Harismendy, O},
doi = {10.1093/bioinformatics/btx160},
issn = {13674803},
journal = {Bioinformatics},
keywords = {Computational Biology; Data Mining; Humans; Machi,biology; data mining; human; machine learning; pha},
number = {15},
pages = {2337--2344},
publisher = {Oxford University Press},
title = {{Deep mining heterogeneous networks of biomedical linked data to predict novel drug-target associations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026403314&doi=10.1093%2Fbioinformatics%2Fbtx160&partnerID=40&md5=dd99f9821acdd7f6fdfcf15fc7098fb7},
volume = {33},
year = {2017}
}
@inproceedings{8667969,
abstract = {Links between issue reports and corresponding code commits to fix them can greatly reduce the maintenance costs of a software project. More often than not, however, these links are missing and thus cannot be fully utilized by developers. Current practices in issue-commit link recovery extract text features and code features in terms of textual similarity from issue reports and commit logs to train their models. These approaches are limited since semantic information could be lost. Furthermore, few of them consider the effect of source code files related to a commit on issue-commit link recovery, let alone the semantics of code context. To tackle these problems, we propose to construct code knowledge graph of a code repository and generate embeddings of source code files to capture the semantics of code context. We also use embeddings to capture the semantics of issue- or commit-related text. Then we use these embeddings to calculate semantic similarity and code similarity using a deep learning approach before training a SVM binary classification model with additional features. Evaluations on real-world projects show that our approach DeepLink can outperform the state-of-the-art method.},
author = {Xie, R and Chen, L and Ye, W and Li, Z and Hu, T and Du, D and Zhang, S},
booktitle = {2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
doi = {10.1109/SANER.2019.8667969},
issn = {1534-5351},
keywords = {graph theory;information retrieval;learning (artif},
month = {feb},
pages = {434--444},
title = {{DeepLink: A Code Knowledge Graph Based Deep Learning Approach for Issue-Commit Link Recovery}},
year = {2019}
}
@inproceedings{Xiong2017564,
abstract = {We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.1 {\textcopyright} 2017 Association for Computational Linguistics.},
annote = {cited By 48; Conference of 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017 ; Conference Date: 9 September 2017 Through 11 September 2017; Conference Code:150071},
author = {Xiong, W and Hoang, T and Wang, W Y},
booktitle = {EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
doi = {10.18653/v1/d17-1060},
isbn = {9781945626838},
keywords = {Continuous state; Knowledge graphs; Language lear,Embeddings; Graph theory; Machine learning; Natura,Reinforcement learning},
pages = {564--573},
publisher = {Association for Computational Linguistics (ACL)},
title = {{DeepPath: A reinforcement learning method for knowledge graph reasoning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073145268&doi=10.18653%2Fv1%2Fd17-1060&partnerID=40&md5=e49f8273a70d50d792ebba80c80bbc3f},
year = {2017}
}
@inproceedings{Zeng2020811,
abstract = {Entity alignment (EA) is to discover equivalent entities in knowledge graphs (KGs), which bridges heterogeneous sources of information and facilitates the integration of knowledge. Existing EA solutions mainly rely on structural information to align entities, typically through KG embedding. Nonetheless, in real-life KGs, only a few entities are densely connected to others, and the rest majority possess rather sparse neighborhood structure. We refer to the latter as long-tail entities, and observe that such phenomenon arguably limits the use of structural information for EA. To mitigate the issue, we revisit and investigate into the conventional EA pipeline in pursuit of elegant performance. For pre-alignment, we propose to amplify long-tail entities, which are of relatively weak structural information, with entity name information that is generally available (but overlooked) in the form of concatenated power mean word embeddings. For alignment, under a novel complementary framework of consolidating structural and name signals, we identify entity's degree as important guidance to effectively fuse two different sources of information. To this end, a degree-aware co-attention network is conceived, which dynamically adjusts the significance of features in a degree-aware manner. For post-alignment, we propose to complement original KGs with facts from their counterparts by using confident EA results as anchors via iterative training. Comprehensive experimental evaluations validate the superiority of our proposed techniques. {\textcopyright} 2020 ACM.},
annote = {cited By 0; Conference of 43rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2020 ; Conference Date: 25 July 2020 Through 30 July 2020; Conference Code:161956},
author = {Zeng, W and Zhao, X and Wang, W and Tang, J and Tan, Z},
booktitle = {SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3397271.3401161},
isbn = {9781450380164},
keywords = {Alignment,Embeddings; Information retrieval; Iterative metho,Experimental evaluation; Heterogeneous sources; I},
pages = {811--820},
publisher = {Association for Computing Machinery, Inc},
title = {{Degree-Aware Alignment for Entities in Tail}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090140462&doi=10.1145%2F3397271.3401161&partnerID=40&md5=0534aa0da2ab5c6afe34563f6895c862},
year = {2020}
}
@article{Yang2020,
abstract = {Along with studies on artificial intelligence technology, research is also being carried out actively in the field of natural language processing to understand and process people's language, in other words, natural language. For computers to learn on their own, the skill of understanding natural language is very important. There are a wide variety of tasks involved in the field of natural language processing, but we would like to focus on the named entity registration and relation extraction task, which is considered to be the most important in understanding sentences. We propose DeNERT-KG, a model that can extract subject, object, and relationships, to grasp the meaning inherent in a sentence. Based on the BERT language model and Deep Q-Network, the named entity recognition (NER) model for extracting subject and object is established, and a knowledge graph is applied for relation extraction. Using the DeNERT-KG model, it is possible to extract the subject, type of subject, object, type of object, and relationship from a sentence, and verify this model through experiments. {\textcopyright} 2020 by the authors.},
annote = {cited By 0},
author = {Yang, S and Yoo, S and Jeong, O},
doi = {10.3390/APP10186429},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
number = {18},
publisher = {MDPI AG},
title = {{DeNERT-KG: named entity and relation extraction model using DQN, knowledge graph, and BERT}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092060828&doi=10.3390%2FAPP10186429&partnerID=40&md5=1c93446f778509abe38cf70c4e55d18f},
volume = {10},
year = {2020}
}
@inproceedings{Chen2014242,
abstract = {Recent works showed the trend of leveraging web-scaled structured semantic knowledge resources such as Freebase for open domain spoken language understanding (SLU). Knowledge graphs provide sufficient but ambiguous relations for the same entity, which can be used as statistical background knowledge to infer possible relations for interpretation of user utterances. This paper proposes an approach to capture the relational surface forms by mapping dependency-based contexts of entities from the text domain to the spoken domain. Relational surface forms are learned from dependency-based entity embeddings, which encode the contexts of entities from dependency trees in a deep learning model. The derived surface forms carry functional dependency to the entities and convey the explicit expression of relations. The experiments demonstrate the efficiency of leveraging derived relational surface forms as local cues together with prior background knowledge. {\textcopyright} 2014 IEEE.},
annote = {cited By 18; Conference of 2014 IEEE Workshop on Spoken Language Technology, SLT 2014 ; Conference Date: 7 December 2014 Through 10 December 2014; Conference Code:111796},
author = {Chen, Y.-N. and Hakkani-Tur, D and Tur, G},
booktitle = {2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings},
doi = {10.1109/SLT.2014.7078581},
isbn = {9781479971299},
keywords = {Back-ground knowledge; Dependency trees; Embeddin,Computational linguistics; Information management;,Speech recognition},
pages = {242--247},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943747009&doi=10.1109%2FSLT.2014.7078581&partnerID=40&md5=eb52f2ee9d50bc4e75f679c19e6ad053},
year = {2014}
}
@inproceedings{Caminhas2019,
abstract = {DBpedia has long been one of the major hubs of the Linked Open Data ecosystem. It is built by a largely automated process that uses many extractors and manually curated mappings to read information from infoboxes on Wikipedia. Given the complexity of the task, it is not surprising that DBpedia contains different kinds of errors, ranging from mistakes in the source text to errors in the extractors themselves (or in the order in which they are applied). Of particular importance are typing errors in which an entity is assigned a type from the DBpedia ontology to which it does not belong. These errors propagate very far, given the modern practice of relying on Knowledge Graphs (KGs) such as DBpedia for obtaining training data through distant supervision. We posit a way to correct these errors is through a post factum analysis of the KG. Thus, we introduce and evaluate a KG refinement approach that uses binary classifiers that rely on semantic embeddings of the entities to detect and remove incorrect type assignments. Our initial evaluation is done using a highly curated gold standard of 35 types from the DBpedia ontology and shows the method is very promising. Copyright held by the author(s). {\textcopyright} 2019 CEUR-WS. All rights reserved.},
annote = {cited By 0; Conference of 1st International Workshop on Challenges and Experiences from Data Integration to Knowledge Graphs, DI2KG 2019 ; Conference Date: 5 August 2019; Conference Code:155354},
author = {Caminhas, D and Cones, D and Hervieux, N and Barbosa, D},
booktitle = {CEUR Workshop Proceedings},
editor = {{Firmani D. Crescenzi V.}, De Angelis A Dong X L Mazzei M Merialdo P Srivastava D},
issn = {16130073},
keywords = {Automated process; Binary classifiers; Dbpedia; G,Automation; Data mining; Embeddings; Errors; Graph,Data integration},
publisher = {CEUR-WS},
title = {{Detecting and correcting typing errors in DBpedia}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076678908&partnerID=40&md5=0ef68f2bef6b1506762ac734417286ef},
volume = {2512},
year = {2019}
}
@inproceedings{Cui2020492,
abstract = {To provide accurate and explainable misinformation detection, it is often useful to take an auxiliary source (e.g., social context and knowledge base) into consideration. Existing methods use social contexts such as users' engagements as complementary information to improve detection performance and derive explanations. However, due to the lack of sufficient professional knowledge, users seldom respond to healthcare information, which makes these methods less applicable. In this work, to address these shortcomings, we propose a novel knowledge guided graph attention network for detecting health misinformation better. Our proposal, named as DETERRENT, leverages on the additional information from medical knowledge graph by propagating information along with the network, incorporates a Medical Knowledge Graph and an Article-Entity Bipartite Graph, and propagates the node embeddings through Knowledge Paths. In addition, an attention mechanism is applied to calculate the importance of entities to each article, and the knowledge guided article embeddings are used for misinformation detection. DETERRENT addresses the limitation on social contexts in the healthcare domain and is capable of providing useful explanations for the results of detection. Empirical validation using two real-world datasets demonstrated the effectiveness of DETERRENT. Comparing with the best results of eight competing methods, in terms of F1 Score, DETERRENT outperforms all methods by at least 4.78% on the diabetes dataset and 12.79% on cancer dataset. We release the source code of DETERRENT at: https://github.com/cuilimeng/DETERRENT. {\textcopyright} 2020 ACM.},
annote = {cited By 0; Conference of 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2020 ; Conference Date: 23 August 2020 Through 27 August 2020; Conference Code:162480},
author = {Cui, L and Seo, H and Tabar, M and Ma, F and Wang, S and Lee, D},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3394486.3403092},
isbn = {9781450379984},
keywords = {Attention mechanisms; Bipartite graphs; Detection,Data mining,Embeddings; Graph theory; Health care; Knowledge b},
pages = {492--502},
publisher = {Association for Computing Machinery},
title = {{DETERRENT: Knowledge Guided Graph Attention Network for Detecting Healthcare Misinformation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090425194&doi=10.1145%2F3394486.3403092&partnerID=40&md5=aa451a65473a7d1f2f73940820a77483},
year = {2020}
}
@inproceedings{Wijesinghe2019,
abstract = {We propose a novel spectral convolutional neural network (CNN) model on graph structured data, namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters, called feedback-looped filters, to provide better localization on vertices, while still attaining fast convergence and linear memory requirements. Theoretically, feedback-looped filters can guarantee convergence w.r.t. a specified error bound, and be applied universally to any graph without knowing its structure. Furthermore, the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks over all datasets. {\textcopyright} 2019 Neural information processing systems foundation. All rights reserved.},
annote = {cited By 0; Conference of 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019 ; Conference Date: 8 December 2019 Through 14 December 2019; Conference Code:161263},
author = {Wijesinghe, A and Wang, Q},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
keywords = {Backpropagation; Convolutional neural networks; Fe,Citation networks; Distributed feedback; Document,Graph structures},
publisher = {Neural information processing systems foundation},
title = {{DFNets: Spectral CNNs for graphs with feedback-looped filters}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090171383&partnerID=40&md5=049fb3c30d084c0df068093295871e4b},
volume = {32},
year = {2019}
}
@inproceedings{9191234,
abstract = {Which are the segmentation algorithms proposed during 2018-2019 in CVPR that have CNN architecture?' Answering this question involves identifying and analyzing the deep learning architecture diagrams from several research papers. Retrieving such information poses significant challenge as most of the existing academic search engines are primarily based on only the text content. In this paper, we introduce Diag2Graph, an end-to-end framework for parsing deep learning diagram-figures, that enables powerful search and retrieval of architectural details in research papers. Our proposed approach automatically localizes figures from research papers, classifies them, and analyses the content of the diagram-figures. The key steps in analyzing the Figure content is the extraction of the different components data and finding their structural relation. Finally, the extracted components and their relations are represented in the form of a deep knowledge graph. A thorough evaluation on a real-word annotated dataset has been done to demonstrate the efficacy of our approach.},
author = {Roy, A and Akrotirianakis, I and Kannan, A V and Fradkin, D and Canedo, A and Koneripalli, K and Kulahcioglu, T},
booktitle = {2020 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP40778.2020.9191234},
issn = {2381-8549},
keywords = {Machine learning;Grammar;Measurement;Dictionaries;},
month = {oct},
pages = {2581--2585},
title = {{Diag2graph: Representing Deep Learning Diagrams In Research Papers As Knowledge Graphs}},
year = {2020}
}
@inproceedings{Chakravarty2019,
abstract = {Many documents are constituted by a sequence of question-answer (QA) pairs. Applying existing natural language processing (NLP) methods such as automatic summarization to such documents leads to poor results. Accordingly, we have developed classification methods based on dialog acts to facilitate subsequent application of NLP techniques. This paper describes the ontology of dialog acts we have devised through a case study of a corpus of legal depositions that are made of QA pairs, as well as our development of machine/deep learning classifiers to identify dialog acts in such corpora. We have adapted state-of-the-art text classification methods based on a convolutional neural network (CNN) and long short term memory (LSTM) to classify the questions and answers into their respective dialog acts. We have also used pre-trained BERT embeddings for one of our classifiers. Experimentation showed we could achieve an F1 score of 0.84 on dialog act classification involving 20 classes. Given such promising techniques to classify questions and answers into dialog acts, we plan to develop custom methods for each dialog act, to transform each QA pair into a form that would allow for the application of NLP or deep learning techniques for other downstream tasks, such as summarization. {\textcopyright} 2019 Copyright held by the owner/author(s).},
annote = {cited By 0; Conference of 3rd Workshop on Automated Semantic Analysis of Information in Legal Texts, ASAIL 2019 ; Conference Date: 21 June 2019; Conference Code:148876},
author = {Chakravarty, S and {Phanindra Chava}, R V S and Fox, E A},
booktitle = {CEUR Workshop Proceedings},
editor = {{Ashley K.D. Atkinson K.}, Branting L K Francesconi E Grabmair M Waltl B Walker V R Wyner A Z Wyner A Z},
issn = {16130073},
keywords = {Automatic summarization; Classification methods;,Classification (of information); Deep learning; Na,Long short-term memory},
publisher = {CEUR-WS},
title = {{Dialog acts classification for question-answer corpora}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067875110&partnerID=40&md5=84be5be039622cbfaca95c0bd2c3aab1},
volume = {2385},
year = {2019}
}
@article{NZQYJ7IK,
abstract = {Massive unstructured geoscience data are buried in geological reports. Geological text classification provides opportunities to leverage this wealth of data for geology and mineralization research. Existing studies of massive geoscience documents/reports have not provided effective classification results for further knowledge discovery and data mining and often lack adequate domain-specific knowledge. In this paper, we present a novel and unified framework (namely, Dic-Att-BiLSTM) that combines domain-specific knowledge and bidirectional long short-term memory (BiLSTM) for effective geological text classification. Dic-Att-BiLSTM benefits from a matching strategy by incorporating domain-specific knowledge developed based on geoscience ontology to grasp the linguistic geoscience clues. Furthermore, Dic-Att-BiLSTM brings together the capacity of a geoscience dictionary matching approach and an attention mechanism to construct a dictionary attention layer. Finally, the network framework of Dic-Att-BiLSTM can utilize domain-specific knowledge and classify geological text automatically. Experimental verifications are conducted on two constructed data sets, and the results clearly indicate that Dic-Att-BiLSTM outperforms other state-of-the-art text classification models. {\textcopyright} 2020 The Authors.},
annote = {cited By 1},
author = {Qiu, Q and Xie, Z and Wu, L and Tao, L},
issn = {23335084},
journal = {Earth and Space Science},
keywords = {algorithm; data mining; data set; memory; minerali},
number = {3},
publisher = {Wiley-Blackwell Publishing Ltd},
title = {{Dictionary-Based Automated Information Extraction From Geological Documents Using a Deep Learning Algorithm}},
volume = {7},
year = {2020}
}
@article{Kumar2019382,
abstract = {Knowledge graphs have become ubiquitous data sources and their utility has been amplified by the research on ability to answer carefully crafted questions over knowledge graphs. We investigate the problem of question generation (QG) over knowledge graphs wherein, the level of difficulty of the question can be controlled. We present an end-to-end neural network-based method for automatic generation of complex multi-hop questions over knowledge graphs. Taking a subgraph and an answer as input, our transformer-based model generates a natural language question. Our model incorporates difficulty estimation based on named entity popularity, and makes use of this estimation to generate difficulty-controllable questions. We evaluate our model on two recent multi-hop QA datasets. Our evaluation shows that our model is able to generate high-quality, fluent and relevant questions. We have released our curated QG dataset and code at https://github.com/liyuanfang/mhqg. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 18th International Semantic Web Conference, ISWC 2019 ; Conference Date: 26 October 2019 Through 30 October 2019; Conference Code:233309},
author = {Kumar, V and Hua, Y and Ramakrishnan, G and Qi, G and Gao, L and Li, Y.-F.},
doi = {10.1007/978-3-030-30793-6_22},
editor = {{Ghidini C. Hartig O.}, Maleshkova M Svatek V Cruz I Hogan A Song J Lefrancois M Gandon F},
isbn = {9783030307929},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automatic Generation; Difficulty estimations; Kno,Graphic methods; HTTP; Natural language processing,Semantic Web},
pages = {382--398},
publisher = {Springer},
title = {{Difficulty-Controllable Multi-hop Question Generation from Knowledge Graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075720005&doi=10.1007%2F978-3-030-30793-6_22&partnerID=40&md5=9ef13d2dfba894395c43d0e9965d082b},
volume = {11778 LNCS},
year = {2019}
}
@article{10.1016/j.cogsys.2018.01.002,
abstract = {Process management thinking to the safety issues facing the air transport industry.Modeling of critical pathways and structured analysis for collaborative activities.Refined three-step process for data pre-processing, model and inference objectives.Data mining methods (Association Rule Mining) to discover interesting knowledge.Ontology formalization for categorization of in-flight passenger medical incidents. ObjectivesIn order to get a clearer idea of in-flight medical emergencies management, the application of Data mining tools can be useful to facilitate knowledge discovery from data collected by existing studies. The objective of this work is to conceptualize the construction of a Clinical Decision Support System (CDSS) in three stages corresponding to the representation levels necessary to extract knowledge from information and raw data. MethodThe method can be summarized in three parts: (1) in-flight medical incident data search, (2) the validation of this data using Data mining tools, (3) the construction of the CDSS in 3 steps corresponding to the levels of knowledge representation. These three steps will be carried out using tools such as EORCA (Event Oriented Representation for Collaborative Activities) which includes action codification with regard to an ontology and event representation. ResultData processing services provide a good structuration for information about in-flight medical incidents from which useful knowledge can be generated could improve the handling of other incidents by adapting the medical emergency equipments, for example. This structuring can be facilitated by the use of CDSS to fill in any gaps, increase coherency, and provide decision makers with a more complete picture of options that might be involved in a critical situation. ConclusionWe proposed an evolving framework facilitating the description of in-flight medical emergencies with adequate data collection and appropriate information that are required for producing interesting rules and better decisions. The data collected nourishes the organization of information, which can be improved over time by continuous integration of evidence gained from the number of incidents treated. Finally, it is proposed to strengthen requirements concerning the medical equipments available on-board, particularly in the light of knowledge resulting from the selection and approval of interesting rules.},
address = {NLD},
author = {Sene, Alsane and Kamsu-Foguem, Bernard and Rumeau, Pierre},
doi = {10.1016/j.cogsys.2018.01.002},
issn = {1389-0417},
journal = {Cogn. Syst. Res.},
keywords = {Air transport,Association rule,Data mining,Decision system,Ontology reasoning},
month = {jun},
number = {C},
pages = {97--113},
publisher = {Elsevier Science Publishers B. V.},
title = {{Discovering Frequent Patterns for In-Flight Incidents}},
url = {https://doi.org/10.1016/j.cogsys.2018.01.002},
volume = {49},
year = {2018}
}
@article{Bang2020,
abstract = {The early-phase design of complex systems is a challenging task, as a decision maker has to take into account the intricate relationships among different design variables. A popular way to help decision makers easily identify important design features is to use data mining. However, many of the existing algorithms output design features that are too complex (e.g., conjunction of many literals with unrelated predicates), making it difficult for a user to understand, remember, and apply these features to find better designs. In this paper, we introduce a new data mining method that extracts compact design features through knowledge generalization. The proposed method performs a search over the space of features using a multi-objective evolutionary algorithm that contains a set of generalization operators in addition to conventional evolutionary operators. Both variables and feature types are generalized by using an ontology defining a set of domain-specific concepts and relationships. Generalization leads to more compact and insightful features, as generalized knowledge encompasses wider concepts. A comparative experiment is conducted on a real-world system architecting problem to demonstrate the gain in compactness of the extracted features without significant reductions in predictive power. {\textcopyright} 2019},
annote = {cited By 0},
author = {Bang, H and Selva, D},
doi = {10.1016/j.eswa.2019.113025},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Adaptive operator selections; Comparative experim,Data mining,Decision making; Feature extraction; Genetic algor},
publisher = {Elsevier Ltd},
title = {{Discovering generalized design knowledge using a multi-objective evolutionary algorithm with generalization operators}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074129528&doi=10.1016%2Fj.eswa.2019.113025&partnerID=40&md5=c750b531a87d33a00ed20201582b1922},
volume = {143},
year = {2020}
}
@article{Araujo20182907,
abstract = {This work applies grammatical evolution to identify taxonomic hierarchies of concepts from Wikipedia. Each article in Wikipedia covers a topic and is cross-linked by hyperlinks that connect related topics. Hierarchical taxonomies and their generalization to ontologies are a highly useful resource for many applications since they enable semantic search and reasoning. Thus, the automatic identification of taxonomies composed of concepts associated with linked Wikipedia pages has attracted much attention. We have developed a system which arranges a set of Wikipedia concepts into a taxonomy. This technique is based on the relationships among a set of features extracted from the contents of the Wikipedia pages. We have used a grammatical evolution algorithm to discover the best way of combining the considered features in an explicit function. Candidate functions are evaluated by applying a genetic algorithm to approximate the optimal taxonomy that the function can provide for a number of training cases. The fitness is computed as an average of the precision obtained by comparing, for the set of training cases, the taxonomy provided by the evaluated function with the reference one. Experimental results show that the proposal is able to provide valuable functions to find high-quality taxonomies. {\textcopyright} 2017, Springer-Verlag Berlin Heidelberg.},
annote = {cited By 1},
author = {Araujo, L and Martinez-Romo, J and Duque, A},
doi = {10.1007/s00500-017-2544-4},
issn = {14327643},
journal = {Soft Computing},
keywords = {Automatic identification; Grammatical evolution;,Automation; Computational grammars; Evolutionary a,Taxonomies},
number = {9},
pages = {2907--2919},
publisher = {Springer Verlag},
title = {{Discovering taxonomies in Wikipedia by means of grammatical evolution}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015663583&doi=10.1007%2Fs00500-017-2544-4&partnerID=40&md5=a9657910b8bee5edb8b998b018b56f23},
volume = {22},
year = {2018}
}
@article{Zhang2018276,
abstract = {Representation learning of knowledge graph aims to transform both the entities and relations into continuous low-dimensional vector space. Though there have been a variety of models for knowledge graph embedding, most existing latent-based models merely explain triples via latent features, while supplementary rich inference patterns hidden in the observed graph features have not been fully employed. For this reason, in this paper we propose the discriminative path-based embedding model (DPTransE) which jointly learns from the latent features and graph features. Our model builds interactions between these two features, and uses the graph features as the crucial prior to offer precise and discriminative embedding. Experimental results demonstrate that our method outperforms other baselines on the task of link prediction and entity classification. {\textcopyright} Springer International Publishing AG, part of Springer Nature 2018.},
annote = {cited By 6; Conference of 40th European Conference on Information Retrieval, ECIR 2018 ; Conference Date: 26 March 2018 Through 29 March 2018; Conference Code:212079},
author = {Zhang, M and Wang, Q and Xu, W and Li, W and Sun, S},
doi = {10.1007/978-3-319-76941-7_21},
editor = {{Azzopardi L. Pasi G.}, Hanbury A Piwowarski B},
isbn = {9783319769400},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Distributed representation; Graph features; Infer,Graph theory,Information retrieval; Knowledge representation; V},
pages = {276--288},
publisher = {Springer Verlag},
title = {{Discriminative path-based knowledge graph embedding for precise link prediction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044457698&doi=10.1007%2F978-3-319-76941-7_21&partnerID=40&md5=73d5b719d427c1efe47018e27e4be2df},
volume = {10772 LNCS},
year = {2018}
}
@article{Blanco-Fernández2020,
abstract = {Organizations that preserve and promote heritage must meet the expectatives of sophisticated visitors who, far from wanting simply to be informed, desire to explore engaging and innovative technology-driven experiences which consider their particular interests and encourage them to discover more. We describe an approach based on quiz games that can be exploited in the deployment of such challenging experiences. The game consists of raising multiple-choice questions about a particular theme which is introduced by a Humanities expert through a brief narrative. Given the input text, a question and its right answer, our strategy provides the expert with a set of wrong alternatives (called distractors). These options are chosen from a (semi)automatically-built tailor-made corpus of documents by considering each player's level of knowledge on the game theme and exploiting Linked Open Data initiatives and natural language processing. On the one hand, automatic selection of distractors assists the Humanities expert to create games about very diverse topics without needing to be a specialist in all of them. On the other one, distractors are related to the right answer of each question in an appealing and meaningful way, which contributes to arouse the visitors' curiosity and their possible interest in exploring similar experiences in future visits. The work has been experimentally validated, achieving better results than a previous distractor identification strategy. {\textcopyright} 2019 Elsevier Ltd},
annote = {cited By 0},
author = {Blanco-Fern{\'{a}}ndez, Y and Gil-Solla, A and Pazos-Arias, J J and Ramos-Cabrer, M and Daif, A and L{\'{o}}pez-Nores, M},
doi = {10.1016/j.eswa.2019.113051},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Dbpedia; Identification strategies; Innovative te,Linked data; Natural language processing systems;,Open Data},
publisher = {Elsevier Ltd},
title = {{Distracting users as per their knowledge: Combining linked open data and word embeddings to enhance history learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074152565&doi=10.1016%2Fj.eswa.2019.113051&partnerID=40&md5=87bc9132b1af642cb13a9976c5540b1b},
volume = {143},
year = {2020}
}
@inproceedings{9047423,
abstract = {Using artificial intelligence to solve medical problems has been an intriguing yet challenging topic. In recent years, with the availability of electronic medical records (EMRs), many researchers have focused on diagnose diseases by mining EMRs. They apply machine learning algorithms to train EMRs, and modify models to improve the accuracy of diseases diagnosis. However, these studies input the patient characteristics into the model at once, without multiple online interactions. Or, put another way, although those methods have performed well in the laboratory, they are not suitable for a real consultation environment. In the real world, doctors often guide patients to describe the condition step by step, and then combine the checkup value to diagnose the disease. In this paper, we simulate this process and propose a novel system called DKDR, which combine a knowledge graph and deep reinforcement learning to diagnose disease. The medical knowledge graph is built by crawling 100K web pages, which help users improve the description of disease characteristics. We use q-learning to find the combination of symptoms in the best diagnosis and use convolutional neural networks (CNN) to train each strategy. Finally, we experiment on real medical datasets and synthetic medical datasets. DKDR finds the best combination of symptoms for diagnosing disease. The diagnostic accuracy rates for pneumonia, hyperlipidemia and obesity are 89%, 90% and 92%, respectively.},
author = {Jia, Y and Tan, Z and Zhang, J},
booktitle = {2019 IEEE Intl Conf on Parallel Distributed Processing with Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Networking (ISPA/BDCloud/SocialCom/SustainCom)},
doi = {10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00187},
keywords = {convolutional neural nets;data mining;diseases;ele},
month = {dec},
pages = {1303--1308},
title = {{DKDR: An Approach of Knowledge Graph and Deep Reinforcement Learning for Disease Diagnosis}},
year = {2019}
}
@article{Xu2019122,
abstract = {Background: Disease named entity recognition (NER) plays an important role in biomedical research. There are a significant number of challenging issues to be addressed; among these, the identification of rare diseases and complex disease names and the problem of tagging inconsistency (i.e., if an entity is tagged differently in a document) are attracting substantial research attention. Methods: We propose a new neural network method named Dic-Att-BiLSTM-CRF (DABLC) for disease NER. DABLC applies an efficient exact string matching method to match disease entities with a disease dictionary; here, the dictionary is constructed based on the Disease Ontology. Furthermore, DABLC constructs a dictionary attention layer by incorporating a disease dictionary matching method and document-level attention mechanism. Finally, a bidirectional long short-term memory network and conditional random field (BiLSTM-CRF) with a dictionary attention layer is proposed to combine the disease dictionary to develop disease NER. Results: Extensive experiments are conducted on two widely-used corpora: the NCBI disease corpus and the BioCreative V CDR corpus. We apply each test on 10 executions of each model, with a 95% confidence interval. DABLC achieves the highest F1 scores (NCBI: Precision = 0.883, Recall = 0.89, F1 = 0.886; BioCreative V CDR: Precision = 0.891, Recall = 0.875, F1 = 0.883), outperforming the state-of-the-art methods. Conclusion: DABLC combines the advantages of both external dictionary resources and deep attention neural networks. This aids the identification of rare diseases and complex disease names; moreover, it reduces the impact of tagging inconsistency. Special disease NER and deep learning models addressing long sentences are noteworthy areas for future examination. {\textcopyright} 2019 Elsevier Ltd},
annote = {cited By 11},
author = {Xu, K and Yang, Z and Kang, P and Wang, Q and Liu, W},
doi = {10.1016/j.compbiomed.2019.04.002},
issn = {00104825},
journal = {Computers in Biology and Medicine},
keywords = {Article; bidirectional long short term memory net,Attention mechanisms; Biomedical informatics; Bio,Clock and data recovery circuits (CDR circuits); C,Data Mining; Deep Learning; Disease; Humans; Lang,Diseases},
pages = {122--132},
publisher = {Elsevier Ltd},
title = {{Document-level attention-based BiLSTM-CRF incorporating disease dictionary for disease named entity recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064312088&doi=10.1016%2Fj.compbiomed.2019.04.002&partnerID=40&md5=4679b6657ba576ea6e56744b5c33001a},
volume = {108},
year = {2019}
}
@inproceedings{8970777,
abstract = {Due to their promising performance in clinical risk prediction with Electronic Health Records (EHRs), deep learning methods have attracted significant interest from healthcare researchers. However, there are 4 challenges: (i) Data insufficiency. Many methods require large amounts of training data to achieve satisfactory results. (ii) Interpretability. Results from many methods are hard to explain to clinicians (e.g., why the models make particular predictions and which events cause clinical outcomes). (iii) Domain knowledge integration. No existing method dynamically exploits complicated medical knowledge (e.g., relations such as cause and is-caused-by between clinical events). (iv) Time interval information. Most existing methods only consider the relative order of visits from EHRs, but ignore the irregular time intervals between neighboring visits. In the study, we propose a new model, Domain Knowledge Guided Recurrent Neural Networks (DG-RNN), by directly introducing domain knowledge from the medical knowledge graph into an RNN architecture, as well as taking the irregular time intervals into account. Experimental results on heart failure risk prediction tasks show that our model not only outperforms state-of-the-art deep-learning based risk prediction models, but also associates individual medical events with heart failure onset, thus paving the way for interpretable accurate clinical risk predictions.},
author = {Yin, C and Zhao, R and Qian, B and Lv, X and Zhang, P},
booktitle = {2019 IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2019.00084},
issn = {2374-8486},
keywords = {electronic health records;graph theory;health care},
month = {nov},
pages = {738--747},
title = {{Domain Knowledge Guided Deep Learning with Electronic Health Records}},
year = {2019}
}
@inproceedings{Gupta2017115,
abstract = {Ontology, the shared formal conceptualization of domain information, has been shown to have multiple applications in modeling, processing and understanding natural language text. In this work, we use distributed word vectors out of various recent language models from Deep Learning for semi-automated domain ontology creation for closed domains. We cover all major aspects of Domain Ontology Induction or Learning like concept identification, attribute identification, taxonomical and nontaxonomical relationship identification using the distributed word vectors. Preliminary results show that simple clustering based methods using distributed word vectors from these language models outperforms methods using models like LSI in ontology learning for closed domains. {\textcopyright} 2016 IEEE.},
annote = {cited By 4; Conference of 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016 ; Conference Date: 18 December 2016 Through 20 December 2016; Conference Code:126310},
author = {Gupta, N and Podder, S and Annervaz, K M and Sengupta, S},
booktitle = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
doi = {10.1109/ICMLA.2016.81},
isbn = {9781509061662},
keywords = {Artificial intelligence; Computational linguistics,Clustering; Language model; Latent Semantic Index,Modeling languages},
pages = {115--119},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Domain ontology induction using word embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015410083&doi=10.1109%2FICMLA.2016.81&partnerID=40&md5=268bdca04031e362ab367ae84757b50f},
year = {2017}
}
@article{Wang2019197,
abstract = {Embedding entities and relations into a continuous multi-dimensional vector space have become the dominant method for knowledge graph embedding in representation learning. However, most existing models ignore to represent hierarchical knowledge, such as the similarities and dissimilarities of entities in one domain. We proposed to learn a Domain Representations over existing knowledge graph embedding models, such that entities that have similar attributes are organized into the same domain. Such hierarchical knowledge of domains can give further evidence in link prediction. Experimental results show that domain embeddings give a significant improvement over the most recent state-of-art baseline knowledge graph embedding models. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 8th CCF International Conference on Natural Language Processing and Chinese Computing, NLPCC 2019 ; Conference Date: 9 October 2019 Through 14 October 2019; Conference Code:232669},
author = {Wang, C and Ren, F and Lin, Z and Zhao, C and Xie, T and Zhang, Y},
doi = {10.1007/978-3-030-32233-5_16},
editor = {{Tang J. Kan M.-Y.}, Zhao D Li S Zan H},
isbn = {9783030322328},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Domain; Domain representations; Hierarchical know,Embeddings,Natural language processing systems; Vector spaces},
pages = {197--210},
publisher = {Springer},
title = {{Domain Representation for Knowledge Graph Embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075587023&doi=10.1007%2F978-3-030-32233-5_16&partnerID=40&md5=c043837444d3f907b73d60852027f7a4},
volume = {11838 LNAI},
year = {2019}
}
@article{Bougiatiotis2020122,
abstract = {Knowledge Graphs provide insights from data extracted in various domains. In this paper, we present an approach discovering probable drug-to-drug interactions, through the generation of a Knowledge Graph from disease-specific literature. The Graph is generated using natural language processing and semantic indexing of biomedical publications and open resources. The semantic paths connecting different drugs in the Graph are extracted and aggregated into feature vectors representing drug pairs. A classifier is trained on known interactions, extracted from a manually curated drug database used as a golden standard, and discovers new possible interacting pairs. We evaluate this approach on two use cases, Alzheimer's Disease and Lung Cancer. Our system is shown to outperform competing graph embedding approaches, while also identifying new drug-drug interactions that are validated retrospectively. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 18th International Conference on Artificial Intelligence in Medicine, AIME 2020 ; Conference Date: 25 August 2020 Through 28 August 2020; Conference Code:249409},
author = {Bougiatiotis, K and Aisopos, F and Nentidis, A and Krithara, A and Paliouras, G},
doi = {10.1007/978-3-030-59137-3_12},
editor = {{Michalowski M.}, Moskovitch R},
isbn = {9783030591366},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biomedical literature; Drug-drug interactions; Fe,Classification (of information); Indexing (materia,Drug interactions},
pages = {122--132},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Drug-Drug Interaction Prediction on a Biomedical Literature Knowledge Graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092241579&doi=10.1007%2F978-3-030-59137-3_12&partnerID=40&md5=c1433d47d57cebc5eb2e8e2938feb5f7},
volume = {12299 LNAI},
year = {2020}
}
@inproceedings{10.1145/3292500.3330706,
abstract = {In talent recruitment, the job interview aims at selecting the right candidates for the right jobs through assessing their skills and experiences in relation to the job positions. While tremendous efforts have been made in improving job interviews, a long-standing challenge is how to design appropriate interview questions for comprehensively assessing the competencies that may be deemed relevant and representative for person-job fit. To this end, in this research, we focus on the development of a personalized question recommender system, namely DuerQuiz, for enhancing the job interview assessment. DuerQuiz is a fully deployed system, in which a knowledge graph of job skills, Skill-Graph, has been built for comprehensively modeling the relevant competencies that should be assessed in the job interview. Specifically, we first develop a novel skill entity extraction approach based on a bidirectional Long Short-Term Memory (LSTM) with a Conditional Random Field (CRF) layer (LSTM-CRF) neural network enhanced with adapted gate mechanism. In particular, to improve the reliability of extracted skill entities, we design a label propagation method based on more than 10 billion click-through data from the large-scale Baidu query logs. Furthermore, we discover the hypernym-hyponym relations between skill entities and construct the Skill-Graph by leveraging the classifier trained with extensive contextual features. Finally, we design a personalized question recommendation algorithm based on the Skill-Graph for improving the efficiency and effectiveness of job interview assessment. Extensive experiments on real-world recruitment data clearly validate the effectiveness of DuerQuiz, which had been deployed for generating written exercises in the 2018 Baidu campus recruitment event and received remarkable performances in terms of efficiency and effectiveness for selecting outstanding talents compared with a traditional non-personalized human-only assessment approach.},
address = {New York, NY, USA},
author = {Qin, Chuan and Zhu, Hengshu and Zhu, Chen and Xu, Tong and Zhuang, Fuzhen and Ma, Chao and Zhang, Jingshuai and Xiong, Hui},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3292500.3330706},
isbn = {9781450362016},
keywords = {intelligence interview system,question recommendation},
pages = {2165--2173},
publisher = {Association for Computing Machinery},
series = {KDD '19},
title = {{DuerQuiz: A Personalized Question Recommender System for Intelligent Job Interview}},
url = {https://doi.org/10.1145/3292500.3330706},
year = {2019}
}
@article{Fang2020,
abstract = {Online product reviews are an important driver of customers' purchasing behavior. Fake reviews seriously mislead consumers, challenging the fairness of the online shopping environment. Although the detection of fake reviews has progressed, several problems remain. First, fake comment recognition ignores the correlation between time and the semantics of the comment texts, which is always hidden in the context of the reviews. Second, the impact of multi-source information on fake comment recognition is not considered, as it constitutes a complex, high-dimensional, heterogeneous relationship between reviewers, reviews, stores and commodities. To overcome these problems, the present paper proposes a dynamic knowledge graph-based method for fake-review detection. Based on the characteristics of online product reviews, it first extracts four types of entities using a developed neural network model called sentence vector/twin-word embedding conditioned bidirectional long short-term memory. Time series related features are then added to the knowledge graph construction process, forming dynamic graph networks. To enhance the fake-review detection, four indicators are newly defined for determining the relationships among the four types of nodes. In experimental evaluations, our method surpassed the state-of-the-art results. {\textcopyright} 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
annote = {cited By 0},
author = {Fang, Y and Wang, H and Zhao, L and Yu, F and Wang, C},
doi = {10.1007/s10489-020-01761-w},
issn = {0924669X},
journal = {Applied Intelligence},
keywords = {Electronic trading; Knowledge representation; Sema,Experimental evaluation; Fake review detections;,Graphic methods},
publisher = {Springer},
title = {{Dynamic knowledge graph based fake-review detection}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087897494&doi=10.1007%2Fs10489-020-01761-w&partnerID=40&md5=ec52daed3487c9d1ad4bba5c4f181513},
year = {2020}
}
@inproceedings{10.1145/3394486.3403209,
abstract = {Modeling concurrent events of multiple types and their involved actors from open-source social sensors is an important task for many domains such as health care, disaster relief, and financial analysis. Forecasting events in the future can help human analysts better understand global social dynamics and make quick and accurate decisions. Anticipating participants or actors who may be involved in these activities can also help stakeholders to better respond to unexpected events. However, achieving these goals is challenging due to several factors: (i) it is hard to filter relevant information from large-scale input, (ii) the input data is usually high dimensional, unstructured, and Non-IID (Non-independent and identically distributed) and (iii) associated text features are dynamic and vary over time. Recently, graph neural networks have demonstrated strengths in learning complex and relational data. In this paper, we study a temporal graph learning method with heterogeneous data fusion for predicting concurrent events of multiple types and inferring multiple candidate actors simultaneously. In order to capture temporal information from historical data, we propose Glean, a graph learning framework based on event knowledge graphs to incorporate both relational and word contexts. We present a context-aware embedding fusion module to enrich hidden features for event actors. We conducted extensive experiments on multiple real-world datasets and show that the proposed method is competitive against various state-of-the-art methods for social event prediction and also provides much-need interpretation capabilities.},
address = {New York, NY, USA},
author = {Deng, Songgaojun and Rangwala, Huzefa and Ning, Yue},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3394486.3403209},
isbn = {9781450379984},
keywords = {knowledge graphs,multi-event forecasting,word graphs},
pages = {1585--1595},
publisher = {Association for Computing Machinery},
series = {KDD '20},
title = {{Dynamic Knowledge Graph Based Multi-Event Forecasting}},
url = {https://doi.org/10.1145/3394486.3403209},
year = {2020}
}
@article{8966369,
abstract = {Numerous knowledge bases have been published on the web, and there are serious heterogeneous problems among them. Unifying these knowledge bases at the semantic level can better promote the development of the Linked Data Project. Various effective methods, the mainstream one of which is the iterative entity alignment algorithm based on TransE (Translation-based Embedding, an efficient knowledge graph embedding representation algorithm), have been put forward to solve the heterogeneous problems among knowledge bases. Although the TransE-based iterative entity alignment algorithm can shorten the distance between two entities with the same semantics, it has low accuracy because it ignores the importance of semantic aggregation generated by many attributes of entities in the entity alignment process. To solve this problem, a novel entity alignment algorithm based on semantic aggregation and attribute attention, named EASA, is proposed in this paper. On the one hand, semantic aggregation of entities can be generated by different attributes and attribute values. On the other hand, the addition of attribute attention can be used to distinguish the different roles of different attributes in the entity alignment process. The experimental results show that our method achieves significant improvements compared to baselines for entity alignment on Chinese and English datasets. The data and source code for this paper can be obtained from https://www.github.com/xinan711456/EASA.},
author = {Huang, L and Luo, X},
doi = {10.1109/ACCESS.2020.2968620},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {graph theory;iterative methods;knowledge based sys},
pages = {18162--18170},
title = {{EASA: Entity Alignment Algorithm Based on Semantic Aggregation and Attribute Attention}},
volume = {8},
year = {2020}
}
@article{Inan201715,
abstract = {RDF embeddings are recently used in Entity Linking systems for disambiguation of candidate entities to match the best mention and entity pairs. In this study, we evaluate the effect of enriched ontology structures for disambiguation task when RDF embeddings are used to identify semantic relatedness between knowledge base concepts. We generate a domain-specific core ontology and put new components upon previous ontology structures. In this way, we obtain four different enriched structures and transform them into RDF embeddings. Then, we observe which enriched structure has more importance to enhance the overall performance of RDF embeddings-based Entity Linking approaches. We select two well-known knowledge-base-agnostic approaches, including AGDISTIS and DoSeR and adapt them into RDF embeddings-based entity disambiguation. Finally, a domain-specific evaluation dataset is generated from Wikipedia to observe the effect of enriched structures on these adapted approaches. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 2; Conference of 11th International Conference on Metadata and Semantic Research, MTSR 2017 ; Conference Date: 28 November 2017 Through 1 December 2017; Conference Code:207139},
author = {Inan, E and Dikenelli, O},
doi = {10.1007/978-3-319-70863-8_2},
editor = {{Koutsomiha D. Garoufallou E.}, Siatri R Virkus S},
isbn = {9783319708621},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Domain specific; Embeddings; Entity disambiguatio,Knowledge based systems; Metadata; Natural languag,Semantic Web},
pages = {15--24},
publisher = {Springer Verlag},
title = {{Effect of enriched ontology structures on RDF embedding-based entity linking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036636261&doi=10.1007%2F978-3-319-70863-8_2&partnerID=40&md5=a36a52c1fd8e59deff8e4827538bb093},
volume = {755},
year = {2017}
}
@article{Domingo2020,
abstract = {Ancient Egyptians had a complex religion, which was active for longer than the time that has passed since Cleopatra until our days. One amazing belief was to be buried with funerary statuettes to help the deceased carry out his/her tasks in the underworld. These funerary statuettes, mainly known as shabtis, were produced in different materials and were usually inscribed in hieroglyphs with formulas including the name of the deceased. Shabtis are important archaeological objects which can help to identify the owners, their jobs, ranks or their families. They are also used for tomb dating because, depending on different elements: color, formula, tools, wig, hand positions, etc., it is possible to associate them to a concrete type or period of time. Shabtis are spread all over the world, in excavations, museums or private collections, and many of them have not been studied and identified because this process requires a deep study and reading of the hieroglyphs. Our system is able to solve this problem using two different YOLO v3 networks for detecting the figure itself and the hieroglyphic names, which provide identification and cataloguing. Until now, there has been no other work on the detection and identification of shabtis. In addition, a semantic approach has been followed, creating an ontology to connect our system with the semantic metadata aggregator, Europeana, linking our results with known shabtis in different museums. A complete dataset has been created, a comparison with previous technologies for similar problems has been provided, such as SIFT in the ancient coin classification, and the results of identification and cataloguing are shown. These results are over similar problems and have led us to create a web application that shows our system and is available on line. {\textcopyright} 2020 by the authors.},
annote = {cited By 0},
author = {Domingo, J D and G{\'{o}}mez-Garc{\'{i}}a-Bermejo, J and Zalama, E},
doi = {10.3390/APP10186408},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
number = {18},
publisher = {MDPI AG},
title = {{Egyptian shabtis identification by means of deep neural networks and semantic integration with europeana}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092039153&doi=10.3390%2FAPP10186408&partnerID=40&md5=decdbab5350ac984a7e4456f1e74d6f9},
volume = {10},
year = {2020}
}
@inproceedings{10.1145/3357384.3357897,
abstract = {Assigning standard medical codes (e.g., ICD-9-CM) representing diagnoses or procedures to electronic health record (EHR) is an important task in the medical domain. However, automatic coding is difficult since the clinical note is composed of multiple long and heterogeneous textual narratives (e.g., discharge diagnosis, pathology reports, surgical procedure notes). Furthermore, the code label space is large and the label distribution is extremely unbalanced. The state-of-the-art methods mainly regard EHR coding as a multi-label text classification task and use shallow convolution neural network with fixed window size, which is incapable of learning variable n-gram features and the ontology structure between codes. In this paper, we leverage a densely connected convolutional neural network which is able to produce variable n-gram features for clinical note feature learning. We also incorporate a multi-scale feature attention to adaptively select multi-scale features since the most informative n-grams in clinical notes for each word can vary in length according to the neighborhood. Furthermore, we leverage graph convolutional neural network to capture both the hierarchical relationships among medical codes and the semantics of each code. Finally, We validate our method on the public dataset, and the evaluation results indicate that our method can significantly outperform other state-of-the-art models.},
address = {New York, NY, USA},
author = {Xie, Xiancheng and Xiong, Yun and Yu, Philip S and Zhu, Yangyong},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3357384.3357897},
isbn = {9781450369763},
keywords = {densely connected cnn,ehr coding,graph convolutional neural network,multi-scale feature attention},
pages = {649--658},
publisher = {Association for Computing Machinery},
series = {CIKM '19},
title = {{EHR Coding with Multi-Scale Feature Attention and Structured Knowledge Graph Propagation}},
url = {https://doi.org/10.1145/3357384.3357897},
year = {2019}
}
@article{10.1016/j.asoc.2015.01.007,
abstract = {The structure of the EEG-based emotion assessment system and arousal-valence space (two arousal levels: low arousal and high arousal; two valence levels: low valence and high valence). Soft computing techniques used: ontologies, statistical tests, and a classification algorithm.EEG- and emotion-related knowledge are represented in the form of semantic sentences.The system maps EEG features to arousal-valence space using decision tree algorithm.Significant correlations can be found between EEG features and arousal-valence space in the system. Currently, emotion is considered as a critical aspect of human behavior; thus it should be embedded within the reasoning module in an intelligent system where the aim is to anticipate or respond to human reactions. Therefore, current research in data mining shows an increasing interest in emotion assessment for improving human-machine interaction. Based on the analysis of electroencephalogram (EEG) which derives from automatic nervous system responses, computers can assess user emotions and find correlations between significant EEG features extracted from the raw data and the human emotional states. With the advent of modern signal processing techniques, the evaluative power of human emotion derived from EEG is increased exponentially due to the huge number of features that are typically extracted from the EEG signals. Notwithstanding that the expanded set of features could allow computers to evaluate emotions in an accurate way, it is too complex a task to manage in a structured way and, for the reasons stated, methods and approaches to enable both EEG information management and evaluation are necessary to support emotion assessment. Starting from this consideration, this paper proposes an enhanced EEG-based emotion assessment system exploiting a collection of ontological models representing EEG feature sets and arousal-valence space (two-dimensional emotion scale), statistical tests capable of evaluating the gender-specific correlations between EEG features and emotional states, and a classification methodology inferring arousal and valence levels. As will be shown in the experimental section where the proposed approach has been tested on a public dataset, the experimental results demonstrate that better performance in emotion assessment can be achieved using our framework as compared with other studies using the same dataset and with three other classification techniques.},
address = {NLD},
author = {Chen, Jing and Hu, Bin and Moore, Philip and Zhang, Xiaowei and Ma, Xu},
doi = {10.1016/j.asoc.2015.01.007},
issn = {1568-4946},
journal = {Appl. Soft Comput.},
keywords = {Electroencephalogram,Emotion assessment,Human-machine interaction,Ontology},
month = {may},
number = {C},
pages = {663--674},
publisher = {Elsevier Science Publishers B. V.},
title = {{Electroencephalogram-Based Emotion Assessment System Using Ontology and Data Mining Techniques}},
url = {https://doi.org/10.1016/j.asoc.2015.01.007},
volume = {30},
year = {2015}
}
@article{Espinosa-Anke2017355,
abstract = {Music consumption habits as well as the Music market have changed dramatically due to the increasing popularity of digital audio and streaming services. Today, users are closer than ever to a vast number of songs, albums, artists and bands. However, the challenge remains in how to make sense of all the data available in the Music domain, and how current state of the art in Natural Language Processing and semantic technologies can contribute in Music Information Retrieval areas such as music recommendation, artist similarity or automatic playlist generation. In this paper, we present and evaluate a distributional sense-based embeddings model in the music domain, which can be easily used for these tasks, as well as a device for improving artist or album clustering. The model is trained on a disambiguated corpus linked to the MusicBrainz musical Knowledge Base, and following current knowledge-based approaches to sense-level embeddings, entity-related vectors are provided {\`{a}} la WordNet, concatenating the id of the entity and its mention. The model is evaluated both intrinsically and extrinsically in a supervised entity typing task, and released for the use and scrutiny of the community. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 2; Conference of 14th International Conference on Semantic Web, ESWC 2017 ; Conference Date: 28 May 2017 Through 1 June 2017; Conference Code:204389},
author = {Espinosa-Anke, L and Oramas, S and Saggion, H and Serra, X},
doi = {10.1007/978-3-319-70407-4_44},
editor = {{Blomqvist E. Hartig O.}, Paulheim H Hose K Ciravegna F Lawrynowicz A},
isbn = {9783319704067},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Artist similarities; Automatic playlist generatio,Audio acoustics; Content based retrieval; Knowledg,Semantic Web},
pages = {355--366},
publisher = {Springer Verlag},
title = {{ELMDist: A Vector Space Model with Words and MusicBrainz Entities}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034220153&doi=10.1007%2F978-3-319-70407-4_44&partnerID=40&md5=2ca1138ea857c62ae2e65a8884c43971},
volume = {10577 LNCS},
year = {2017}
}
@article{Metcalf2018264,
abstract = {Case indexing decisions must often confront the tradeoff between rich semantic indexing schemes, which provide effective retrieval at large indexing cost, and shallower indexing schemes, which enable low-cost indexing but may be less reliable. Indexing for textual case-based reasoning is often based on information retrieval approaches that minimize index acquisition cost but sacrifice semantic information. This paper presents JointEmbed, a method for automatically generating rich indices. JointEmbed automatically generates continuous vector space embeddings that implicitly capture semantic information, leveraging multiple knowledge sources such as free text cases and pre-existing knowledge graphs. JointEmbed generates effective indices by applying pTransR, a novel approach for modelling knowledge graphs, to encode and summarize contents of domain knowledge resources. JointEmbed is applied to the medical CBR task of retrieving relevant patient electronic health records, for which potential health consequences make retrieval quality paramount. An evaluation supports that JointEmbed outperforms previous methods. {\textcopyright} 2018, Springer Nature Switzerland AG.},
annote = {cited By 1; Conference of 26th International Conference on Case-Based Reasoning, ICCBR 2018 ; Conference Date: 9 July 2018 Through 12 July 2018; Conference Code:219569},
author = {Metcalf, K and Leake, D},
doi = {10.1007/978-3-030-01081-2_18},
editor = {{Cox M.T. Funk P.}, Begum S},
isbn = {9783030010805},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Acquisition costs; Electronic health record; Info,Case based reasoning,Health; Indexing (of information); Knowledge manag},
pages = {264--280},
publisher = {Springer Verlag},
title = {{Embedded Word Representations for Rich Indexing: A Case Study for Medical Records}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055690796&doi=10.1007%2F978-3-030-01081-2_18&partnerID=40&md5=7a5b3ffe97fc02a55d88fac39f8b316c},
volume = {11156 LNAI},
year = {2018}
}
@article{Wang2018141,
abstract = {Representation learning of knowledge graphs encodes entities and relation types into a continuous low-dimensional vector space, learns embeddings of entities and relation types. Most existing methods only concentrate on knowledge triples, ignoring logic rules which contain rich background knowledge. Although there has been some work aiming at leveraging both knowledge triples and logic rules, they ignore the transitivity and asymmetry of logic rules. In this paper, we propose a novel approach to learn knowledge representations with entities and ordered relations in knowledges and logic rules. The key idea is to integrate knowledge triples and logic rules, and approximately order the relation types in logic rules to utilize the transitivity and asymmetry of logic rules. All entries of the embeddings of relation types are constrained to be non-negative. We translate the general constrained optimization problem into an unconstrained optimization problem to solve the non-negative matrix factorization. Experimental results show that our model significantly outperforms other baselines on knowledge graph completion task. It indicates that our model is capable of capturing the transitivity and asymmetry information, which is significant when learning embeddings of knowledge graphs. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
annote = {cited By 1; Conference of 22nd Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2018 ; Conference Date: 3 June 2018 Through 6 June 2018; Conference Code:214589},
author = {Wang, M and Rong, E and Zhuo, H and Zhu, H},
doi = {10.1007/978-3-319-93037-4_12},
editor = {{Ho B. Phung D.}, Webb G I Tseng V S Ganji M Rashidi L},
isbn = {9783319930367},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Asymmetry; Knowledge graphs; Logic rules; Nonnega,Computer circuits,Constrained optimization; Data mining; Factorizati},
pages = {141--153},
publisher = {Springer Verlag},
title = {{Embedding knowledge graphs based on transitivity and asymmetry of rules}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049366703&doi=10.1007%2F978-3-319-93037-4_12&partnerID=40&md5=238299871df5030b79adda860b57c624},
volume = {10938 LNAI},
year = {2018}
}
@article{Zhao2019,
abstract = {Embedding learning on knowledge graphs (KGs) aims to encode all entities and relationships into a continuous vector space, which provides an effective and flexible method to implement downstream knowledge-driven artificial intelligence (AI) and natural language processing (NLP) tasks. Since KG construction usually involves automatic mechanisms with less human supervision, it inevitably brings in plenty of noises to KGs. However, most conventional KG embedding approaches inappropriately assume that all facts in existing KGs are completely correct and ignore noise issues, which brings about potentially serious errors. To address this issue, in this paper we propose a novel approach to learn embeddings with triple trustiness on KGs, which takes possible noises into consideration. Specifically, we calculate the trustiness value of triples according to the rich and relatively reliable information from large amounts of entity type instances and entity descriptions in KGs. In addition, we present a cross-entropy based loss function for model optimization. In experiments, we evaluate our models on KG noise detection, KG completion and classification. Through extensive experiments on three datasets, we demonstrate that our proposed model can learn better embeddings than all baselines on noisy KGs. {\textcopyright} 2019 by the authors.},
annote = {cited By 1},
author = {Zhao, Y and Feng, H and Gallinari, P},
doi = {10.3390/e21111083},
issn = {10994300},
journal = {Entropy},
number = {11},
publisher = {MDPI AG},
title = {{Embedding learning with triple trustiness on noisy knowledge graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075465861&doi=10.3390%2Fe21111083&partnerID=40&md5=b36481ea4e03b9b6fc51e0dacbd942a5},
volume = {21},
year = {2019}
}
@article{Zhang2019514,
abstract = {The basic unit of knowledge graph is triplet, including head entity, relation and tail entity. Centering on knowledge graph, knowledge graph completion has attracted more and more attention and made great progress. However, these models are all verified by open domain data sets. When applied in specific domain case, they will be challenged by practical data distributions. For example, due to poor presentation of tail entities caused by their relation-oriented feature, they can not deal with the completion of enzyme knowledge graph. Inspired by question answering and rectilinear propagation of lights, this paper puts forward a tail-oriented method - Embedding for Multi-Tails knowledge graph (EMT). Specifically, it first represents head and relation in question space; then, finishes projection to answer one by tail-related matrix; finally, gets tail entity via translating operation in answer space. To overcome time-space complexity of EMT, this paper includes two improved models: EMTv and EMTs. Taking some optimal translation and composition models as baselines, link prediction and triplets classification on an enzyme knowledge graph sample and Kinship proved our performance improvements, especially in tails prediction. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 0; Conference of 23rd Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2019 ; Conference Date: 14 April 2019 Through 17 April 2019; Conference Code:225109},
author = {Zhang, Y and Du, Z and Meng, X},
doi = {10.1007/978-3-030-16142-2_40},
editor = {{Yang Q. Huang S.-J.}, Zhang M.-L. Zhou Z.-H. Gong Z},
isbn = {9783030161415},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining,Domain knowledge; Embedding; Knowledge graphs; Or,Embeddings; Enzymes},
pages = {514--527},
publisher = {Springer Verlag},
title = {{EMT: A tail-oriented method for specific domain knowledge graph completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065011305&doi=10.1007%2F978-3-030-16142-2_40&partnerID=40&md5=048c972f386d202033c3500b70d2e0f6},
volume = {11441 LNAI},
year = {2019}
}
@article{Sorokin201770,
abstract = {In this paper we present a knowledge base question answering system for participation in Task 4 of the QALD-7 shared task. Our system is an end-to-end neural architecture for constructing a structural semantic representation of a natural language question. We define semantic representations as graphs that are generated step-wise and can be translated into knowledge base queries to retrieve answers. We use a convolutional neural network (CNN) model to learn vector encodings for the questions and the semantic graphs and use it to select the best matching graph for the input question. We show on two different datasets that our system is able to successfully generalize to new data. {\textcopyright} 2017, Springer International Publishing AG.},
annote = {cited By 8; Conference of 4th International Conference on Semantic Web Evaluation Challenge, SemWebEval 2017 ; Conference Date: 28 May 2017 Through 1 June 2017; Conference Code:203919},
author = {Sorokin, D and Gurevych, I},
doi = {10.1007/978-3-319-69146-6_7},
editor = {{Dragoni M. Solanki M.}, Blomqvist E},
isbn = {9783319691459},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Convolution; Knowledge based systems; Natural lang,Convolutional neural network; Question Answering;,Semantic Web},
pages = {70--83},
publisher = {Springer Verlag},
title = {{End-to-end representation learning for question answering with weak supervision}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034253573&doi=10.1007%2F978-3-319-69146-6_7&partnerID=40&md5=36e1ff1bc7359886b982d0ab4a52f7e5},
volume = {769},
year = {2017}
}
@inproceedings{Melacci20191012,
abstract = {Supervised models for Word Sense Disambiguation (WSD) currently yield to state-of-the-art results in the most popular benchmarks. Despite the recent introduction of Word Embeddings and Recurrent Neural Networks to design powerful context-related features, the interest in improving WSD models using Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based approaches. In this paper, we enhance “modern” supervised WSD models exploiting two popular SLRs: WordNet and WordNet Domains. We propose an effective way to introduce semantic features into the classifiers, and we consider using the SLR structure to augment the training data. We study the effect of different types of semantic features, investigating their interaction with local contexts encoded by means of mixtures of Word Embeddings or Recurrent Neural Networks, and we extend the proposed model into a novel multi-layer architecture for WSD. A detailed experimental comparison in the recent Unified Evaluation Framework (Raganato et al., 2017) shows that the proposed approach leads to supervised models that compare favourably with the state-of-the art. {\textcopyright} LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved.},
annote = {cited By 3; Conference of 11th International Conference on Language Resources and Evaluation, LREC 2018 ; Conference Date: 7 May 2018 Through 12 May 2018; Conference Code:143414},
author = {Melacci, S and Globo, A and Rigutini, L},
booktitle = {LREC 2018 - 11th International Conference on Language Resources and Evaluation},
editor = {{Isahara H. Maegaard B.}, Piperidis S Cieri C Declerck T Hasida K Mazo H Choukri K Goggi S Mariani J Moreno A Calzolari N Odijk J Tokunaga T},
isbn = {9791095546009},
keywords = {Classification (of information); Knowledge based s,Evaluation framework; Experimental comparison; Kn,Natural language processing systems},
pages = {1012--1017},
publisher = {European Language Resources Association (ELRA)},
title = {{Enhancing modern supervised word sense disambiguation models by semantic lexical resources}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059908637&partnerID=40&md5=08140c53c56cbf1f0a2b693f96e7e2dc},
year = {2019}
}
@inproceedings{10.1145/3397271.3401213,
abstract = {Top-N recommendations are widely applied in various real life domains and keep attracting intense attention from researchers and industry due to available multi-type information, new advances in AI models and deeper understanding of user satisfaction. Whileaccuracy has been the prevailing issue of the recommendation problem for the last decades, other facets of the problem, namelydiversity andexplainability, have received much less attention. In this paper, we focus on enhancing diversity of top-N recommendation, while ensuring the trade-off between accuracy and diversity. Thus, we propose an effective framework DivKG leveraging knowledge graph embedding and determinantal point processes (DPP). First, we capture different kinds of relations among users, items and additional entities through a knowledge graph structure. Then, we represent both entities and relations as k-dimensional vectors by optimizing a margin-based loss with all kinds of historical interactions. We use these representations to construct kernel matrices of DPP in order to make top-N diversified predictions. We evaluate our framework on MovieLens datasets coupled with IMDb dataset. Our empirical results show substantial improvement over the state-of-the-art regarding both accuracy and diversity metrics.},
address = {New York, NY, USA},
author = {Gan, Lu and Nurbakova, Diana and Laporte, L{\'{e}}a and Calabretto, Sylvie},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3397271.3401213},
isbn = {9781450380164},
keywords = {determinantal point processes,diversity,knowledge graph,recommender systems},
pages = {2001--2004},
publisher = {Association for Computing Machinery},
series = {SIGIR '20},
title = {{Enhancing Recommendation Diversity Using Determinantal Point Processes on Knowledge Graphs}},
url = {https://doi.org/10.1145/3397271.3401213},
year = {2020}
}
@article{Diamantopoulos2018960,
abstract = {Enhancing the requirements elicitation process has always been of added value to software engineers, since it expedites the software lifecycle and reduces errors in the conceptualization phase of software products. The challenge posed to the research community is to construct formal models that are capable of storing requirements from multimodal formats (text and UML diagrams) and promote easy requirements reuse, while at the same time being traceable to allow full control of the system design, as well as comprehensible to software engineers and end users. In this work, we present an approach that enhances requirements reuse while capturing the static (functional requirements, use case diagrams) and dynamic (activity diagrams) view of software projects. Our ontology-based approach allows for reasoning over the stored requirements, while the mining methodologies employed detect incomplete or missing software requirements, this way reducing the effort required for requirements elicitation at an early stage of the project lifecycle. {\textcopyright} 2017, {\textcopyright} 2017 Informa UK Limited, trading as Taylor \& Francis Group.},
annote = {cited By 4},
author = {Diamantopoulos, T and Symeonidis, A},
doi = {10.1080/17517575.2017.1416177},
issn = {17517575},
journal = {Enterprise Information Systems},
keywords = {Computer software reusability; Life cycle; Ontolog,Data mining,Functional requirement; Multi-modal; Requirements},
number = {8-9},
pages = {960--981},
publisher = {Taylor and Francis Ltd.},
title = {{Enhancing requirements reusability through semantic modeling and data mining techniques}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054831990&doi=10.1080%2F17517575.2017.1416177&partnerID=40&md5=d16f4fb6fa7564ed1c25d45107456fd1},
volume = {12},
year = {2018}
}
@inproceedings{Celikyilmaz201539,
abstract = {Unsupervised word embeddings provide rich linguistic and conceptual information about words. However, they may provide weak information about domain specific semantic relations for certain tasks such as semantic parsing of natural language queries, where such information about words can be valuable. To encode the prior know ledge about the semantic word relations, we present new method as follows: we extend the neural network based lexical word embedding objective function (Mikolov et al. 2013) by incorporating the information about relationship between entities that we extract from knowledge bases. Our model can jointly learn lexical word representations from free text enriched by the relational word embeddings from relational data (e.g. Freebase) for each type of entity relations. We empirically show on the task of semantic tagging of natural language queries that our enriched embeddings can provide information about not only short-range syntactic dependencies but also long-range semantic dependencies between words. Using the enriched embeddings, we obtain an average of 2% improvement in F-score compared to the previous baselines. Copyright {\textcopyright} 2015. Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
annote = {cited By 9; Conference of 2015 AAAI Spring Symposium ; Conference Date: 23 March 2015 Through 25 March 2015; Conference Code:113922},
author = {Celikyilmaz, A and Hakkani-Tiir, D and Pasupat, P and Sarikaya, R},
booktitle = {AAAI Spring Symposium - Technical Report},
isbn = {9781577357070},
keywords = {Domain specific semantics; Knowledge graphs; Natu,Knowledge representation; Natural language process,Semantics},
pages = {39--42},
publisher = {AI Access Foundation},
title = {{Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987629985&partnerID=40&md5=deb001215138bdab456b70f41603852e},
volume = {SS-15-03},
year = {2015}
}
@inproceedings{8599288,
abstract = {Entity alignment across knowledge graphs is an important task in web mining. The aligned entities can be used for transferring knowledge across knowledge graphs and benefit several tasks such as cross-lingual knowledge graph construction and knowledge reasoning. This paper propose a representation learning based algorithm for embedding knowledge graph and aligning entities. In particular, considering the multi-type relations in knowledge graph, we select the alignment-task driven representative relations based on the pre-aligned entity pairs. With the help of selected relations, we embed the entities across networks into a common space by modeling entities' head/tail are with corresponding context vectors. For entity alignment task, pre-aligned entities are adopted to facilitate the transfer of context information across the knowledges graphs. Through this way, the problem of entity embedding and alignment can be solved simultaneously under a unified framework. Extensive experiments on two multi-lingual knowledge graphs demonstrate the effectiveness of the proposed model comparing with several state-of-the-art models.},
author = {Zhang, Y and Liu, L and Fu, S and Zhong, F},
booktitle = {2018 5th International Conference on Systems and Informatics (ICSAI)},
doi = {10.1109/ICSAI.2018.8599288},
keywords = {data mining;graph theory;Internet;knowledge based},
month = {nov},
pages = {1056--1061},
title = {{Entity Alignment Across Knowledge Graphs Based on Representative Relations Selection}},
year = {2018}
}
@article{Basaldella2017,
abstract = {Background: This article describes a high-recall, high-precision approach for the extraction of biomedical entities from scientific articles. Method: The approach uses a two-stage pipeline, combining a dictionary-based entity recognizer with a machine-learning classifier. First, the OGER entity recognizer, which has a bias towards high recall, annotates the terms that appear in selected domain ontologies. Subsequently, the Distiller framework uses this information as a feature for a machine learning algorithm to select the relevant entities only. For this step, we compare two different supervised machine-learning algorithms: Conditional Random Fields and Neural Networks. Results: In an in-domain evaluation using the CRAFT corpus, we test the performance of the combined systems when recognizing chemicals, cell types, cellular components, biological processes, molecular functions, organisms, proteins, and biological sequences. Our best system combines dictionary-based candidate generation with Neural-Network-based filtering. It achieves an overall precision of 86% at a recall of 60% on the named entity recognition task, and a precision of 51% at a recall of 49% on the concept recognition task. Conclusion: These results are to our knowledge the best reported so far in this particular task. {\textcopyright} 2017 The Author(s).},
annote = {cited By 8},
author = {Basaldella, M and Furrer, L and Tasso, C and Rinaldi, F},
doi = {10.1186/s13326-017-0157-6},
issn = {20411480},
journal = {Journal of Biomedical Semantics},
keywords = {Algorithms; Data Mining; Machine Learning; Neural,Automated; Reproducibility of Results; Semantics;,Controlled,algorithm; artificial neural network; automated pa},
number = {1},
publisher = {BioMed Central Ltd.},
title = {{Entity recognition in the biomedical domain using a hybrid approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034617345&doi=10.1186%2Fs13326-017-0157-6&partnerID=40&md5=5ea0462528f2fb0d12864e81690e8c42},
volume = {8},
year = {2017}
}
@inproceedings{Biswas2020,
abstract = {Open Knowledge Graphs (such as DBpedia, Wikidata, YAGO) has been recognized as the backbone of diverse applications in the field of data mining and information retrieval. Hence, the completeness and correctness of the Knowledge Graphs (KGs) is vital. Most of these KGs are mostly created either via an automated information extraction from Wikipedia snapshots or information accumulation provided by the users or using heuristics. However, it has been observed that the type information of these KGs is often noisy, incomplete and incorrect. To deal with this problem a multi-label classification approach is proposed in this work for entity typing using KG embeddings. We compare our approach with the current state-of-the-art type prediction method and report on experiments with the KGs. Copyright {\textcopyright} 2020 for this paper by its authors.},
annote = {cited By 0; Conference of 3rd Workshop on Deep Learning for Knowledge Graphs, DL4KG 2020 ; Conference Date: 2 June 2020; Conference Code:161657},
author = {Biswas, R and Sofronova, R and Alam, M and Sack, H},
booktitle = {CEUR Workshop Proceedings},
editor = {{Alam M. Buscaldi D.}, Cochez M Osborne F Recupero D R Sack H},
issn = {16130073},
keywords = {Classification (of information); Deep learning; Em,Data mining,Diverse applications; Entity-types; Information a},
publisher = {CEUR-WS},
title = {{Entity type prediction in knowledge graphs using embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091062793&partnerID=40&md5=a69cb24c9e1f25442477ee8c999f1aec},
volume = {2635},
year = {2020}
}
@inproceedings{7552082,
abstract = {We present an ensemble approach for categorizing search query entities in the recruitment domain. Understanding the types of entities expressed in a search query (Company, Skill, Job Title, etc.) enables more intelligent information retrieval based upon those entities compared to a traditional keyword-based search. Because search queries are typically very short, leveraging a traditional bag-of-words model to identify entity types would be inappropriate due to the lack of contextual information. Our approach instead combines clues from different sources of varying complexity in order to collect real-world knowledge about query entities. We employ distributional semantic representations of query entities through two models: 1) contextual vectors generated from encyclopedic corpora like Wikipedia, and 2) high dimensional word embedding vectors generated from millions of job postings using word2vec. Additionally, our approach utilizes both entity linguistic properties obtained from WordNet and ontological properties extracted from DBpedia. We evaluate our approach on a data set created at CareerBuilder, the largest job board in the US. The data set contains entities extracted from millions of job seekers/recruiters search queries, job postings, and resume documents. After constructing the distributional vectors of search entities, we use supervised machine learning to infer search entity types. Empirical results show that our approach outperforms the state-of-the-art word2vec distributional semantics model trained on Wikipedia. Moreover, we achieve micro-averaged F1 score of 97% using the proposed distributional representations ensemble.},
author = {Shalaby, W and {Al Jadda}, K and Korayem, M and Grainger, T},
booktitle = {2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)},
doi = {10.1109/COMPSAC.2016.109},
issn = {0730-3157},
keywords = {learning (artificial intelligence);ontologies (art},
month = {jun},
pages = {631--636},
title = {{Entity Type Recognition Using an Ensemble of Distributional Semantic Models to Enhance Query Understanding}},
volume = {1},
year = {2016}
}
@inproceedings{Zhang20201441,
abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/ERNIE. {\textcopyright} 2019 Association for Computational Linguistics},
annote = {cited By 20; Conference of 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019 ; Conference Date: 28 July 2019 Through 2 August 2019; Conference Code:159206},
author = {Zhang, Z and Han, X and Liu, Z and Jiang, X and Sun, M and Liu, Q},
booktitle = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
isbn = {9781950737482},
keywords = {Computational linguistics,External knowledge; Knowledge graphs; Knowledge i,Natural language processing systems; Semantics},
pages = {1441--1451},
publisher = {Association for Computational Linguistics (ACL)},
title = {{ErniE: Enhanced language representation with informative entities}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084078111&partnerID=40&md5=cfe166d215db5d03ec646d13bc3f4770},
year = {2020}
}
@inproceedings{Xiang20152419,
abstract = {As a key representation model of knowledge, ontology has been widely used in a lot of NLP related tasks, such as semantic parsing, information extraction and text mining etc. In this paper, we study the task of ontology matching, which concentrates on finding semantically related entities between different ontologies that describe the same domain, to solve the semantic heterogeneity problem. Previous works exploit different kinds of descriptions of an entity in ontology directly and separately to find the correspondences without considering the higher level correlations between the descriptions. Besides, the structural information of ontology haven't been utilized adequately for ontology matching. We propose in this paper an ontology matching approach, named ERSOM, which mainly includes an unsupervised representation learning method based on the deep neural networks to learn the general representation of the entities and an iterative similarity propagation method that takes advantage of more abundant structure information of the ontology to discover more mappings. The experimental results on the datasets from Ontology Alignment Evaluation Initiative (OAEI1) show that ER-SOM achieves a competitive performance compared to the state-of-the-art ontology matching systems. {\textcopyright} 2015 Association for Computational Linguistics.},
annote = {cited By 12; Conference of Conference on Empirical Methods in Natural Language Processing, EMNLP 2015 ; Conference Date: 17 September 2015 Through 21 September 2015; Conference Code:116677},
author = {Xiang, C and Jiang, T and Chang, B and Sui, Z},
booktitle = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/d15-1289},
isbn = {9781941643327},
keywords = {Backpropagation; Data mining; Deep neural networks,Competitive performance; Iterative similarity pro,Ontology},
pages = {2419--2429},
publisher = {Association for Computational Linguistics (ACL)},
title = {{ERSOM: A structural ontology matching approach using automatically learned entity representation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959925534&doi=10.18653%2Fv1%2Fd15-1289&partnerID=40&md5=6482e8620a668de16d1749a27f0d2034},
year = {2015}
}
@inproceedings{Wei201940,
abstract = {Entity summarization task aims at creating brief but informative descriptions of entities from Knowledge Graph. While previous work mostly focuses on traditional techniques such as clustering algorithms and graph models, we make an attempt to integrate deep learning methods into this task. In this paper, we propose an Entity Summarization with Attention (ESA) model, which is a neural network with supervised attention mechanisms for entity summarization. Specifically, we first calculate attention weights for facts in each entity. Then, we rank facts to generate reliable summaries. We explore techniques to solve complex learning problems presented by the ESA. On several benchmarks, experimental results show that ESA improves the quality of the entity summaries in both F-measure and MAP compared with some state-of-the-art methods, demonstrating the effectiveness of ESA. The source code and output can be accessed in https://github.com/WeiDongjunGabriel/ESA1 1Copyright @ 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
annote = {cited By 1; Conference of 2nd International Workshop on EntitY REtrieval, EYRE 2019 ; Conference Date: 3 November 2019; Conference Code:151820},
author = {Wei, D and Liu, Y and Zhu, F and Zang, L and Zhou, W and Han, J and Hu, S},
booktitle = {CEUR Workshop Proceedings},
editor = {{Cheng G. Gunaratna K.}, Wang J},
issn = {16130073},
keywords = {Attention mechanisms; Complex learning; Entity su,Benchmarking; Clustering algorithms; Knowledge man,Deep learning},
pages = {40--44},
publisher = {CEUR-WS},
title = {{ESA: Entity summarization with attention}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072722358&partnerID=40&md5=dada326d46e85cd2d495ad8ac0a7af00},
volume = {2446},
year = {2019}
}
@inproceedings{ISI:000462191600009,
abstract = {Estimating the semantic similarity between texts is of vital importance
for a wide range of application scenarios in natural language
processing. With the increasing availability of large text corpora,
data-driven approaches like Word2Vec became quite successful. In
contrast, semantic methods, which employ manually designed knowledge
bases like ontologies lost some of their former popularity. However,
manually designed knowledge can still be a valuable resource, since it
can be leveraged to boost the performance of data-driven approaches. We
introduce in this paper a novel hybrid similarity estimate based on
fuzzy sets that exploits both word embeddings and a lexical ontology. As
ontology we use Odenet, a freely available resource recently developed
by the Darmstadt University of Applied Sciences. Our application
scenario is targeted marketing, in which we aim to match people to the
best fitting marketing target group based on short German text snippets.
The evaluation showed that the use of an ontology did indeed improve the
overall result in comparison with a baseline data-driven estimate.},
address = {PO BOX 7827, WILMINGTON, DE 19803 USA},
annote = {12th International Conference on Advances in Semantic Processing
(SEMAPRO), Athens, GREECE, NOV 18-22, 2018},
author = {vor der Bruck, Tim},
booktitle = {SEMAPRO 2018: THE TWELFTH INTERNATIONAL CONFERENCE ON ADVANCES IN SEMANTIC PROCESSING},
editor = {{Spranger, M and Lorenz, P}},
isbn = {978-1-61208-678-1},
keywords = {Odenet; Fuzzy sets; Targeted marketing; Histogram},
organization = {IARIA},
pages = {48--52},
publisher = {IARIA XPS PRESS},
title = {{Estimating Semantic Similarity for Targeted Marketing based on Fuzzy Sets and the Odenet Ontology}},
type = {Proceedings Paper},
year = {2018}
}
@article{Dassereto202041,
abstract = {Nowadays word embeddings are used for many natural language processing (NLP) tasks thanks to their ability of capturing the semantic relations between words. Word embeddings have been mostly used to solve traditional NLP problems, such as question answering, textual entailment and sentiment analysis. This work proposes a new way of thinking about word embeddings that exploits them in order to represent geographical knowledge (e.g., geographical ontologies). We also propose metrics for evaluating the effectiveness of an embedding with respect to the ontological structure on which it is created both in an absolute way and with reference to its application within geolocation algorithms. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 22nd AGILE Conference on Geographic Information Science, 2019 ; Conference Date: 17 June 2019 Through 20 June 2019; Conference Code:225769},
author = {Dassereto, F and {Di Rocco}, L and Guerrini, G and Bertolotto, M},
doi = {10.1007/978-3-030-14745-7_3},
editor = {{Kyriakidis P. Skarlatos D.}, Hadjimitsis D Mansourian A},
isbn = {9783030147440},
issn = {18632246},
journal = {Lecture Notes in Geoinformation and Cartography},
keywords = {Embedding evaluation; Geographical ontology; Geol,Embeddings,Geographic information systems; Ontology; Regional},
pages = {41--57},
publisher = {Springer Berlin Heidelberg},
title = {{Evaluating the effectiveness of embeddings in representing the structure of geospatial ontologies}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065771962&doi=10.1007%2F978-3-030-14745-7_3&partnerID=40&md5=c9af92fb7f82d041a6912a7db4885da6},
year = {2020}
}
@inproceedings{Çelebi2018,
abstract = {Current approaches to identifying drug-drug interactions (DDIs), which involve clinical evaluation of drugs and post-marketing surveillance, are unable to provide complete, accurate information, nor do they alert the public to potentially dangerous DDIs before the drugs reach the market. Predicting potential drug-drug interaction helps reduce unanticipated drug interactions and drug development costs and optimizes the drug design process. Many bioinformatics databases have begun to present their data as Linked Open Data (LOD), a graph data model, using Semantic Web technologies. The knowledge graphs provide a powerful model for defining the data, in addition to making it possible to use underlying graph structure for extraction of meaningful information. In this work, we have applied Knowledge Graph (KG) Embedding approaches to extract feature vector representation of drugs using LOD to predict potential drug-drug interactions. We have investigated the effect of different embedding methods on the DDI prediction and showed that the knowledge embeddings are powerful predictors and comparable to current state-of-the-art methods for inferring new DDIs. We have applied Logistic Regression, Naive Bayes and Random Forest on Drugbank KG with the 10-fold traditional cross validation (CV) using RDF2Vec, TransE and TransD. RDF2Vec with uniform weighting surpass other embedding methods. {\textcopyright} 2018 CEUR Workshop Proceedings. All rights reserved.},
annote = {cited By 0; Conference of 11th International Conference Semantic Web Applications and Tools for Life Sciences, SWAT4LS 2018 ; Conference Date: 3 December 2018 Through 6 December 2018; Conference Code:143171},
author = {{\c{C}}elebi, R and Yasar, E and Uyar, H and Gumus, O and Dikenelli, O and Dumontier, M},
booktitle = {CEUR Workshop Proceedings},
editor = {{Splendiani A. Baker C.J.O.}, Beyan O D Marshall M S Waagmeester A},
issn = {16130073},
keywords = {Bioinformatics database; Drug development costs;,Commerce; Data mining; Decision trees; Forecasting,Drug interactions},
publisher = {CEUR-WS},
title = {{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using linked open data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058951382&partnerID=40&md5=0bf4001a4b1a55af9d2a45ec11842261},
volume = {2275},
year = {2018}
}
@inproceedings{Li2016821,
abstract = {Knowledge bases, which consist of a collection of entities, attributes, and the relations between them are widely used and important for many information retrieval tasks. Knowledge base schemas are often constructed manually using experts with specific domain knowledge for the field of interest. Once the knowledge base is generated then many tasks such as automatic content extraction and knowledge base population can be performed, which have so far been robustly studied by the Natural Language Processing community. However, the current approaches ignore visual information that could be used to build or populate these structured ontologies. Preliminary work on visual knowledge base construction only explores limited basic objects and scene relations. In this paper, we propose a novel multimodal pattern mining approach towards constructing a high-level "event" schema semi-automatically, which has the capability to extend text only methods for schema construction. We utilize a large unconstrained corpus of weakly-supervised image-caption pairs related to high-level events such as "attack" and "demonstration" to both discover visual aspects of an event, and name these visual components automatically. We compare our method with several state-of-the-art visual pattern mining approaches and demonstrate that our proposed method can achieve dramatic improvements in terms of the number of concepts discovered (33% gain), semantic consistence of visual patterns (52% gain), and correctness of pattern naming (150% gain). {\textcopyright} 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
annote = {cited By 10; Conference of 24th ACM Multimedia Conference, MM 2016 ; Conference Date: 15 October 2016 Through 19 October 2016; Conference Code:124107},
author = {Li, H and Ellis, J G and Ji, H and Chang, S.-F.},
booktitle = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
doi = {10.1145/2964284.2964287},
isbn = {9781450336031},
keywords = {Convolutional neural network; Event schema; Knowl,Data mining; Natural language processing systems;,Knowledge based systems},
pages = {821--830},
publisher = {Association for Computing Machinery, Inc},
title = {{Event specific multimodal pattern mining for knowledge base construction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994663352&doi=10.1145%2F2964284.2964287&partnerID=40&md5=7c6fe277718f0b85fee4515e202c84e7},
year = {2016}
}
@article{D’Amato2016113,
abstract = {In the Semantic Web, OWL ontologies play the key role of domain conceptualizations, while the corresponding assertional knowledge is given by the heterogeneousWeb resources referring to them. However, being strongly decoupled, ontologies and assertional knowledge can be out of sync. In particular, an ontology may be incomplete, noisy, and sometimes inconsistent with the actual usage of its conceptual vocabulary in the assertions. Despite of such problematic situations, we aim at discovering hidden knowledge patterns from ontological knowledge bases, in the form of multi-relational association rules, by exploiting the evidence coming from the (evolving) assertional data. The final goal is to make use of such patterns for (semi-)automatically enriching/completing existing ontologies. An evolutionary search method applied to populated ontological knowledge bases is proposed for the purpose. The method is able to mine intensional and assertional knowledge by exploiting problemaware genetic operators, echoing the refinement operators of inductive logic programming, and by taking intensional knowledge into account, which allows to restrict the search space and direct the evolutionary process. The discovered rules are represented in SWRL, so that they can be straightforwardly integrated within the ontology, thus enriching its expressive power and augmenting the assertional knowledge that can be derived from it. Discovered rules may also suggest new (schema) axioms to be added to the ontology. We performed experiments on publicly available ontologies, validating the performances of our approach and comparing them with the main state-of-the-art systems. {\textcopyright}Springer International Publishing AG 2016.},
annote = {cited By 8; Conference of 20th International Conference on Knowledge Engineering and Knowledge Management, EKAW 2016 ; Conference Date: 19 November 2016 Through 23 November 2016; Conference Code:186709},
author = {D'Amato, C and Tettamanzi, A G B and Minh, T D},
doi = {10.1007/978-3-319-49004-5_8},
editor = {{Ciancarini P. Poggi F.}, Vitali F Blomqvist E},
isbn = {9783319490038},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Association rules; Concentration (process); Data d,Description logic; Evolutionary process; Evolution,Ontology},
pages = {113--128},
publisher = {Springer Verlag},
title = {{Evolutionary discovery of multi-relational association rules from ontological knowledge bases}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997295011%7B%5C&%7Ddoi=10.1007%7B%5C%25%7D2F978-3-319-49004-5%7B%5C_%7D8%7B%5C&%7DpartnerID=40%7B%5C&%7Dmd5=76d9d43451eab2ee165cc3ec37dbfc41},
volume = {10024 LNAI},
year = {2016}
}
@article{Sousa2020,
abstract = {Background: In recent years, biomedical ontologies have become important for describing existing biological knowledge in the form of knowledge graphs. Data mining approaches that work with knowledge graphs have been proposed, but they are based on vector representations that do not capture the full underlying semantics. An alternative is to use machine learning approaches that explore semantic similarity. However, since ontologies can model multiple perspectives, semantic similarity computations for a given learning task need to be fine-tuned to account for this. Obtaining the best combination of semantic similarity aspects for each learning task is not trivial and typically depends on expert knowledge. Results: We have developed a novel approach, evoKGsim, that applies Genetic Programming over a set of semantic similarity features, each based on a semantic aspect of the data, to obtain the best combination for a given supervised learning task. The approach was evaluated on several benchmark datasets for protein-protein interaction prediction using the Gene Ontology as the knowledge graph to support semantic similarity, and it outperformed competing strategies, including manually selected combinations of semantic aspects emulating expert knowledge. evoKGsim was also able to learn species-agnostic models with different combinations of species for training and testing, effectively addressing the limitations of predicting protein-protein interactions for species with fewer known interactions. Conclusions: EvoKGsim can overcome one of the limitations in knowledge graph-based semantic similarity applications: The need to expertly select which aspects should be taken into account for a given application. Applying this methodology to protein-protein interaction prediction proved successful, paving the way to broader applications. {\textcopyright} 2019 The Author(s).},
annote = {cited By 2},
author = {Sousa, R T and Silva, S and Pesquita, C},
doi = {10.1186/s12859-019-3296-1},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Agnostic; article; gene ontology; human; machine,Algorithms; Biological Ontologies; Data Mining; G,Benchmark datasets; Biomedical domain; Biomedical,Data mining; Forecasting; Genetic algorithms; Gene,Learning systems},
number = {1},
publisher = {BioMed Central},
title = {{Evolving knowledge graph similarity for supervised learning in complex biomedical domains}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077480514&doi=10.1186%2Fs12859-019-3296-1&partnerID=40&md5=d5f438ef510c46cc920d03cad1be2f06},
volume = {21},
year = {2020}
}
@article{Gad-Elrab2016234,
abstract = {Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, YAGO and Wikidata. These KGs are inevitably bound to be incomplete. To fill in the gaps, data correlations in the KG can be analyzed to infer Horn rules and to predict new facts. However, Horn rules do not take into account possible exceptions, so that predicting facts via such rules introduces errors. To overcome this problem, we present a method for effective revision of learned Horn rules by adding exceptions (i.e., negated atoms) into their bodies. This way errors are largely reduced. We apply our method to discover rules with exceptions from real-world KGs. Our experimental results demonstrate the effectiveness of the developed method and the improvements in accuracy for KG completion by rule-based fact prediction. {\textcopyright} Springer International Publishing AG 2016.},
annote = {cited By 13; Conference of 15th International Semantic Web Conference, ISWC 2016 ; Conference Date: 17 October 2016 Through 21 October 2016; Conference Code:185129},
author = {Gad-Elrab, M H and Stepanova, D and Urbani, J and Weikum, G},
doi = {10.1007/978-3-319-46523-4_15},
editor = {{Krotzsch M. Lecue F.}, Lecue F Simperl E Gray A Flock F Gil Y Groth P Sabou M},
isbn = {9783319465227},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automatic construction; Data correlations; Dbpedi,Data mining; Forecasting,Semantic Web},
pages = {234--251},
publisher = {Springer Verlag},
title = {{Exception-enriched rule learning from knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992645513&doi=10.1007%2F978-3-319-46523-4_15&partnerID=40&md5=4a6c6c6743a5b3fc29cd467a782b9ed6},
volume = {9981 LNCS},
year = {2016}
}
@article{ISI:000566213200001,
abstract = {International Classification of Diseases (ICD) is an authoritative
health care classification system of different diseases. It is widely
used for disease and health records, assisted medical reimbursement
decisions, and collecting morbidity and mortality statistics. The most
existing ICD coding models only translate the simple diagnosis
descriptions into ICD codes. And it obscures the reasons and details
behind specific diagnoses. Besides, the label (code) distribution is
uneven. And there is a dependency between labels. Based on the above
considerations, the knowledge graph and attention mechanism were
expanded into medical code prediction to improve interpretability. In
this study, a new method called G_Coder was presented, which mainly
consists of Multi-CNN, graph presentation, attentional matching, and
adversarial learning. The medical knowledge graph was constructed by
extracting entities related to ICD-9 from freebase. Ontology contains 5
entity classes, which are disease, symptom, medicine, surgery, and
examination. The result of G_Coder on the MIMIC-III dataset showed that
the micro-F1 score is 69.2% surpassing the state of art. The following
conclusions can be obtained through the experiment: G_Coder integrates
information across medical records using Multi-CNN and embeds knowledge
into ICD codes. Adversarial learning is used to generate the adversarial
samples to reconcile the writing styles of doctor. With the knowledge
graph and attention mechanism, most relevant segments of medical codes
can be explained. This suggests that the knowledge graph significantly
improves the precision of code prediction and reduces the working
pressure of the human coders.},
address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
author = {Teng, Fei and Yang, Wei and Chen, Li and Huang, LuFei and Xu, Qiang},
doi = {10.3389/fbioe.2020.00867},
issn = {2296-4185},
journal = {FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY},
keywords = {automated ICD coding; knowledge graphs; explainabl},
month = {aug},
publisher = {FRONTIERS MEDIA SA},
title = {{Explainable Prediction of Medical Codes With Knowledge Graphs}},
type = {Article},
volume = {8},
year = {2020}
}
@article{Adhikari20163,
abstract = {This paper presents an approach to semi-automated data analysis, supported by tools for pattern construction, exploration and explanation. The proposed three-part methodology for multiresolution 0–1 data analysis consists of data clustering with mixture models, extraction of rules from clusters, as well as data and rule visualization using banded matrices. The results of the three-part process: clusters, rules from clusters, and banded structure of the data matrix are finally merged in a unified visual banded matrix display. The incorporation of multiresolution data is enabled by the supporting ontology, describing the relationships between the different resolutions, which is used as background knowledge in the semantic pattern mining process of descriptive rule induction. The presented experimental use case highlights the usefulness of the proposed methodology for analyzing complex DNA copy number amplification data, studied in previous research, for which we provide new insights in terms of induced semantic patterns and cluster/pattern visualization. The methodology is successfully evaluated on four other publicly available data sets, which further demonstrates the utility of the proposed approach. {\textcopyright} 2016, The Author(s).},
annote = {cited By 6},
author = {Adhikari, P R and Vavpeti{\v{c}}, A and Kralj, J and Lavra{\v{c}}, N and Hollm{\'{e}}n, J},
doi = {10.1007/s10994-016-5550-3},
issn = {08856125},
journal = {Machine Learning},
keywords = {Banded matrices; Clustering; Mixture model; Patte,Clustering algorithms; Data handling; Data mining;,Matrix algebra},
number = {1},
pages = {3--39},
publisher = {Springer New York LLC},
title = {{Explaining mixture models through semantic pattern mining and banded matrix visualization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973658787&doi=10.1007%2Fs10994-016-5550-3&partnerID=40&md5=66a810af45ef42186ead32ddc4b807a3},
volume = {105},
year = {2016}
}
@inproceedings{Xiong20171271,
abstract = {This paper introduces Explicit Semantic Ranking (ESR), a new ranking technique that leverages knowledge graph embedding. Analysis of the query log from our academic search engine, SemanticScholar.org, reveals that a major error source is its inability to understand the meaning of research concepts in queries. To addresses this challenge, ESR represents queries and documents in the entity space and ranks them based on their semantic connections from their knowledge graph embedding. Experiments demonstrate ESR's ability in improving Semantic Scholar's online production system, especially on hard queries where word-based ranking fails. {\textcopyright} 2017 International World Wide Web Conference Committee (IW3C2).},
annote = {cited By 74; Conference of 26th International World Wide Web Conference, WWW 2017 ; Conference Date: 3 April 2017 Through 7 April 2017; Conference Code:138082},
author = {Xiong, C and Power, R and Callan, J},
booktitle = {26th International World Wide Web Conference, WWW 2017},
doi = {10.1145/3038912.3052558},
isbn = {9781450349130},
keywords = {Academic search; Entity-based ranking; Error sour,Information retrieval,Search engines; Semantic Web; Semantics; World Wid},
pages = {1271--1279},
publisher = {International World Wide Web Conferences Steering Committee},
title = {{Explicit semantic ranking for academic search via knowledge graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040254209&doi=10.1145%2F3038912.3052558&partnerID=40&md5=90352fa222874be0c6b19d53edb995bb},
year = {2017}
}
@inproceedings{10.1145/3387904.3389281,
abstract = {Bug localization automatic localize relevant source files given a natural language description of bug within a software project. For a large project containing hundreds and thousands of source files, developers need cost lots of time to understand bug reports generated by quality assurance and localize these buggy source files. Traditional methods are heavily depending on the information retrieval technologies which rank the similarity between source files and bug reports in lexical level. Recently, deep learning based models are used to extract semantic information of code with significant improvements for bug localization. However, programming language is a highly structural and logical language, which contains various relations within and cross source files. Thus, we propose KGBugLocator to utilize knowledge graph embeddings to extract these interrelations of code, and a keywords supervised bi-directional attention mechanism regularize model with interactive information between source files and bug reports. With extensive experiments on four different projects, we prove our model can reach the new the-state-of-art(SOTA) for bug localization.},
address = {New York, NY, USA},
author = {Zhang, Jinglei and Xie, Rui and Ye, Wei and Zhang, Yuhan and Zhang, Shikun},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
doi = {10.1145/3387904.3389281},
isbn = {9781450379588},
keywords = {bug localization,code representation,deep learning,knowledge graph},
pages = {219--229},
publisher = {Association for Computing Machinery},
series = {ICPC '20},
title = {{Exploiting Code Knowledge Graph for Bug Localization via Bi-Directional Attention}},
url = {https://doi.org/10.1145/3387904.3389281},
year = {2020}
}
@article{Zhu20188,
abstract = {With the increasing popularity of large scale Knowledge Graph (KG)s, many applications such as semantic analysis, search and question answering need to link entity mentions in texts to entities in KGs. Because of the polysemy problem in natural language, entity disambiguation is thus a key problem in current research. Existing disambiguation methods have considered entity prominence, context similarity and entity-entity relatedness to discriminate ambiguous entities, which are mainly working on document or paragraph level texts containing rich contextual information, and based on lexical matching for computing context similarity. When meeting short texts containing limited contextual information, such as web queries, questions and tweets, those conventional disambiguation methods are not good at handling single entity mention and measuring context similarity. In order to enhance the performance of disambiguation methods based on context similarity with such short texts, we propose SCSNED method for disambiguation based on semantic similarity between contextual words and informative words of entities in KGs. Specially, we exploit the effectiveness of both knowledge-based and corpus-based semantic similarity methods for entity disambiguation with SCSNED. Moreover, we propose a Category2Vec embedding model based on joint learning of word and category embedding, in order to compute word-category similarity for entity disambiguation. We show the effectiveness of these proposed methods with illustrative examples, and evaluate their effectiveness in a comparative experiment for entity disambiguation in real world web queries, questions and tweets. The experimental results have identified the effectiveness of different semantic similarity methods, and demonstrated the improvement of semantic similarity methods in SCSNED and Category2Vec over the conventional context similarity baseline. We further compare the proposed approaches with the state of the art entity disambiguation systems and show the performances of the proposed approaches are among the best performing systems. In addition, one important feature of the proposed approaches using semantic similarity, is the potential application on any existing KGs since they mainly use common features of entity descriptions and categories. Another contribution of the paper is an updated survey on background of entity disambiguation in KGs and semantic similarity methods. {\textcopyright} 2018 The Authors},
annote = {cited By 30},
author = {Zhu, G and Iglesias, C A},
doi = {10.1016/j.eswa.2018.02.011},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Context similarity; Entity linking; Knowledge gra,Knowledge based systems; Semantics,Natural language processing systems},
pages = {8--24},
publisher = {Elsevier Ltd},
title = {{Exploiting semantic similarity for named entity disambiguation in knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042942927&doi=10.1016%2Fj.eswa.2018.02.011&partnerID=40&md5=941784ddc7691e6ab6e27050349988b6},
volume = {101},
year = {2018}
}
@article{Perianin2016137,
abstract = {Word representation learning methods such as word2vec usually associate one vector per word; however, in order to face polysemy problems, it's important to produce distributed representations for each meaning, not for each surface form of a word. In this paper, we propose an extension for the existing AutoExtend model, an auto-encoder architecture that utilises synonymy relations to learn sense representations. We introduce a new layer in the architecture to exploit hypernymy relations predominantly present in existing ontologies. We evaluate the quality of the obtained vectors on word-sense disambiguation tasks and show that the use of the hypernymy relation leads to improvements of 1.2% accuracy on Senseval-3 and 0.8% on Semeval-2007 English lexical sample tasks, compared to the original model. {\textcopyright} Springer International Publishing AG 2016.},
annote = {cited By 0; Conference of 18th International Conference on Asia-Pacific Digital Libraries, ICADL 2016 ; Conference Date: 7 December 2016 Through 9 December 2016; Conference Code:187139},
author = {Perianin, T and Senuma, H and Aizawa, A},
doi = {10.1007/978-3-319-49304-6_17},
editor = {{Morishima A. Rauber A.}, li Liew C},
isbn = {9783319493039},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Auto encoders; Hypernymy; Semantic relations; Sen,Digital libraries,Learning systems; Natural language processing syst},
pages = {137--143},
publisher = {Springer Verlag},
title = {{Exploiting synonymy and hypernymy to learn efficient meaning representations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005943288&doi=10.1007%2F978-3-319-49304-6_17&partnerID=40&md5=06ae5384ec75808dc2298d77b4fa4e12},
volume = {10075 LNCS},
year = {2016}
}
@article{Petrucci201866,
abstract = {Automated ontology learning from unstructured textual sources has been proposed in literature as a way to support the difficult and time-consuming task of knowledge modeling for semantic applications. In this paper we propose a system, based on a neural network in the encoder–decoder configuration, to translate natural language definitions into Description Logics formul{\ae} through syntactic transformation. The model has been evaluated to assess its capacity to generalize over different syntactic structures, tolerate unknown words, and improve its performance by enriching the training set with new annotated examples. The results obtained in our evaluation show how approaching the ontology learning problem as a neural machine translation task can be a valid way to tackle long term expressive ontology learning challenges such as language variability, domain independence, and high engineering costs. {\textcopyright} 2018 Elsevier B.V.},
annote = {cited By 6},
author = {Petrucci, G and Rospocher, M and Ghidini, C},
doi = {10.1016/j.websem.2018.10.002},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Computational linguistics; Computer aided language,Description logic; Domain independences; Expressi,Ontology},
pages = {66--82},
publisher = {Elsevier B.V.},
title = {{Expressive ontology learning as neural machine translation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056574730&doi=10.1016%2Fj.websem.2018.10.002&partnerID=40&md5=1583f592956b1e700e937223306c9b93},
volume = {52-53},
year = {2018}
}
@article{Intarapaiboon2011479,
abstract = {Based on sliding-window rule application and extraction filtering, we present a framework for extracting multi-slot frames describing chemical reactions from Thai free text with unknown target-phrase boundaries. A supervised rule learning algorithm is employed for automatic construction of pattern-based extraction rules from hand-tagged training phrases. A filtering method is devised for removal of incorrect extraction results based on features observed from text portions appearing between adjacent slot fillers in source documents. Extracted reaction frames are represented as concept expressions in description logics and are used as metadata for document indexing. A document knowledge base supporting semantics-based information retrieval is constructed by integrating document metadata with domain-specific ontologies. {\textcopyright} 2011 The Institute of Electronics, Information and Communication Engineers.},
annote = {cited By 3},
author = {Intarapaiboon, P and Nantajeewarawat, E and Theeramunkong, T},
doi = {10.1587/transinf.E94.D.479},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Automated reasoning; Automatic construction; Desc,Chemical reactions; Data description; Formal langu,Information retrieval},
number = {3},
pages = {479--486},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{Extracting chemical reactions from thai text for semantics-based information retrieval}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952166898&doi=10.1587%2Ftransinf.E94.D.479&partnerID=40&md5=455fab43c0b762d79638800f2eebe1ae},
volume = {E94-D},
year = {2011}
}
@inproceedings{Zeng-Treitler201820,
abstract = {Frailty is increasingly recognized as a leading indicator of worsening health outcomes, even death. Frailty is also used implicitly by many clinicians for decision-making, but rarely in a systematic way. We developed a novel ontology-based natural language processing approach to extract frailty status from clinical notes and support retrospective clinical studies and prospective clinical decision making. The natural language processing classification of frailty description achieved an accuracy of 80.3%. We used the extracted frailty information along with other clinical variables to predict mortality after major cardiovascular procedures. The area under the curve for mortality prediction was 78.3% on the test data for a deep neural network model, compared to 74.9% using a support vector machine. {\textcopyright} 2018 MCCSIS 2018 - Multi Conference on Computer Science and Information Systems; Proceedings of the International Conferences on e-Health 2018, ICT, Society, and Human Beings 2018 and Web Based Communities and Social Media 2018. All rights reserved.},
annote = {cited By 0; Conference of 10th International Conference on e-Health 2018, the 11th International Conference on ICT, Society, and Human Beings 2018 and of the 15th International Conference Web Based Communities and Social Media 2018, part of the Multi Conference on Computer Science and Information Systems 2018, MCCSIS 2018 ; Conference Date: 17 July 2018 Through 19 July 2018; Conference Code:139335},
author = {Zeng-Treitler, Q and Shao, Y and Cheng, Y and Doing-Harris, K and Shah, R U and Weir, C R and Bray, B E},
booktitle = {MCCSIS 2018 - Multi Conference on Computer Science and Information Systems; Proceedings of the International Conferences on e-Health 2018, ICT, Society, and Human Beings 2018 and Web Based Communities and Social Media 2018},
editor = {{Kommers P. Rodrigues L.}, Macedo M},
isbn = {9789898533777},
keywords = {Area under the curves; Clinical decision making;,Decision making; Deep neural networks; Forecasting,Natural language processing systems},
pages = {20--28},
publisher = {IADIS},
title = {{Extracting frailty status for post surgical mortality prediction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054165982&partnerID=40&md5=cd7969f8ff2ebfcfabff25aca1bd5dcd},
year = {2018}
}
@article{Liu2014,
abstract = {Background: Time delays are important factors that are often neglected in gene regulatory network (GRN) inferencemodels. Validating time delays from knowledge bases is a challenge since the vast majority of biological databasesdo not record temporal information of gene regulations. Biological knowledge and facts on gene regulations aretypically extracted from bio-literature with specialized methods that depend on the regulation task. In this paper, wemine evidences for time delays related to the transcriptional regulation of yeast from the PubMed abstracts.Results: Since the vast majority of abstracts lack quantitative time information, we can only collect qualitativeevidences of time delays. Specifically, the speed-up or delay in transcriptional regulation rate can provide evidencesfor time delays (shorter or longer) in GRN. Thus, we focus on deriving events related to rate changes intranscriptional regulation. A corpus of yeast regulation related abstracts was manually labeled with such events. Inorder to capture these events automatically, we create an ontology of sub-processes that are likely to result intranscription rate changes by combining textual patterns and biological knowledge. We also propose effectivefeature extraction methods based on the created ontology to identify the direct evidences with specific details ofthese events. Our ontologies outperform existing state-of-the-art gene regulation ontologies in the automatic rulelearning method applied to our corpus. The proposed deterministic ontology rule-based method can achievecomparable performance to the automatic rule learning method based on decision trees. This demonstrates theeffectiveness of our ontology in identifying rate-changing events. We also tested the effectiveness of the proposedfeature mining methods on detecting direct evidence of events. Experimental results show that the machinelearning method on these features achieves an F1-score of 71.43%.Conclusions: The manually labeled corpus of events relating to rate changes in transcriptional regulation for yeastis available in https://sites.google.com/site/wentingntu/data. The created ontologies summarized both biologicalcauses of rate changes in transcriptional regulation and corresponding positive and negative textual patterns fromthe corpus. They are demonstrated to be effective in identifying rate-changing events, which shows the benefits ofcombining textual patterns and biological knowledge on extracting complex biological events. {\textcopyright} 2014 Liu et al.},
annote = {cited By 2},
author = {Liu, W and Miao, K and Li, G and Chang, K and Zheng, J and Rajapakse, J C},
doi = {10.1186/1471-2105-15-S2-S4},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Abstracting; Complex networks; Decision trees; Gen,Artificial Intelligence; Biological Ontologies; D,Extraction method; Gene regulatory networks; Rule,Genetic,Time delay,decision tree; DNA transcription; experimental mo},
publisher = {BioMed Central Ltd.},
title = {{Extracting rate changes in transcriptional regulation from MEDLINE abstracts}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901289631&doi=10.1186%2F1471-2105-15-S2-S4&partnerID=40&md5=c6380e801d9295e5cf8b3f479c4f316a},
volume = {15},
year = {2014}
}
@inproceedings{8550982,
abstract = {Gene Ontology (GO) is said to be the most popular bio-ontology that describes the gene products and its characteristics. For describing the gene product and its characteristics the three terms are used, namely, biological process, molecular function and cellular component. GO annotation is the term that is derived from kind of sub-ontologies at various stages. It is a vital source that describes the relationship between the three sub-ontologies. For effective information finding the data mining approach names as association rule mining, which is modified for mining the relationships from various ontologies from GO annotation data. For mining the relationship the abstractions are mandatory. In the existing system, GO-WAR (GeneeOntology-based Weighted Association Rules) methodology was proposed by using a FP - growth algorithm for extracting weighted association rules. GO-WAR is could extract association rules with a high information(IC)without loss of support and confidence from a dataset of annotated data. However, the performance was low for extracting weighted association rules. This paper introduces the new methodology GOPAR (Gene Ontology based Predictive Association Rule) to eliminate the drawback of GO-WAR. GOPAR avoids the repeated association rule and generating the predictive grouped rules, the grouping of association rules strictly eliminates multiple GO terms. Further, the paper presents the GOPOAR (Gene Ontology based Predictive Optimized Association Rule) approach is to achieve a best optimal solution and find the missing values of predictive association rule. Based on the experimental result the GOPOAR extracted more number of significant rules also providing truthful and better optimal results.},
author = {Dhanalakshmi, P and Porkodi, R},
booktitle = {2018 International Conference on Current Trends towards Converging Technologies (ICCTCT)},
doi = {10.1109/ICCTCT.2018.8550982},
keywords = {biology computing;data mining;genetics;lab-on-a-ch},
month = {mar},
pages = {1--9},
title = {{Extraction of Association Rules from Tobacco Smoke Effect on the Placenta Microarray Dataset using Gene Ontology Based Optmized Association Rule Mining}},
year = {2018}
}

@article{CTRU92VT,
abstract = {Background: An adverse drug event (ADE) is commonly defined as ` an
injury resulting from medical intervention related to a drug.{''}
Providing information related to ADEs and alerting caregivers at the
point of care can reduce the risk of prescription and diagnostic errors
and improve health outcomes. ADEs captured in structured data in
electronic health records (EHRs) as either coded problems or allergies
are often incomplete, leading to underreporting. Therefore, it is
important to develop capabilities to process unstructured EHR data in
the form of clinical notes, which contain a richer documentation of a
patient's ADE. Several natural language processing (NLP) systems have
been proposed to automatically extract information related to ADEs.
However, the results from these systems showed that significant
improvement is still required for the automatic extraction of ADEs from
clinical notes.
Objective: This study aims to improve the automatic extraction of ADEs
and related information such as drugs, their attributes, and reason for
administration from the clinical notes of patients.
Methods: This research was conducted using discharge summaries from the
Medical Information Mart for Intensive Care III (MIMIC-III) database
obtained through the 2018 National NLP Clinical Challenges (n2c2)
annotated with drugs, drug attributes (ie, strength, form, frequency,
route, dosage, duration), ADEs, reasons, and relations between drugs and
other entities. We developed a deep learning-based system for extracting
these drug-centric concepts and relations simultaneously using a joint
method enhanced with contextualized embeddings, a position-attention
mechanism, and knowledge representations. The joint method generated
different sentence representations for each drug, which were then used
to extract related concepts and relations simultaneously. Contextualized
representations trained on the MIMIC-III database were used to capture
context-sensitive meanings of words. The position-attention mechanism
amplified the benefits of the joint method by generating sentence
representations that capture long-distance relations. Knowledge
representations were obtained from graph embeddings created using the US
Food and Drug Administration Adverse Event Reporting System database to
improve relation extraction, especially when contextual clues were
insufficient.
Results: Our system achieved new state-of-the-art results on the n2c2
data set, with significant improvements in recognizing crucial
drug-reason (F1=0.650 versus F1=0.579) and drug-ADE (F1=0.490 versus
F1=0.476) relations.
Conclusions: This study presents a system for extracting drug-centric
concepts and relations that outperformed current state-of-the-art
results and shows that contextualized embeddings, position-attention
mechanisms, and knowledge graph embeddings effectively improve deep
learning-based concepts and relation extraction. This study demonstrates
the potential for deep learning-based methods to help extract real-world
evidence from unstructured patient data for drug safety surveillance.},
address = {130 QUEENS QUAY E, STE 1102, TORONTO, ON M5A 0P6, CANADA},
author = {B. Dandala and V. Joopudi and C-H. Tsou and J. Liang and P. Suryanarayanan},
journal = {JMIR medical informatics},
keywords = {electronic health records; adverse drug events; na},
number = {7},
publisher = {JMIR PUBLICATIONS, INC},
title = {{Extraction of Information Related to Drug Safety Surveillance From Electronic Health Record Notes: Joint Modeling of Entities and Relations Using Knowledge-Aware Neural Attentive Models}},
type = {Article},
volume = {8},
year = {2020}
}
@inproceedings{Oliveira202072,
abstract = {We describe how a natural language interface can be developed for a wordnet with a small set of handcrafted templates, leveraging on sentence embeddings. The proposed approach does not use rules for parsing natural language queries but experiments showed that the embeddings model is tolerant enough for correctly predicting relation types that do not match known patterns exactly. It was tested with OpenWordNet-PT, for which this method may provide an alternative interface, with benefits also on the curation process. {\textcopyright} Copyright by Oficyna Wydawnicza Politechniki Wroc{\l}awskiej, Wroc{\l}aw 2019.},
annote = {cited By 0; Conference of 10th Global WordNet Conference, GWC 2019 ; Conference Date: 23 July 2019 Through 27 July 2019; Conference Code:158320},
author = {Oliveira, H G and Rademaker, A},
booktitle = {Proceedings of the 10th Global WordNet Conference},
editor = {{Fellbaum C. Vossen P.}, Rudnicka E Maziarz M Piasecki M},
isbn = {9788374931083},
keywords = {Curation; Natural language interfaces; Natural la,Embeddings; Ontology; Syntactics,Natural language processing systems},
pages = {72--78},
publisher = {Oficyna Wydawnicza Politechniki Wroclawskiej},
title = {{Fast developing of a natural language interface for a Portuguese Wordnet: Leveraging on sentence embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082460144&partnerID=40&md5=af1807dd71accc8c0ccf04b77d111017},
year = {2020}
}
@article{Galárraga2015707,
abstract = {Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive logic programming (ILP) can be used to mine logical rules from these KBs, such as “If two persons are married, then they (usually) live in the same city.” While ILP is a mature field, mining logical rules from KBs is difficult, because KBs make an open-world assumption. This means that absent information cannot be taken as counterexamples. Our approach AMIE (Gal{\'{a}}rraga et al. in WWW, 2013) has shown how rules can be mined effectively from KBs even in the absence of counterexamples. In this paper, we show how this approach can be optimized to mine even larger KBs with more than 12M statements. Extensive experiments show how our new approach, AMIE+, extends to areas of mining that were previously beyond reach. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
annote = {cited By 138},
author = {Gal{\'{a}}rraga, L and Teflioudi, C and Hose, K and Suchanek, F M},
doi = {10.1007/s00778-015-0394-1},
issn = {10668888},
journal = {VLDB Journal},
keywords = {Data mining,ILP; Knowledge basis; Knowledge basis (KBs); Mach,Inductive logic programming (ILP)},
number = {6},
pages = {707--730},
publisher = {Springer New York LLC},
title = {{Fast rule mining in ontological knowledge bases with AMIE+}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947127173&doi=10.1007%2Fs00778-015-0394-1&partnerID=40&md5=94e250e1a944ec55226182181c14b9ca},
volume = {24},
year = {2015}
}
@inproceedings{Chen2019,
abstract = {Computational approach to predict effective drug combination can significantly improve drug efficacy while reducing drug toxicity. In this work, we employed a deep feature compression approach on gene expression data, pathway information and Ontology Fingerprints to improve the performance of a deep learning framework for effective drug combination. Our method indicates that the deep feature compression approach is an effective way to improve the performance of drug combination prediction. Copyright {\textcopyright} 2019 for this paper by its authors.},
annote = {cited By 0; Conference of 4th International Workshop on Semantics-Powered Data Mining and Analytics, SEPDA 2019 ; Conference Date: 27 October 2019; Conference Code:151003},
author = {Chen, G and Jiang, X and {Jim Zheng}, W},
booktitle = {CEUR Workshop Proceedings},
editor = {{Tao C. Bian J.}, Zhang R He Z},
issn = {16130073},
keywords = {Computational approach; Drug combinations; Drug e,Data mining,Deep learning; Forecasting; Gene expression; Ontol},
publisher = {CEUR-WS},
title = {{Feature compression for predicting effective drug combination}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071780138&partnerID=40&md5=5e86c951c9362e1b2192843796cd1bb7},
volume = {2427},
year = {2019}
}
@inproceedings{9202824,
abstract = {Nowadays, explosive growth of ontologies are used for managing data in various domains. They usually own different vocabularies and structures following different fashions. Ontology alignment finding semantic correspondences between elements of these ontologies can effectively facilitate the data communication and novel application creation in many practical scenarios. However, we noticed that, the traditional parametric ontology mapping methods still depend on individualistic abilities for setting proper parameters for mapping. When trying to utilize artificial neural networks for the automatic ontology mapping, the training data are found insufficient in most of the cases. This paper analyzes these problems, and proposes a few-shot ontology alignment model, which can automatically learn how to map two ontologies from only a few training links between their element pairs. The proposed model applies the Siamese neural network in computer vision on ontology alignment and designs an attention detection network learning the attention weights for different ontology attributes. A few experiments that conducted on the anatomy ontology alignment show that our model achieves good performance (94.3% of F-measure) with 200 training alignments without traditional parametric setting.},
author = {Sun, J and Takeuchi, S and Yamasaki, I},
booktitle = {2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)},
doi = {10.1109/COMPSAC48688.2020.00-90},
issn = {0730-3157},
keywords = {Machine learning,Ontology Matching,Siamese neural network,data handling;learning (artificial intelligence);o},
month = {jul},
pages = {1218--1222},
title = {{Few-Shot Ontology Alignment Model with Attribute Attentions}},
year = {2020}
}
@inproceedings{Ramezani2013,
abstract = {Linked Data is used in the Web to create typed links between data from different sources. Connecting diffused data by using these links provides new data which could be employed in different applications. Association Rules Mining (ARM) is a data mining technique which aims to find interesting patterns and rules from a large set of data. In this paper, the problem of applying association rules mining using Linked Data in centralization approach has been addressed i.e. arranging collected data from different data sources into a single dataset and then apply ARM on the generated dataset. Firstly, a number of challenges in collecting data from Linked Data have been presented, followed by applying the ARM using the dataset of connected data sources. Preliminary experiments have been performed on this semantic data showing promising results and proving the efficiency, robust, and useful of the used approach. {\textcopyright} 2013 IEEE.},
address = {Mashhad},
annote = {cited By 1; Conference of 2013 21st Iranian Conference on Electrical Engineering, ICEE 2013 ; Conference Date: 14 May 2013 Through 16 May 2013; Conference Code:100029},
author = {Ramezani, R and Saraee, M and Nematbakhsh, M A},
booktitle = {2013 21st Iranian Conference on Electrical Engineering, ICEE 2013},
doi = {10.1109/IranianCEE.2013.6599550},
isbn = {9781467356343},
keywords = {Association rules mining; Data-sources; Frequent i,Data handling; Electrical engineering,Data mining},
title = {{Finding association rules in linked data, a centralization approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886908517&doi=10.1109%2FIranianCEE.2013.6599550&partnerID=40&md5=8dc4dd395734c95364c6230424b3d233},
year = {2013}
}
@inproceedings{9010987,
abstract = {Text present in images are not merely strings, they provide useful cues about the image. Despite their utility in better image understanding, scene texts are not used in traditional visual question answering (VQA) models. In this work, we present a VQA model which can read scene texts and perform reasoning on a knowledge graph to arrive at an accurate answer. Our proposed model has three mutually interacting modules: i. proposal module to get word and visual content proposals from the image, ii. fusion module to fuse these proposals, question and knowledge base to mine relevant facts, and represent these facts as multi-relational graph, iii. reasoning module to perform a novel gated graph neural network based reasoning on this graph. The performance of our knowledge-enabled VQA model is evaluated on our newly introduced dataset, viz. text-KVQA. To the best of our knowledge, this is the first dataset which identifies the need for bridging text recognition with knowledge graph based reasoning. Through extensive experiments, we show that our proposed method outperforms traditional VQA as well as question-answering over knowledge base-based methods on text-KVQA.},
author = {Singh, A K and Mishra, A and Shekhar, S and Chakraborty, A},
booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2019.00470},
issn = {2380-7504},
keywords = {document image processing;graph theory;inference m},
month = {oct},
pages = {4601--4611},
title = {{From Strings to Things: Knowledge-Enabled VQA Model That Can Read and Reason}},
year = {2019}
}
@inproceedings{7929965,
abstract = {Embedded markup based on Microdata, RDFa, and Microformats have become prevalent on the Web and constitute an unprecedented source of data. However, RDF statements extracted from markup are fundamentally different from traditional RDF graphs: entity descriptions are flat, facts are highly redundant, and despite very frequent co-references explicit links are missing. Therefore, carrying out typical entity-centric tasks such as retrieval and summarisation cannot be tackled sufficiently with state-of-the-art methods and require preliminary data fusion. Given the scale and dynamics of Web markup, the applicability of general data fusion approaches is limited. We present a novel query-centric data fusion approach which overcomes such issues through a combination of entity retrieval and fusion techniques geared towards the specific challenges associated with embedded markup. To ensure precise and diverse entity descriptions, we follow a supervised learning approach and train a classifier for data fusion of a pool of candidate facts relevant to a given query and obtained through a preliminary entity retrieval step. We perform a thorough evaluation on a subset of the Web Data Commons dataset and show significant improvement over existing baselines. In addition, an investigation into the coverage and complementarity of facts from the constructed entity descriptions compared to DBpedia, shows potential for aiding tasks such as knowledge base population.},
author = {Yu, R and Gadiraju, U and Fetahu, B and Dietze, S},
booktitle = {2017 IEEE 33rd International Conference on Data Engineering (ICDE)},
doi = {10.1109/ICDE.2017.69},
issn = {2375-026X},
keywords = {data mining;Internet;query processing;FuseM;query-},
month = {apr},
pages = {179--182},
title = {{FuseM: Query-Centric Data Fusion on Structured Web Markup}},
year = {2017}
}
@article{Ali2019,
abstract = {Intelligent Transportation Systems (ITSs) utilize a sensor network-based system to gather and interpret traffic information. In addition, mobility users utilize mobile applications to collect transport information for safe traveling. However, these types of information are not sufficient to examine all aspects of the transportation networks. Therefore, both ITSs and mobility users need a smart approach and social media data, which can help ITSs examine transport services, support traffic and control management, and help mobility users travel safely. People utilize social networks to share their thoughts and opinions regarding transportation, which are useful for ITSs and travelers. However, user-generated text on social media is short in length, unstructured, and covers a broad range of dynamic topics. The application of recent Machine Learning (ML) approach is inefficient for extracting relevant features from unstructured data, detecting word polarity of features, and classifying the sentiment of features correctly. In addition, ML classifiers consistently miss the semantic feature of the word meaning. A novel fuzzy ontology-based semantic knowledge with Word2vec model is proposed to improve the task of transportation features extraction and text classification using the Bi-directional Long Short-Term Memory (Bi-LSTM) approach. The proposed fuzzy ontology describes semantic knowledge about entities and features and their relation in the transportation domain. Fuzzy ontology and smart methodology are developed in Web Ontology Language and Java, respectively. By utilizing word embedding with fuzzy ontology as a representation of text, Bi-LSTM shows satisfactory improvement in both the extraction of features and the classification of the unstructured text of social media. {\textcopyright} 2019 by the authors. Licensee MDPI, Basel, Switzerland.},
annote = {cited By 6},
author = {Ali, F and El-Sappagh, S and Kwak, D},
doi = {10.3390/s19020234},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Classification (of information),Data mining; Extraction; Feature extraction; Intel,Features extraction; Intelligent transportation s},
number = {2},
publisher = {MDPI AG},
title = {{Fuzzy ontology and LSTM-based text mining: A transportation network monitoring system for assisting travel}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059891856&doi=10.3390%2Fs19020234&partnerID=40&md5=7aeadaca4420fe6f391fd13d7de72b0d},
volume = {19},
year = {2019}
}
@article{BenM’barek2020133,
abstract = {Community detection has become an important research direction for data mining in complex networks. It aims to identify topological structures and discover patterns in complex networks, which presents an important problem of great significance. In this paper, we are interested in the detection of communities in the Protein-Protein or Gene-gene Interaction (PPI) networks. These networks represent a set of proteins or genes that collaborate at the same cellular function. The goal is to identify such semantic and topological communities from gene annotation sources such as Gene Ontology. We propose a Genetic Algorithm (GA) based approach to detect communities having different sizes from PPI networks. For this purpose, we introduce three specific components to the GA: a fitness function based on a similarity measure and the interaction value between proteins or genes, a solution for representing a community with dynamic size and a specific mutation operator. In the computational tests carried out in this work, the introduced algorithm achieved excellent results to detect existing or even new communities from PPI networks. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 14th International Conference on Software Technologies, ICSOFT 2019 ; Conference Date: 26 July 2019 Through 28 July 2019; Conference Code:242759},
author = {{Ben M'barek}, M and Borgi, A and {Ben Hmida}, S and Rukoz, M},
doi = {10.1007/978-3-030-52991-8_7},
editor = {{van Sinderen M. Maciaszek L.A.}, Maciaszek L A},
isbn = {9783030529901},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Community detection; Computational tests; Gene-ge,Complex networks,Data mining; Genes; Genetic algorithms; Population},
pages = {133--155},
publisher = {Springer},
title = {{Ga-ppi-net: A genetic algorithm for community detection in protein-protein interaction networks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089235631&doi=10.1007%2F978-3-030-52991-8_7&partnerID=40&md5=c03d8f132d59656be619f269fc75ff24},
volume = {1250 CCIS},
year = {2020}
}
@article{Shen201852095,
abstract = {Ontology plays an increasingly important role in knowledge management and the semantic Web. However, ontology cannot perform well in realistic diagnosis reasoning unless it contains timely and accurate medical information and its individual items display all attributes of the categories they belong to. In this paper, we present a method that extracts synonyms along with concepts and their relationships for gastroenterology ontology construction. Specifically, we reuse the existing ontology as the basis for ontology completion. In addition, we conduct synonym identification through a combined application of global context features, local context features, and medical-specific features, and incorporate dependency information into deep neural networks for relation extraction. The extracted information is merged for ontology completion. Experimental results demonstrate that the proposed synonym identification and relation extraction method achieves the best performance compared with state-of-the-art methods and also builds a more complete ontology compared with existing gastroenterology disease ontologies. Our results are reproducible, and we will release the source code and ontology of this work after publication: https://github.com/shenyingpku/gastrointestinal-owl. {\textcopyright} 2013 IEEE.},
annote = {cited By 2},
author = {Shen, Y and Li, Y and Deng, Y and Zhang, J and Yang, M and Chen, J and Si, S and Lei, K},
doi = {10.1109/ACCESS.2018.2862885},
issn = {21693536},
journal = {IEEE Access},
keywords = {Data acquisition; Deep neural networks; Diagnosis;,Data mining,Dependency informations; Diagnosis reasoning; Med},
pages = {52095--52104},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Gastroenterology Ontology Construction Using Synonym Identification and Relation Extraction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051039880&doi=10.1109%2FACCESS.2018.2862885&partnerID=40&md5=3b55396920b13a189217dc11880ba181},
volume = {6},
year = {2018}
}
@inproceedings{DoAmaral2010,
abstract = {Computational approaches have been applied in many different biology application domains. When such tools are based on conventional computation, they have shown limitations to approach complex biological problems. In the present study, a computational evolutionary environment (CEE) is proposed as tool to extract classification rules from biological datasets. The main goal of the proposed approach is to allow the discovery of concise, yet accurate, high-level rules (from a biological database) which can be used as a classification system. More than focusing only on the classification accuracy, the proposed CEE model aims at balancing prediction precision, interpretability and comprehensibility. The obtained results show that the proposed CEE is promising and capable of extracting useful high-level knowledge that could not be extracted by traditional classifications methods such as Decision Trees, One R and the Single Conjunctive Rule Learner using the same dataset. {\textcopyright} 2010 IEEE.},
address = {Barcelona},
annote = {cited By 4; Conference of 2010 6th IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010 ; Conference Date: 18 July 2010 Through 23 July 2010; Conference Code:85187},
author = {{Do Amaral}, L R and {Hruschka Jr.}, E R},
booktitle = {2010 IEEE World Congress on Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation, CEC 2010},
doi = {10.1109/CEC.2010.5586011},
isbn = {9781424469109},
keywords = {Application domains; Biological database; Biologic,Artificial intelligence; Classification (of infor,Data mining},
title = {{Gene ontology classification: Building high-level knowledge using genetic algorithms}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959416891&doi=10.1109%2FCEC.2010.5586011&partnerID=40&md5=935c24a27c1222b39fbca096d7c39640},
year = {2010}
}
@article{Qiu2019157,
abstract = {A large amount of unstructured textual data about geoscience structures and minerals is buried in geoscience documents and is unused. Automatic keyphrase extraction provides opportunities to leverage this wealth of data for analysis and knowledge discovery. However, keyphrase extraction remains a complicated task, and the performance of state-of-the-art approaches is still low. Automatic discovery of high-quality and meaningful keyphrases requires the application of useful knowledge and suitable techniques. Seeing both challenges and opportunities in the situation described above, this paper proposes an ontology and enhanced word embedding-based (OEWE) methodology for automatic keyphrase extraction from geoscience documents. We first develop a quantitative analysis for keyphrase extraction evaluation based on conditional probability and the naive Bayesian model, which is valuable when human-annotated keyphrases are not available. The domain ontology is then performed on a multiway tree to enrich the domain-specific knowledge on certain concepts and relationships in a domain. Simultaneously, word2vec, a model of a word distribution using deep learning, is updated by applying the geological ontology, and it links domain background information and identifies infrequent but representative keyphrases. We use two homemade geoscience datasets to evaluate the performance of OEWE. We compare our method with frequency, term frequency-inverse document frequency (TF-IDF), TextRank and rapid automatic keyword extraction (RAKE), finding that our method achieves average F1 scores of 30.1% and 40.7% on two manually annotated datasets. {\textcopyright} 2019 Elsevier Ltd},
annote = {cited By 6},
author = {Qiu, Q and Xie, Z and Wu, L and Li, W},
doi = {10.1016/j.eswa.2019.02.001},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Background information; Conditional probabilities,Bayesian networks; Deep learning; Embeddings; Extr,Geology},
pages = {157--169},
publisher = {Elsevier Ltd},
title = {{Geoscience keyphrase extraction algorithm using enhanced word embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061125181&doi=10.1016%2Fj.eswa.2019.02.001&partnerID=40&md5=db00180edeedb563a31061f7df4514dc},
volume = {125},
year = {2019}
}
@article{Cochez2017190,
abstract = {Vector space embeddings have been shown to perform well when using RDF data in data mining and machine learning tasks. Existing approaches, such as RDF2Vec, use local information, i.e., they rely on local sequences generated for nodes in the RDF graph. For word embeddings, global techniques, such as GloVe, have been proposed as an alternative. In this paper, we show how the idea of global embeddings can be transferred to RDF embeddings, and show that the results are competitive with traditional local techniques like RDF2Vec. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 27; Conference of 16th International Semantic Web Conference, ISWC 2017 ; Conference Date: 21 October 2017 Through 25 October 2017; Conference Code:200299},
author = {Cochez, M and Ristoski, P and Ponzetto, S P and Paulheim, H},
doi = {10.1007/978-3-319-68288-4_12},
editor = {{Cudre-Mauroux P. Lange C.}, d'Amato C Fernandez M Heflin J Lecue F Tamma V Sequeda J},
isbn = {9783319682877},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining; Learning systems; Vector spaces,Embeddings; Graph embeddings; Linked open datum;,Semantic Web},
pages = {190--207},
publisher = {Springer Verlag},
title = {{Global RDF vector space embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032173118&doi=10.1007%2F978-3-319-68288-4_12&partnerID=40&md5=f705ffba1a8d4e5181478b8ac1a99c7d},
volume = {10587 LNCS},
year = {2017}
}
@inproceedings{10.1145/3097983.3098126,
abstract = {Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: - Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results.Interpretation: The representations learned by deep learning methods should align with medical knowledge.To address these challenges, we propose GRaph-based Attention Model (GRAM) that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism.We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.},
address = {New York, NY, USA},
author = {Choi, Edward and Bahadori, Mohammad Taha and Song, Le and Stewart, Walter F and Sun, Jimeng},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3097983.3098126},
isbn = {9781450348874},
keywords = {attention model,electronic health records,graph,predictive healthcare},
pages = {787--795},
publisher = {Association for Computing Machinery},
series = {KDD '17},
title = {{GRAM: Graph-Based Attention Model for Healthcare Representation Learning}},
url = {https://doi.org/10.1145/3097983.3098126},
year = {2017}
}
@inproceedings{8588768,
abstract = {Graph-structured queries provide an efficient way to retrieve the desired data from large-scale knowledge graphs. However, it is difficult for non-expert users to write such queries, and users prefer expressing their query intention through natural language questions. Therefore, automatically constructing graph-structured queries of given natural language questions has received wide attention in recent years. Most existing methods rely on natural language processing techniques to perform the query construction process, which is complicated and time-consuming. In this paper, we focus on the query construction process and propose a novel framework which stands on recent advances in knowledge graph embedding techniques. Our framework first encodes the underlying knowledge graph into a low-dimensional embedding space by leveraging the generalized local knowledge graphs. Then, given a natural language question, our framework computes the structure of the target query and determines the vertices/edges which form the target query based on the learned embedding vectors. Finally, the target graph-structured query is constructed according to the query structure and determined vertices/edges. Extensive experiments were conducted on the benchmark dataset. The results demonstrate that our framework outperforms several state-of-the-art baseline models regarding effectiveness and efficiency.},
author = {Wang, R and Wang, M and Liu, J and Yao, S and Zheng, Q},
booktitle = {2018 IEEE International Conference on Big Knowledge (ICBK)},
doi = {10.1109/ICBK.2018.00009},
keywords = {graph theory;knowledge based systems;natural langu},
month = {nov},
pages = {1--8},
title = {{Graph Embedding Based Query Construction Over Knowledge Graphs}},
year = {2018}
}
@article{8886405,
abstract = {Knowledge Graph (KG) usually contains billions of facts about the real world, where a fact is represented as a triplet in the form of (head entity, relation, tail entity). KG is a complex network and consists of numerous nodes (entities) and edges (relations). Given that most KGs are noisy and far from being complete, KG analysis and completion methods are becoming more and more important. Knowledge graph embedding (KGE) aims to embed entities and relations in a low dimensional and continuous vector space, which is proven to be a quite efficient and effective method in knowledge graph completion tasks. KGE models devise various kinds of score functions to evaluate each fact in KG, which assign high points for true facts and low points for invalid ones. In a KG of the real world, some nodes may have hundreds of links with other nodes. There is a wealth of information around an entity, and the surrounding information (i.e., the sub-graph structure information) of one entity can make a significant contribution to predicting new facts. However, many previous works including, translational approaches such as Trans(E, H, R, and D), factorization approaches such as DistMult, ComplEx, and other deep learning approaches such as NTN, ConvE, concentrate on rating each fact in an isolated and separated way and lack a specially designed mechanism to learn the sub-graph structure information of the entity in KG. To conquer this challenge, we leverage the information fusion mechanism (Graph2Seq) used in graph neural network which is specially designed for graph-structured data, to learn fusion embeddings for entities in KG. And a novel fusion embedding learning KGE model (referred as G2SKGE) which aims to learn the sub-graph structure information of the entity in KG is proposed. With empirical experiments on four benchmark datasets, our proposed model achieves promising results and outperforms the state-of-the-art models.},
author = {Li, W and Zhang, X and Wang, Y and Yan, Z and Peng, R},
doi = {10.1109/ACCESS.2019.2950230},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {graph theory;learning (artificial intelligence);na},
pages = {157960--157971},
title = {{Graph2Seq: Fusion Embedding Learning for Knowledge Graph Completion}},
volume = {7},
year = {2019}
}
@article{Sang20198404,
abstract = {Drug discovery is the process by which new candidate medications are discovered. Developing a new drug is a lengthy, complex, and expensive process. Here, in this paper, we propose a biomedical knowledge graph embedding-based recurrent neural network method called GrEDeL, which discovers potential drugs for diseases by mining published biomedical literature. GrEDeL first builds a biomedical knowledge graph by exploiting the relations extracted from biomedical abstracts. Then, the graph data are converted into a low dimensional space by leveraging the knowledge graph embedding methods. After that, a recurrent neural network model is trained by the known drug therapies which are represented by graph embeddings. Finally, it uses the learned model to discover candidate drugs for diseases of interest from biomedical literature. The experimental results show that our method could not only effectively discover new drugs by mining literature, but also could provide the corresponding mechanism of actions for the candidate drugs. It could be a supplementary method for the current traditional drug discovery methods. {\textcopyright} 2013 IEEE.},
annote = {cited By 5},
author = {Sang, S and Yang, Z and Liu, X and Wang, L and Lin, H and Wang, J and Dumontier, M},
doi = {10.1109/ACCESS.2018.2886311},
issn = {21693536},
journal = {IEEE Access},
keywords = {Biomedical abstracts; Biomedical literature; Drug,Deep learning; Drug therapy; Recurrent neural netw,Knowledge management},
pages = {8404--8415},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery from Biomedical Literatures}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058619411&doi=10.1109%2FACCESS.2018.2886311&partnerID=40&md5=d3d12457982d881365d535bd869d632d},
volume = {7},
year = {2019}
}
@inproceedings{8970718,
abstract = {Cross-lingual Entity Alignment (CEA) aims at identifying entities with their counterparts in different language knowledge graphs. Knowledge embedding alignment plays an important role in CEA due to its advantages of easy implementation and run-time robustness. However, existing embedding alignment methods haven't considered the problem of embedding distribution alignment which refers to the alignment of spatial shapes of embedding spaces. To this end, we present a new Adversarial Knowledge Embedding framework (AKE for short) that jointly learns the representation, mapping and adversarial modules in an end-to-end manner. By reducing the discrepancy of embedding distributions, AKE can approximately preserve an isomorphism between source and target embeddings. In addition, we introduce two new orthogonality constraints into mapping to obtain the self-consistency and numerical stability of transformation. Experiments on real-world datasets demonstrate that our method significantly outperforms state-of-the-art baselines.},
author = {Lin, X and Yang, H and Wu, J and Zhou, C and Wang, B},
booktitle = {2019 IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2019.00053},
issn = {2374-8486},
keywords = {graph theory;learning (artificial intelligence);na},
month = {nov},
pages = {429--438},
title = {{Guiding Cross-lingual Entity Alignment via Adversarial Knowledge Embedding}},
year = {2019}
}
@article{Wojtusiak20181,
abstract = {Ontologies are popular way of representing knowledge and semantics of data in medical and health fields. Surprisingly, few machine learning methods allow for encoding semantics of data and even fewer allow for using ontologies to guide learning process. This paper discusses the use of data semantics and ontologies in health and medical applications of supervised learning, and particularly describes how the Unified Medical Language System (UMLS) is used within AQ21 rule learning software. Presented concepts are illustrated using two applications based on distinctly different types of data and methodological issues. {\textcopyright} 2018, IFIP International Federation for Information Processing.},
annote = {cited By 0; Conference of 4th IFIP WG 12.6 International Workshop on Artificial Intelligence for Knowledge Management, AI4KM 2016 Held at International Joint Conference on Artificial Intelligence, IJCAI 2016 ; Conference Date: 9 July 2016 Through 9 July 2016; Conference Code:214419},
author = {Wojtusiak, J and Min, H and Elashkar, E and Mobahi, H},
doi = {10.1007/978-3-319-92928-6_1},
editor = {{Mercier-Laurent E.}, Boulanger D},
isbn = {9783319929279},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {Application programs; Artificial intelligence; Kno,Bio-ontologies; Biomedical ontologies; Learning p,Ontology},
pages = {1--18},
publisher = {Springer New York LLC},
title = {{Guiding supervised learning by bio-ontologies in medical data analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049046969&doi=10.1007%2F978-3-319-92928-6_1&partnerID=40&md5=8fc1cfd952f3ce39781d80e9bf91a509},
volume = {518},
year = {2018}
}
@article{9123330,
abstract = {Knowledge graphs (KGs) play an important role in many real-world applications like information retrieval, question answering, relation extraction, etc. To reveal implicit knowledge from a knowledge graph (KG), viz. knowledge graph completion (KGC), is a crucial task for the downstream applications based on KG. For this purpose various embedding-based approaches have been proposed recently. This paper proposes a new approach named HRESCAL to KGC. It extends the well-known embedding-based approach RESCAL by introducing Hamming distance-based encoder to capture implicit multihop and partial inverse relation features in a KG. Experimental results on widely used KGC benchmarks show that the new approach achieves state-of-the-art or is competitive AUC performance.},
author = {Chen, P and Wang, Y and Yu, Q and Fan, Y and Feng, R},
doi = {10.1109/ACCESS.2020.3004448},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {graph theory;Hamming codes;knowledge representatio},
pages = {117146--117158},
title = {{Hamming Distance Encoding Multihop Relation Knowledge Graph Completion}},
volume = {8},
year = {2020}
}
@article{8466588,
abstract = {The discovery of disease-causing genes is a critical step towards understanding the nature of a disease and determining a possible cure for it. In recent years, many computational methods to identify disease genes have been proposed. However, making full use of disease-related (e.g., symptoms) and gene-related (e.g., gene ontology and protein-protein interactions) information to improve the performance of disease gene prediction is still an issue. Here, we develop a heterogeneous disease-gene-related network (HDGN) embedding representation framework for disease gene prediction (called HerGePred). Based on this framework, a low-dimensional vector representation (LVR) of the nodes in the HDGN can be obtained. Then, we propose two specific algorithms, namely, an LVR-based similarity prediction and a random walk with restart on a reconstructed heterogeneous disease-gene network (RWRDGN), to predict disease genes with high performance. First, to validate the rationality of the framework, we analyze the similarity-based overlap distribution of disease pairs and design an experiment for disease-gene association recovery, the results of which revealed that the LVR of nodes performs well at preserving the local and global network structure of the HDGN. Then, we apply tenfold cross validation and external validation to compare our methods with other well-known disease gene prediction algorithms. The experimental results show that the RW-RDGN performs better than the state-of-the-art algorithm. The prediction results of disease candidate genes are essential for molecular mechanism investigation and experimental validation. The source codes of HerGePred and experimental data are available at https://github.com/yangkuoone/HerGePred.},
author = {Yang, K and Wang, R and Liu, G and Shu, Z and Wang, N and Zhang, R and Yu, J and Chen, J and Li, X and Zhou, X},
doi = {10.1109/JBHI.2018.2870728},
issn = {2168-2208},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {biology computing;diseases;genetics;molecular biop},
month = {jul},
number = {4},
pages = {1805--1815},
title = {{HerGePred: Heterogeneous Network Embedding Representation for Disease Gene Prediction}},
volume = {23},
year = {2019}
}
@inproceedings{Tissot201872,
abstract = {This paper focuses the problem of learning the knowledge low-dimensional embedding representation for entities and relations extracted from domain-specific datasets. Existing embedding methods aim to represent entities and relations from a knowledge graph as vectors in a continuous low-dimensional space. Different approaches have been proposed, being usually evaluated on standard benchmark knowledge graphs, such as Wordnet and Freebase. However, the nature of such data sources prevents those methods of taking advantage of more detailed and enriched metadata, lacking more accurate results on the evaluation tasks. In this paper, we propose HEXTRATO, a novel embedding approach that extends a traditional baseline model TransE by adding ontology-based constraints in order to better capture the relationships between categorised entities and their symbolic representation in the vector space. Our method is evaluated on an adapted version of Freebase, on a publicly available dataset used on machine learning benchmarks, and on two datasets in the clinical domain. Our method outperforms the state-of-the-art accuracy on the link prediction task, evidencing the learnt entity and relation embedding representation can be used to improve more complex embedding models. Copyright 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
annote = {cited By 2; Conference of 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2018 ; Conference Date: 18 September 2018 Through 20 September 2018; Conference Code:143001},
author = {Tissot, H},
booktitle = {IC3K 2018 - Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
doi = {10.5220/0006923700720081},
editor = {{Fred A.}, Filipe J},
isbn = {9789897583308},
keywords = {Electronic health record; Knowledge embedding; Li,Embeddings,Knowledge engineering; Knowledge management; Ontol},
pages = {72--81},
publisher = {SciTePress},
title = {{HEXTRATO: Using ontology-based constraints to improve accuracy on learning domain-specific entity and relationship embedding representation for knowledge resolution}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059013404&doi=10.5220%2F0006923700720081&partnerID=40&md5=e691d89eb12783e43571b3e32cd9ac9d},
volume = {1},
year = {2018}
}
@inproceedings{Bao2020,
abstract = {This paper proposes a chatbot framework that adopts a hybrid model which consists of a knowledge graph and a text similarity model. Based on this chatbot framework, we build HHH, an online question-and-answer (QA) Healthcare Helper system for answering complex medical questions. HHH maintains a knowledge graph constructed from medical data collected from the Internet. HHH also implements a novel text representation and similarity deep learning model, Hierarchical BiLSTM Attention Model (HBAM), to find the most similar question from a large QA dataset. We compare HBAM with other state-of-the-art language models such as bidirectional encoder representation from transformers (BERT) and Manhattan LSTM Model (MaLSTM). We train and test the models with a subset of the Quora duplicate questions dataset in the medical area. The experimental results show that our model is able to achieve a superior performance than these existing methods. {\textcopyright} 2020 ACM.},
annote = {cited By 2; Conference of 2020 Australasian Computer Science Week Multiconference, ACSW 2020 ; Conference Date: 3 February 2020 Through 7 February 2020; Conference Code:157482},
author = {Bao, Q and Ni, L and Liu, J},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3373017.3373049},
isbn = {9781450376976},
keywords = {Attention model; Chatbot; Knowledge graphs; NAtur,Deep learning; Large dataset; Natural language pro,Long short-term memory},
publisher = {Association for Computing Machinery},
title = {{HHH: An Online Medical Chatbot System based on Knowledge Graph and Hierarchical Bi-Directional Attention}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079865624&doi=10.1145%2F3373017.3373049&partnerID=40&md5=03da275abf6df819631230ddce5ebf38},
year = {2020}
}
@article{8657942,
abstract = {Online movies' recommender systems aim to address the information explosion of movies and make the personalized recommendation for users. Recently, knowledge graphs have been proven to be highly effective to recommender systems, because they are able to fuse various recommendation models and can handle the issues of data sparsity and cold start to improve recommendation performance. However, less consideration is given to the information about the user's properties than the item's properties in the existing knowledge graph recommendation methods, which leads to some limitations in the recommendation results. In this paper, we propose HI2Rec, which integrates multiple information to learn the user's and item's vector representations for top-N recommendation to address the above-mentioned issues. We extract the movie-related information from the Linked Open Data and then leverage the knowledge representation learning approach to embed this information as well as real-world datasets' information of recommender systems to a unified vector space. These vector representations are further calculated to generate a preliminary recommendation list. Finally, we utilize a collaborative filter approach to generate a precision recommendation list. The experimental results on the real-world datasets demonstrate that HI2Rec gives substantial performance improvements against the state-of-the-art recommendation models.},
author = {He, M and Wang, B and Du, X},
doi = {10.1109/ACCESS.2019.2902398},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {collaborative filtering;entertainment;learning (ar},
pages = {30276--30284},
title = {{HI2Rec: Exploring Knowledge in Heterogeneous Information for Movie Recommendation}},
volume = {7},
year = {2019}
}
@inproceedings{10.1145/3394486.3403067,
abstract = {Medical ontologies are widely used to represent and organize medical terminologies. Examples include ICD-9, ICD-10, UMLS etc. The ontologies are often constructed in hierarchical structures, encoding the multi-level subclass relationships among different medical concepts, allowing very fine distinctions between concepts. Medical ontologies provide a great source for incorporating domain knowledge into a healthcare prediction system, which might alleviate the data insufficiency problem and improve predictive performance with rare categories. To incorporate such domain knowledge, Gram, a recent graph attention model, represents a medical concept as a weighted sum of its ancestors' embeddings in the ontology using an attention mechanism. Although showing improved performance, Gram only considers the unordered ancestors of a concept, which does not fully leverage the hierarchy thus having limited expressibility. In this paper, we propose Hierarchical Attention Propagation (HAP), a novel medical ontology embedding model that hierarchically propagate attention across the entire ontology structure, where a medical concept adaptively learns its embedding from all other concepts in the hierarchy instead of only its ancestors. We prove that HAP learns more expressive medical concept embeddings -- from any medical concept embedding we are able to fully recover the entire ontology structure. Experimental results on two sequential procedure/diagnosis prediction tasks demonstrate HAP's better embedding quality than Gram and other baselines. Furthermore, we find that it is not always best to use the full ontology. Sometimes using only lower levels of the hierarchy outperforms using all levels.},
address = {New York, NY, USA},
author = {Zhang, Muhan and King, Christopher R and Avidan, Michael and Chen, Yixin},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3394486.3403067},
isbn = {9781450379984},
keywords = {attention mechanism,medical ontology,network embedding},
pages = {249--256},
publisher = {Association for Computing Machinery},
series = {KDD '20},
title = {{Hierarchical Attention Propagation for Healthcare Representation Learning}},
url = {https://doi.org/10.1145/3394486.3403067},
year = {2020}
}
@inproceedings{5599780,
abstract = {Hierarchical Classification is a very important classification task for arranging data in a hierarchical structure. Hierarchical arrangement of data is one of the best methods to achieve better understanding of complex data. In this paper, we propose the HMAC method to perform Hierarchical Multi-label Associative Classification. This method uses multiple and negative rules to predict class-set and to filter exceptional cases in order to improve both accuracy and explanatory ability of the resulting classifier. Redundant rule pruning method for negative rules and rule ranking method for hierarchical associative classification are developed. Moreover, we propose a rule evaluation measure, Sim, that tread-off between F-measure and Jaccard's coefficient to encode hierarchical structure meaning of actual and predicted node collections. We show that Sim is robust and simple rule evaluation measure. Experimental results show that HMAC improves accuracy and explanatory ability of hierarchical classifiers compared with various rule ranking and pruning techniques.},
author = {Sangsuriyun, S and Marukatat, S and Waiyamai, K},
booktitle = {9th IEEE International Conference on Cognitive Informatics (ICCI'10)},
doi = {10.1109/COGINF.2010.5599780},
keywords = {data mining;pattern classification;hierarchical mu},
month = {jul},
pages = {919--924},
title = {{Hierarchical Multi-label Associative Classification (HMAC) using negative rules}},
year = {2010}
}
@inproceedings{8953728,
abstract = {In radiologists' routine work, one major task is to read a medical image, e.g., a CT scan, find significant lesions, and describe them in the radiology report. In this paper, we study the lesion description or annotation problem. Given a lesion image, our aim is to predict a comprehensive set of relevant labels, such as the lesion's body part, type, and attributes, which may assist downstream fine-grained diagnosis. To address this task, we first design a deep learning module to extract relevant semantic labels from the radiology reports associated with the lesion images. With the images and text-mined labels, we propose a lesion annotation network (LesaNet) based on a multilabel convolutional neural network (CNN) to learn all labels holistically. Hierarchical relations and mutually exclusive relations between the labels are leveraged to improve the label prediction accuracy. The relations are utilized in a label expansion strategy and a reliable hard example mining algorithm. We also attach a simple score propagation layer on LesaNet to enhance recall and explore implicit relation between labels. Multilabel metric learning is combined with classification to enable interpretable prediction. We evaluated LesaNet on the public DeepLesion dataset, which contains over 32K diverse lesion images. Experiments show that LesaNet can precisely annotate the lesions using an ontology of 171 fine-grained labels with an average AUC of 0.9344.},
author = {Yan, K and Peng, Y and Sandfort, V and Bagheri, M and Lu, Z and Summers, R M},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2019.00872},
issn = {2575-7075},
keywords = {computerised tomography;convolutional neural nets;},
month = {jun},
pages = {8515--8524},
title = {{Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images: Learning From Radiology Reports and Label Ontology}},
year = {2019}
}
@article{Shen2019,
abstract = {Background: In precision medicine, deep phenotyping is defined as the precise and comprehensive analysis of phenotypic abnormalities, aiming to acquire a better understanding of the natural history of a disease and its genotype-phenotype associations. Detecting phenotypic relevance is an important task when translating precision medicine into clinical practice, especially for patient stratification tasks based on deep phenotyping. In our previous work, we developed node embeddings for the Human Phenotype Ontology (HPO) to assist in phenotypic relevance measurement incorporating distributed semantic representations. However, the derived HPO embeddings hold only distributed representations for IS-A relationships among nodes, hampering the ability to fully explore the graph. Methods: In this study, we developed a framework, HPO2Vec+, to enrich the produced HPO embeddings with heterogeneous knowledge resources (i.e., DECIPHER, OMIM, and Orphanet) for detecting phenotypic relevance. Specifically, we parsed disease-phenotype associations contained in these three resources to enrich non-inheritance relationships among phenotypic nodes in the HPO. To generate node embeddings for the HPO, node2vec was applied to perform node sampling on the enriched HPO graphs based on random walk followed by feature learning over the sampled nodes to generate enriched node embeddings. Four HPO embeddings were generated based on different graph structures, which we hereafter label as HPOEmb-Original, HPOEmb-DECIPHER, HPOEmb-OMIM, and HPOEmb-Orphanet. We evaluated the derived embeddings quantitatively through an HPO link prediction task with four edge embeddings operations and six machine learning algorithms. The resulting best embeddings were then evaluated for patient stratification of 10 rare diseases using electronic health records (EHR) collected at Mayo Clinic. We assessed our framework qualitatively by visualizing phenotypic clusters and conducting a use case study on primary hyperoxaluria (PH), a rare disease, on the task of inferring relevant phenotypes given 22 annotated PH related phenotypes. Results: The quantitative link prediction task shows that HPOEmb-Orphanet achieved an optimal AUROC of 0.92 and an average precision of 0.94. In addition, HPOEmb-Orphanet achieved an optimal F1 score of 0.86. The quantitative patient similarity measurement task indicates that HPOEmb-Orphanet achieved the highest average detection rate for similar patients over 10 rare diseases and performed better than other similarity measures implemented by an existing tool, HPOSim, especially for pairwise patients with fewer shared common phenotypes. The qualitative evaluation shows that the enriched HPO embeddings are generally able to detect relationships among nodes with fine granularity and HPOEmb-Orphanet is particularly good at associating phenotypes across different disease systems. For the use case of detecting relevant phenotypic characterizations for given PH related phenotypes, HPOEmb-Orphanet outperformed the other three HPO embeddings by achieving the highest average P@5 of 0.81 and the highest P@10 of 0.79. Compared to seven conventional similarity measurements provided by HPOSim, HPOEmb-Orphanet is able to detect more relevant phenotypic pairs, especially for pairs not in inheritance relationships. Conclusion: We drew the following conclusions based on the evaluation results. First, with additional non-inheritance edges, enriched HPO embeddings can detect more associations between fine granularity phenotypic nodes regardless of their topological structures in the HPO graph. Second, HPOEmb-Orphanet not only can achieve the optimal performance through link prediction and patient stratification based on phenotypic similarity, but is also able to detect relevant phenotypes closer to domain expert's judgments than other embeddings and conventional similarity measurements. Third, incorporating heterogeneous knowledge resources do not necessarily result in better performance for detecting relevant phenotypes. From a clinical perspective, in our use case study, clinical-oriented knowledge resources (e.g., Orphanet) can achieve better performance in detecting relevant phenotypic characterizations compared to biomedical-oriented knowledge resources (e.g., DECIPHER and OMIM). {\textcopyright} 2019 Elsevier Inc.},
annote = {cited By 4},
author = {Shen, F and Peng, S and Fan, Y and Wen, A and Liu, S and Wang, Y and Wang, L and Liu, H},
doi = {10.1016/j.jbi.2019.103246},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Algorithms; Area Under Curve; Biological Ontologi,Article; B cell lymphoma; case study; chronic kid,Diseases; Embeddings; Forecasting; Knowledge manag,Distributed representation; Electronic health rec,Genetic; Electronic Health Records; Genetic Assoc,Graph theory,Statistical; Phenotype; Precision Medicine; Rare},
publisher = {Academic Press Inc.},
title = {{HPO2Vec+: Leveraging heterogeneous knowledge resources to enrich node embeddings for the Human Phenotype Ontology}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068448928&doi=10.1016%2Fj.jbi.2019.103246&partnerID=40&md5=c5daa776594476d6ca37be86adee3a63},
volume = {96},
year = {2019}
}
@inproceedings{Song201925,
abstract = {Text classification is one of the most important tasks in natural language processing and information retrieval due to the increasing availability of documents in digital form and the ensuing need to access them in flexible ways. By assigning documents to labeled classes, text classification can reduce the search space and expedite the process of retrieving relevant documents. In this paper, we propose a novel text representation method, Hybrid Word Embeddings (HWE), which combines semantic information obtained from WordNet and contextual information extracted from text documents to provide concise and accurate representations of text documents. The proposed HWE method can improve the efficiency of deriving word semantics from text by taking advantage of the semantic relationships extracted from WordNet with less training corpus. Experimental study on classification of documents shows that the proposed HWE outperforms existing methods, including Doc2Vec and Word2Vec, in terms of classification accuracy, recall, precision, etc. {\textcopyright} 2019 Association for Computing Machinery.},
annote = {cited By 1; Conference of 3rd International Conference on Natural Language Processing and Information Retrieval, NLPIR 2019 ; Conference Date: 28 June 2019 Through 30 June 2019; Conference Code:150792},
author = {Song, X and Srimani, P K and Wang, J Z},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3342827.3342837},
isbn = {9781450362795},
keywords = {Classification (of information); Electronic docume,Classification accuracy; Contextual information;,Text processing},
pages = {25--29},
publisher = {Association for Computing Machinery},
title = {{Hwe: Hybrid word embeddings for text classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071688774&doi=10.1145%2F3342827.3342837&partnerID=40&md5=7dfa65b47ca37901592dd5e3252269f6},
year = {2019}
}
@inproceedings{9185274,
abstract = {Representation learning of knowledge graph aims to embed both entities and relations into a low-dimensional space. However, there are still some gaps in the knowledge graph embedding methods in providing interpretation of knowledge graph while encoding the semantic meaning of the concepts and structured information of knowledge graphs. To address this issue, we propose a hybrid approach for Accurate and Interpretable Representation Learning (AIRL) method for embedding entities and relations of knowledge graphs by utilizing the rich information located in entity descriptions and hierarchical types of entities. Here we use hybrid approach to learn interpretable knowledge representations by capturing the semantics and structure of entities using this rich information. We adopt FB15K dataset generated from a large knowledge graph freebase, to evaluate the performance of the proposed model. The results of experiments demonstrate AIRL significantly outperforms translation embeddings and other state-of-the-art methods.},
author = {Yogendran, N and Kanagarajah, A and Chandiran, K and Thayasivam, U},
booktitle = {2020 Moratuwa Engineering Research Conference (MERCon)},
doi = {10.1109/MERCon50084.2020.9185274},
keywords = {graph theory;information retrieval;knowledge repre},
month = {jul},
pages = {1--6},
title = {{Hybrid Approach for Accurate and Interpretable Representation Learning of Knowledge Graph}},
year = {2020}
}
@article{9194302,
abstract = {Endowing ubiquitous robots with cognitive capabilities for recognizing emotions, sentiments, affects, and moods of humans in their context is an important challenge, which requires sophisticated and novel approaches of emotion recognition. Most studies explore data-driven pattern recognition techniques that are generally highly dependent on learning data and insufficiently effective for emotion contextual recognition. In this article, a hybrid model-based emotion contextual recognition approach for cognitive assistance services in ubiquitous environments is proposed. This model is based on: 1) a hybrid-level fusion exploiting a multilayer perceptron (MLP) neural-network model and the possibilistic logic and 2) an expressive emotional knowledge representation and reasoning model to recognize nondirectly observable emotions; this model exploits jointly the emotion upper ontology (EmUO) and the n-ary ontology of events HTemp supported by the NKRL language. For validation purposes of the proposed approach, experiments were carried out using a YouTube dataset, and in a real-world scenario dedicated to the cognitive assistance of visitors in a smart devices showroom. Results demonstrated that the proposed multimodal emotion recognition model outperforms all baseline models. The real-world scenario corroborates the effectiveness of the proposed approach in terms of emotion contextual recognition and management and in the creation of emotion-based assistance services.},
author = {Ayari, N and Abdelkawy, H and Chibani, A and Amirat, Y},
doi = {10.1109/TCYB.2020.3013112},
issn = {2168-2275},
journal = {IEEE Transactions on Cybernetics},
keywords = {Emotion recognition;Robots;Ontologies;Hidden Marko},
pages = {1--10},
title = {{Hybrid Model-Based Emotion Contextual Recognition for Cognitive Assistance Services}},
year = {2020}
}
@inproceedings{Dasgupta20202001,
abstract = {Knowledge Graph (KG) embedding has emerged as an active area of research resulting in the development of several KG embedding methods. Relational facts in KG often show temporal dynamics, e.g., the fact (Cristiano Ronaldo, playsFor, Manchester United) is valid only from 2003 to 2009. Most of the existing KG embedding methods ignore this temporal dimension while learning embeddings of the KG elements. In this paper, we propose HyTE, a temporally aware KG embedding method which explicitly incorporates time in the entity-relation space by associating each timestamp with a corresponding hyperplane. HyTE not only performs KG inference using temporal guidance, but also predicts temporal scopes for relational facts with missing time annotations. Through extensive experimentation on temporal datasets extracted from real-world KGs, we demonstrate the effectiveness of our model over both traditional as well as temporal KG embedding methods. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 17; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Dasgupta, S S and Ray, S N and Talukdar, P},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {Active area; Embedding method; Knowledge graphs;,Embeddings,Geometry; Natural language processing systems},
pages = {2001--2011},
publisher = {Association for Computational Linguistics},
title = {{Hyte: Hyperplane-based temporally aware knowledge graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081738912&partnerID=40&md5=b9ca53716d6b082ab5bb7ec3a2270e2d},
year = {2020}
}
@inproceedings{Gao20198303,
abstract = {Recently, with the ever-growing action categories, zero-shot action recognition (ZSAR) has been achieved by automatically mining the underlying concepts (e.g., actions, attributes) in videos. However, most existing methods only exploit the visual cues of these concepts but ignore external knowledge information for modeling explicit relationships between them. In fact, humans have remarkable ability to transfer knowledge learned from familiar classes to recognize unfamiliar classes. To narrow the knowledge gap between existing methods and humans, we propose an end-to-end ZSAR framework based on a structured knowledge graph, which can jointly model the relationships between action-attribute, action-action, and attribute-attribute. To effectively leverage the knowledge graph, we design a novel Two-Stream Graph Convolutional Network (TS-GCN) consisting of a classifier branch and an instance branch. Specifically, the classifier branch takes the semantic-embedding vectors of all the concepts as input, then generates the classifiers for action categories. The instance branch maps the attribute embeddings and scores of each video instance into an attribute-feature space. Finally, the generated classifiers are evaluated on the attribute features of each video, and a classification loss is adopted for optimizing the whole network. In addition, a self-attention module is utilized to model the temporal information of videos. Extensive experimental results on three realistic action benchmarks Olympic Sports, HMDB51 and UCF101 demonstrate the favorable performance of our proposed framework. {\textcopyright} 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
annote = {cited By 10; Conference of 33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019 ; Conference Date: 27 January 2019 Through 1 February 2019; Conference Code:160302},
author = {Gao, J and Zhang, T and Xu, C},
booktitle = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
isbn = {9781577358091},
keywords = {Action recognition; Convolutional networks; Exter,Benchmarking; Classifiers; Convolution; Embeddings,Convolutional neural networks},
pages = {8303--8311},
publisher = {AAAI Press},
title = {{I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090804953&partnerID=40&md5=a8ae43d44ab51dd57c0bbed3a8d530ac},
year = {2019}
}
@inproceedings{7514617,
abstract = {Information extraction and classification using Natural Language Processing techniques of layered architecture such as pre-processing task, processing of semantic analysis etc., helps in implementing further deeper evaluation techniques for the accuracy of natural language based electronic database. This paper explores relational information extraction of multilingual IndoWordNet database matching with domain specific terms. Further, extracted information are processed through conventional statistical methods, Normalized Web Distance (NWD) similarity method and two other machine learning evaluation techniques such as Support Vector Machine (SVM), Neural Network (NN) to compare with their accuracy. Results of machine leaning based techniques outperform with significant improved accuracy over conventional methods. The objective of using these techniques along with semantic web technology is to initiate a proof of concept for ontology generation by identification and classification of extracted relational information from IndoWordNet. This paper also highlights domain specific challenges and issues in developing relational model of ontology.},
author = {Sinha, B and Garg, M and Chandra, S},
booktitle = {2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT)},
doi = {10.1109/ICCTICT.2016.7514617},
keywords = {information retrieval;learning (artificial intelli},
month = {mar},
pages = {415--420},
title = {{Identification and classification of relations for Indian languages using machine learning approaches for developing a domain specific ontology}},
year = {2016}
}
@inproceedings{8531243,
abstract = {Healthcare environment is rich of data, but still needs knowledge extraction that is necessarily important for saving people lives. Medical Knowledge discovery is a process of extracting knowledge patterns from biomedical data, which is useful and crucial for making effective decisions especially in developing strategies and policies of preventive medical treatments. Data mining methods are the best-known way to recognize the hidden data standards. Ontology engineering used to improve knowledge domain representation, and further is considered for the enhancement and refinement of the mining techniques based on the discovered patterns driven from ontological data mining. In this paper, we apply ontology driven data mining techniques on a data set of diabetes patients who have cardiovascular disease. That process performed to identify the relationship between type two diabetes mellitus patients and their important laboratory tests specified by doctors. Doctors aim to investigate the probability of cardiovascular disease occurrence and stroke happening. Ontology driven Data mining techniques also used in experimental study as well as rule induction, association rules methods. In a late phase, we used frequent pattern discovery and rules induction method using ontological data mining algorithm (RMonto). The findings of this study reveals that the use of ontologies minimizes the number of attributes in the preprocessing stage and helps in all data mining stages; in addition to its important role in ontological data mining, we have a higher learning accuracy ratio exceeding 90%. The results of data mining methods and ontological data mining shows that the significance of some laboratory tests like: LP(a),CRP,HDL,FBG,TG,LDH and Chol to predict CVD risk among T2DM patients with a high accuracy.},
author = {Qrenawi, M I and {Al Sarraj}, W},
booktitle = {2018 International Conference on Promising Electronic Technologies (ICPET)},
doi = {10.1109/ICPET.2018.00030},
keywords = {cardiovascular system;data mining;diseases;learnin},
month = {oct},
pages = {129--134},
title = {{Identification of Cardiovascular Diseases Risk Factors among Diabetes Patients Using Ontological Data Mining Techniques}},
year = {2018}
}
@inproceedings{10.1145/3331184.3331220,
abstract = {We propose a method for identifying a set of entity properties from text. Identifying entity properties is similar to a relation extraction task that can be cast as a classification of sentences. Normally, this task can be achieved by distant supervised learning by automatically preparing training sentences for each property; however, it is impractical to prepare training sentences for every property. Therefore, we describe a zero-shot learning problem for this task and propose a neural network-based model that does not rely on a complete training set comprising training sentences for every property. To achieve this, we utilize embeddings of properties obtained from a knowledge graph embedding using different components of a knowledge graph structure. The embeddings of properties are combined with the model to enable identification of properties with no available training sentences. By using our newly constructed dataset as well as an existing dataset, experiments revealed that our model achieved a better performance for properties with no training sentences, relative to baseline results, even comparable to that achieved for properties with training sentences.},
address = {New York, NY, USA},
author = {Imrattanatrai, Wiradee and Kato, Makoto P and Yoshikawa, Masatoshi},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3331184.3331220},
isbn = {9781450361729},
keywords = {property embedding,property identification,sentence classification},
pages = {195--204},
publisher = {Association for Computing Machinery},
series = {SIGIR'19},
title = {{Identifying Entity Properties from Text with Zero-Shot Learning}},
url = {https://doi.org/10.1145/3331184.3331220},
year = {2019}
}
@article{Qin2018213,
abstract = {Semantic segmentation is a fundamental and challenging task for semantic mapping. Most of the existing approaches focus on taking advantage of deep learning and conditional random fields (CRFs) based techniques to acquire pixel-level labeling. One major issue among these methods is the limited capacity of deep learning techniques on utilizing the obvious relationships among different objects which are specified as semantic knowledge. For CRFs, their basic low-order forms cannot bring substantial enhancement for labeling performance. To this end, we propose a novel approach that employs semantic knowledge to intensify the image segmentation capability. The semantic constraints are established by constructing an ontology-based knowledge network. In particular, hierarchical conditional random fields fused with semantic knowledge are used to infer and optimize the final segmentation. Experimental comparison with the state-of-the-art semantic segmentation methods has been carried out. Results reveal that our method improves the performance in terms of pixel and object-level. {\textcopyright} Springer Nature Switzerland AG 2018.},
annote = {cited By 0; Conference of 1st Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2018 ; Conference Date: 23 November 2018 Through 26 November 2018; Conference Code:220829},
author = {Qin, C and Zhang, Y and Hu, M and Chu, H and Wang, L},
doi = {10.1007/978-3-030-03398-9_19},
editor = {{Lai J.-H. Zha H.}, Zhou J Liu C.-L. Tan T Zheng N Chen X},
isbn = {9783030033972},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Computer vision; Deep learning; Ontology; Pixels;,Conditional random field; Conditional Random Fiel,Image segmentation},
pages = {213--225},
publisher = {Springer Verlag},
title = {{Image segmentation based on semantic knowledge and hierarchical conditional random fields}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057080761&doi=10.1007%2F978-3-030-03398-9_19&partnerID=40&md5=d41233fd0f9a234142423e5d6c245b3b},
volume = {11256 LNCS},
year = {2018}
}
@inproceedings{Chen201515,
abstract = {In recent years, there has been an increas-ing interest in learning a distributed rep-resentation of word sense. Traditional context clustering based models usually require careful tuning of model parame-ters, and typically perform worse on infre-quent word senses. This paper presents a novel approach which addresses these lim-itations by first initializing the word sense embeddings through learning sentence-level embeddings from WordNet glosses using a convolutional neural networks. The initialized word sense embeddings are used by a context clustering based model to generate the distributed representations of word senses. Our learned represen-tations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. {\textcopyright} 2015 Association for Computational Linguistics.},
annote = {cited By 18; Conference of 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015 ; Conference Date: 26 July 2015 Through 31 July 2015; Conference Code:114195},
author = {Chen, T and Xu, R and He, Y and Wang, X},
booktitle = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/p15-2003},
isbn = {9781941643730},
keywords = {Analogical reasoning; Context clustering; Convolu,Computational linguistics; Embeddings; Neural netw,Natural language processing systems},
pages = {15--20},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Improving distributed representation of word sense via WordNet Gloss composition and context clustering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944073513&doi=10.3115%2Fv1%2Fp15-2003&partnerID=40&md5=76cc11826073e476e60cbe7894ea5831},
volume = {2},
year = {2015}
}
@article{Dekhili201910,
abstract = {Commonsense can be vital in some applications like Natural Language Understanding, where it is often required to resolve ambiguity arising from implicit knowledge and under-specification. In spite of the remarkable success of neural network approaches on a variety of Natural Language Processing tasks, many of them struggle to react effectively in cases that require commonsense knowledge. In the present research paper, we take advantage of the availability of the open multilingual knowledge graph ConceptNet, by using it as an additional external resource in a Named Entity Recognition system (NER). Our proposed architecture involves BiLSTM layers combined with a CRF layer that was augmented with some features such as pre-trained word embedding layers and dropout layers. Moreover, apart from using word representations, we used also character-based representation to capture the morphological and the orthographic information. Our experiments and evaluations showed an improvement in the overall performance with +2.86 in the F1-measure. To the best of our knowledge, there is no study relating the integration of a commonsense knowledge base in NER. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 16th Pacific Rim Knowledge Acquisition Workshop, PKAW 2019 held in conjunction with the 16th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2019 ; Conference Date: 26 August 2019 Through 27 August 2019; Conference Code:230979},
author = {Dekhili, G and Le, N T and Sadat, F},
doi = {10.1007/978-3-030-30639-7_2},
editor = {{Ohara K.}, Bai Q},
isbn = {9783030306380},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Commonsense; Commonsense knowledge; Commonsense k,Deep neural networks; Embeddings; Intelligent syst,Natural language processing systems},
pages = {10--20},
publisher = {Springer Verlag},
title = {{Improving Named Entity Recognition with Commonsense Knowledge Pre-training}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072853690&doi=10.1007%2F978-3-030-30639-7_2&partnerID=40&md5=c11be8283c4bb247e076e2b193ba4c04},
volume = {11669 LNAI},
year = {2019}
}
@inproceedings{9101658,
abstract = {Relation extraction (RE) aims at extracting the relation between two entities from the text corpora. It is a crucial task for Knowledge Graph (KG) construction. Most existing methods predict the relation between an entity pair by learning the relation from the training sentences, which contain the targeted entity pair. In contrast to existing distant supervision approaches that suffer from insufficient training corpora to extract relations, our proposal of mining implicit mutual relation from the massive unlabeled corpora transfers the semantic information of entity pairs into the RE model, which is more expressive and semantically plausible. After constructing an entity proximity graph based on the implicit mutual relations, we preserve the semantic relations of entity pairs via embedding each vertex of the graph into a low-dimensional space. As a result, we can easily and flexibly integrate the implicit mutual relations and other entity information, such as entity types, into the existing RE methods.Our experimental results on a New York Times and another Google Distant Supervision datasets suggest that our proposed neural RE framework provides a promising improvement for the RE task, and significantly outperforms the state-of-the-art methods. Moreover, the component for mining implicit mutual relations is so flexible that can help to improve the performance of both CNN-based and RNN-based RE models significant.},
author = {Kuang, J and Cao, Y and Zheng, J and He, X and Gao, M and Zhou, A},
booktitle = {2020 IEEE 36th International Conference on Data Engineering (ICDE)},
doi = {10.1109/ICDE48307.2020.00093},
issn = {2375-026X},
keywords = {convolutional neural nets;data mining;graph theory},
month = {apr},
pages = {1021--1032},
title = {{Improving Neural Relation Extraction with Implicit Mutual Relations}},
year = {2020}
}
@article{Li2019,
abstract = {Background: Accurately recognizing rare diseases based on symptom description is an important task in patient triage, early risk stratification, and target therapies. However, due to the very nature of rare diseases, the lack of historical data poses a great challenge to machine learning-based approaches. On the other hand, medical knowledge in automatically constructed knowledge graphs (KGs) has the potential to compensate the lack of labeled training examples. This work aims to develop a rare disease classification algorithm that makes effective use of a knowledge graph, even when the graph is imperfect. Method: We develop a text classification algorithm that represents a document as a combination of a "bag of words" and a "bag of knowledge terms," where a "knowledge term" is a term shared between the document and the subgraph of KG relevant to the disease classification task. We use two Chinese disease diagnosis corpora to evaluate the algorithm. The first one, HaoDaiFu, contains 51,374 chief complaints categorized into 805 diseases. The second data set, ChinaRe, contains 86,663 patient descriptions categorized into 44 disease categories. Results: On the two evaluation data sets, the proposed algorithm delivers robust performance and outperforms a wide range of baselines, including resampling, deep learning, and feature selection approaches. Both classification-based metric (macro-averaged F 1 score) and ranking-based metric (mean reciprocal rank) are used in evaluation. Conclusion: Medical knowledge in large-scale knowledge graphs can be effectively leveraged to improve rare diseases classification models, even when the knowledge graph is incomplete. {\textcopyright} 2019 The Author(s).},
annote = {cited By 2},
author = {Li, X and Wang, Y and Wang, D and Yuan, W and Peng, D and Mei, Q},
doi = {10.1186/s12911-019-0938-1},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
keywords = {Algorithms; Humans; Machine Learning; Pattern Rec,Automated; Rare Diseases; Triage,algorithm; automated pattern recognition; classifi},
publisher = {BioMed Central Ltd.},
title = {{Improving rare disease classification using imperfect knowledge graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076117282&doi=10.1186%2Fs12911-019-0938-1&partnerID=40&md5=88fb5bcbfba3f4be4c557c21fb112661},
volume = {19},
year = {2019}
}
@article{Baier201753,
abstract = {Structured scene descriptions of images are useful for the automatic processing and querying of large image databases. We show how the combination of a statistical semantic model and a visual model can improve on the task of mapping images to their associated scene description. In this paper we consider scene descriptions which are represented as a set of triples (subject, predicate, object), where each triple consists of a pair of visual objects, which appear in the image, and the relationship between them (e.g. man-riding-elephant, man-wearing-hat). We combine a standard visual model for object detection, based on convolutional neural networks, with a latent variable model for link prediction. We apply multiple state-of-the-art link prediction methods and compare their capability for visual relationship detection. One of the main advantages of link prediction methods is that they can also generalize to triples which have never been observed in the training data. Our experimental results on the recently published Stanford Visual Relationship dataset, a challenging real world dataset, show that the integration of a statistical semantic model using link prediction methods can significantly improve visual relationship detection. Our combined approach achieves superior performance compared to the state-of-the-art method from the Stanford computer vision group. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 4; Conference of 16th International Semantic Web Conference, ISWC 2017 ; Conference Date: 21 October 2017 Through 25 October 2017; Conference Code:200299},
author = {Baier, S and Ma, Y and Tresp, V},
doi = {10.1007/978-3-319-68288-4_4},
editor = {{Cudre-Mauroux P. Lange C.}, d'Amato C Fernandez M Heflin J Lecue F Tamma V Sequeda J},
isbn = {9783319682877},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automatic processing; Convolutional neural networ,Forecasting; Image enhancement; Neural networks; O,Semantic Web},
pages = {53--68},
publisher = {Springer Verlag},
title = {{Improving visual relationship detection using semantic modeling of scene descriptions}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032230455&doi=10.1007%2F978-3-319-68288-4_4&partnerID=40&md5=123218b99118612de516d85a5fa41d4e},
volume = {10587 LNCS},
year = {2017}
}
@article{Sun2019188,
abstract = {Representation learning of medical knowledge graphs aims to embedding entities and relations in low-dimensional vector spaces, which is beneficial to the application of medical knowledge graphs in intelligent medical systems such as intelligent guidance, disease risk prediction and question answering system of medical field. Recently, some translation-based methods including TransE, TransH and TransR built entity and relation embeddings by regarding a relation as translation from head entity to tail entity. These methods solely use the information of triplets and don't take text information into consideration. In this paper, we process a novel representation learning method by incorporating the embeddings of entity descriptions with classical translation-based methods. The embeddings of entity descriptions are built by Doc2Vec. It is easily applied for a large-scale domain-specific knowledge graphs because of its simplicity. Besides, we compare our method with classical translation-based methods to demonstrate the effectiveness of our method in medical knowledge graphs representation learning. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 2; Conference of 4th International Conference on Human Centered Computing, HCC 2018 ; Conference Date: 5 December 2018 Through 7 December 2018; Conference Code:224739},
author = {Sun, X and Man, Y and Zhao, Y and He, J and Liu, N},
doi = {10.1007/978-3-030-15127-0_19},
editor = {{Zu Q. Rodriguez Garcia J.G.}, Tang Y},
isbn = {9783030151263},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Embeddings; Natural language processing systems; V,Graphic methods,Knowledge graphs; Learning methods; Low dimension},
pages = {188--194},
publisher = {Springer Verlag},
title = {{Incorporating Description Embeddings into Medical Knowledge Graphs Representation Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064599787&doi=10.1007%2F978-3-030-15127-0_19&partnerID=40&md5=22dc270d10fa3eb82dc305acaf6c6a70},
volume = {11354 LNCS},
year = {2019}
}
@inproceedings{Luo20182473,
abstract = {Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 19; Conference of 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018 ; Conference Date: 15 July 2018 Through 20 July 2018; Conference Code:145927},
author = {Luo, F and Liu, T and Xia, Q and Chang, B and Sui, Z},
booktitle = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
doi = {10.18653/v1/p18-1230},
isbn = {9781948087322},
keywords = {Computational linguistics; Knowledge based systems,Knowledge-based methods; Lexical knowledge; Seman,Natural language processing systems},
pages = {2473--2482},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Incorporating glosses into neural word sense disambiguation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061282520&doi=10.18653%2Fv1%2Fp18-1230&partnerID=40&md5=cab8f2ebd80b8014c14e488dfc15307f},
volume = {1},
year = {2018}
}
@article{Zhao2020,
abstract = {Background: Most biomedical information extraction focuses on binary relations within single sentences. However, extracting n-ary relations that span multiple sentences is in huge demand. At present, in the cross-sentence n-ary relation extraction task, the mainstream method not only relies heavily on syntactic parsing but also ignores prior knowledge. Results: In this paper, we propose a novel cross-sentence n-ary relation extraction method that utilizes the multihead attention and knowledge representation that is learned from the knowledge graph. Our model is built on self-attention, which can directly capture the relations between two words regardless of their syntactic relation. In addition, our method makes use of entity and relation information from the knowledge base to impose assistance while predicting the relation. Experiments on n-ary relation extraction show that combining context and knowledge representations can significantly improve the n-ary relation extraction performance. Meanwhile, we achieve comparable results with state-of-the-art methods. Conclusions: We explored a novel method for cross-sentence n-ary relation extraction. Unlike previous approaches, our methods operate directly on the sequence and learn how to model the internal structures of sentences. In addition, we introduce the knowledge representations learned from the knowledge graph into the cross-sentence n-ary relation extraction. Experiments based on knowledge representation learning show that entities and relations can be extracted in the knowledge graph, and coding this knowledge can provide consistent benefits. {\textcopyright} 2020 The Author(s).},
annote = {cited By 0},
author = {Zhao, D and Wang, J and Zhang, Y and Wang, X and Lin, H and Yang, Z},
doi = {10.1186/s12859-020-03629-9},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Algorithms; Biomedical Research; Humans; Knowledg,Biomedical information extractions; Internal stru,Data mining; Knowledge based systems; Syntactics,Knowledge representation,Theoretical; Reproducibility of Results,article; attention; extraction; feature learning},
number = {1},
publisher = {BioMed Central},
title = {{Incorporating representation learning and multihead attention to improve biomedical cross-sentence n-ary relation extraction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088495556&doi=10.1186%2Fs12859-020-03629-9&partnerID=40&md5=73c1ef57e5d7da6f09651b7fc569c8a6},
volume = {21},
year = {2020}
}
@inproceedings{Barajas2013,
abstract = {We present a framework based on Statistical Topics Models, Language Models, Information Extraction, and Ontology Analysis to retrieve healthcare related documents for the CLEF eHealth 2013 Task 3. In this framework we add global information based on latent topics from the documents to improve the document retrieval. We perform six different experiments which consist of a baseline and six variants of the model. Preliminary results show that the use of Language Models with a bag of words scheme results better estimates. However model tunning in the Topic Based model is required to achieve optimal results.},
annote = {cited By 1; Conference of 2013 Cross Language Evaluation Forum Conference, CLEF 2013 ; Conference Date: 23 September 2013 Through 26 September 2013; Conference Code:110354},
author = {Barajas, K L C and Akella, R},
booktitle = {CEUR Workshop Proceedings},
editor = {{Forner P. Ferro N.}, Navigli R Tufis D},
issn = {16130073},
keywords = {Bag of words; Document Retrieval; Ehealth; Global,Computational linguistics; Health care; Informatio,Data mining},
publisher = {CEUR-WS},
title = {{Incorporating statistical topic models in the retrieval of healthcare documents}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922042095&partnerID=40&md5=eb66444a3ab12e21078d9fe25161521e},
volume = {1179},
year = {2013}
}
@inproceedings{Vandewiele20191,
abstract = {Deep-learning based techniques are increasingly being used for different machine learning tasks on knowledge graphs. While it has been shown empirically that these techniques often achieve better predictive performances than their classical counterparts, where features are extracted from the graph, they lack interpretability. Interpretability is a vital aspect in critical domains such as the health and financial sector. In this paper, we present a technique that builds a decision tree of class-specific substructures in order to classify different entities within the knowledge graph. We show how our proposed technique is competitive to current state-of-the-art deep-learning techniques on four benchmark datasets, while being fully interpretable. Copyright {\textcopyright} 2019 for this paper by its authors.},
annote = {cited By 0; Conference of 4th International Workshop on Semantics-Powered Data Mining and Analytics, SEPDA 2019 ; Conference Date: 27 October 2019; Conference Code:151003},
author = {Vandewiele, G and Steenwinckel, B and Ongenae, F and {De Turck}, F},
booktitle = {CEUR Workshop Proceedings},
editor = {{Tao C. Bian J.}, Zhang R He Z},
issn = {16130073},
keywords = {Benchmark datasets; Classical counterpart; Financ,Data mining,Decision trees; Deep learning; Machine learning; T},
pages = {1--6},
publisher = {CEUR-WS},
title = {{Inducing a decision tree with discriminative paths to classify entities in a knowledge graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071758758&partnerID=40&md5=d186f1b6090bff584aff618d3c9129e2},
volume = {2427},
year = {2019}
}
@inproceedings{10.5555/3298023.3298201,
abstract = {Structured knowledge about concepts plays an increasingly important role in areas such as information retrieval. The available ontologies and knowledge graphs that encode such conceptual knowledge, however, are inevitably incomplete. This observation has led to a number of methods that aim to automatically complete existing knowledge bases. Unfortunately, most existing approaches rely on black box models, e.g. formulated as global optimization problems, which makes it difficult to support the underlying reasoning process with intuitive explanations. In this paper, we propose a new method for knowledge base completion, which uses interpretable conceptual space representations and an explicit model for inductive inference that is closer to human forms of commonsense reasoning. Moreover, by separating the task of representation learning from inductive reasoning, our method is easier to apply in a wider variety of contexts. Finally, unlike optimization based approaches, our method can naturally be applied in settings where various logical constraints between the extensions of concepts need to be taken into account.},
author = {Bouraoui, Zied and Jameel, Shoaib and Schockaert, Steven},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4364--4370},
publisher = {AAAI Press},
series = {AAAI'17},
title = {{Inductive Reasoning about Ontologies Using Conceptual Spaces}},
year = {2017}
}
@inproceedings{9006509,
abstract = {The information of the real world is stored as triplets (head entity, relation, tail entity) in knowledge graphs. They are extremely useful resources for many intelligent applications but suffer from incompleteness. This paper proposes a knowledge graph representation model to infer latent privacy based on the existing data in attribute network. In our model, considering the nodes are heterogeneous, we classify the nodes into attribute nodes and entity nodes. In order to protect the privacy of entities, we don't follow the previous methods to learn and store the feature embedding of each entity in knowledge graph. Our model focuses in capturing the restriction patterns of attribute nodes, which is safe when merging data from various sources. Given a triplet (entity node, relation, attribute node), firstly, we get the embedding of the entity node by using a sophisticated way to utilize all the information of the node, not only the node connections but also the external text information. Then, we infer the attribute node for the entity node in a certain relation. Finally, we calculate the probability that the triplet is exist. In experiments, we evaluate our model on the tasks of triplet classification and link prediction. Evaluation results show that our approach outperforms the state-of-the-art methods with an accuracy rate of 90.0% in the task of triplet classification on the person attribute knowledge graph FB13. Besides, our model reaches promising performance by MeanRank =5.10, Hits@l = 35.14% and Hits@5=64.94% in the task of conference prediction on the academic network DBLP.},
author = {Cui, Z and Pan, L and Liu, S and Cui, L},
booktitle = {2019 IEEE International Conference on Big Data (Big Data)},
doi = {10.1109/BigData47090.2019.9006509},
keywords = {data mining;data privacy;graph theory;information},
month = {dec},
pages = {2542--2551},
title = {{Infer Latent Privacy for Attribute Network in Knowledge Graph}},
year = {2019}
}
@article{Buranasing2019419,
abstract = {Cultural heritage is the legacy of social values, traditions and beliefs a group of people inherited from past generation. It can give people to understand the history better where a specific group of people come from and why they behave in their way of life. To preserve cultural heritage, information extraction acquires textual information from various data sources and then converts the information extracted to structured representations. This cultural information is kept digitally for future generations. However, since many approaches on information extraction focus on determining name entities and relationships between a pair of entities, the lack of explicit semantic concept relations makes it difficult for these approaches to apply in a variety of applications. This paper introduces an approach to extracting information from various cultural heritage data sources using word embedding with feature extraction. We focus on identifying named entities and determining the semantic relation triple such as part of speech tags and position tags by using conditional probability for discovering this triple relationship. The method was evaluated using CIDOC-CRM, a common standard ontology for cultural heritage information. The results demonstrated that our approach can achieve high accuracy at 81%. {\textcopyright} 2019, Springer International Publishing AG, part of Springer Nature.},
annote = {cited By 0; Conference of 12th International Conference on Complex, Intelligent, and Software Intensive Systems, CISIS 2018 ; Conference Date: 4 July 2018 Through 6 July 2018; Conference Code:214909},
author = {Buranasing, W and Phoomvuthisarn, S},
doi = {10.1007/978-3-319-93659-8_37},
editor = {{Barolli L. Ikeda M.}, Takizawa M Javaid N},
isbn = {9783319936581},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Artificial intelligence; Historic preservation; In,Conditional probabilities; Cultural heritage info,Data mining},
pages = {419--430},
publisher = {Springer Verlag},
title = {{Information extraction for cultural heritage knowledge acquisition using word vector representation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049246054&doi=10.1007%2F978-3-319-93659-8_37&partnerID=40&md5=96f202cdcfff754b0e67081266fb5687},
volume = {772},
year = {2019}
}
@inproceedings{9150891,
abstract = {We view information extraction from document images as a complex problem that requires a combination of 1) state of the art deep learning vision models for detection of entities and primitive relations, 2) symbolic background knowledge that expresses prior information of spatial and semantic relationships, using the entities and primitive relations from the neural detectors, and 3) learning of symbolic extraction rules using one, or few examples of annotated document images. Several challenges arise in ensuring that this neuro-symbolic software stack works together seamlessly. These include vision-based challenges to ensure that the documents are "seen" at the appropriate level of detail to detect entities; symbolic representation challenges in identifying primitive relations between the entities identified by the vision system; learning-based challenges of identifying the appropriate level of symbolic abstraction for the retrieval rules, the need to identify background knowledge that is relevant to the documents being analyzed, and learning general symbolic rules in data-deficient domains. In this paper, we describe how we meet some of these challenges in the design of our document-reading platform. In particular, we focus on use cases with multiple templates which additionally involves finding structurally similar images in large heterogeneous document image collections. An adaptive lattice based template allocation module was utilized for evaluating document similarity based on both textual content and document structure. A knowledge graph is used for capturing document structure and a relational rule learning system is employed on the knowledge graph for generating extraction rules. Experiments on a publicly shared data-set1 of 1400 trade finance documents demonstrates the viability of the proposed system.},
author = {Rastogi, M and Ali, S A and Rawat, M and Vig, L and Agarwal, P and Shroff, G and Srinivasan, A},
booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW50498.2020.00287},
issn = {2160-7516},
keywords = {data mining;document image processing;graph theory},
month = {jun},
pages = {2377--2385},
title = {{Information Extraction from Document Images via FCA based Template Detection and Knowledge Graph Rule Induction}},
year = {2020}
}
@article{Ren2020275,
abstract = {The aim of this paper is to provide a systematic route of information retrieval from a knowledge-based database (or domain knowledge) through a dialog system of natural language interaction. The application is about a comprehensive building at a university, with classrooms, laboratory rooms, meeting rooms, research rooms and offices, and is to present related information the user asks for. First, the domain knowledge is expressed with predicate expressions based on the ontology structure; then the vocabulary is presented distributedly with word embedding enhanced with the domain knowledge; queries from the user are then converted into the intent (general) and slot elements (specific) with the help of trained recurrent neural network (RNN). The system works smoothly. The key point is integrating the two methods of knowledge-based and data-driven natural language processing into one system, and the domain knowledge is in the central part which is incorporated into the word embedding to make it specifically fit the natural language in this application. {\textcopyright} 2020 The Authors. Published by Atlantis Press SARL.},
annote = {cited By 0},
author = {Ren, J and Wang, H and Liu, T},
doi = {10.2991/ijcis.d.200310.002},
issn = {18756891},
journal = {International Journal of Computational Intelligence Systems},
keywords = {Comprehensive buildings; Domain knowledge; Knowle,Embeddings; Information retrieval; Knowledge based,Natural language processing systems},
number = {1},
pages = {275--290},
publisher = {Atlantis Press},
title = {{Information retrieval based on knowledge-enhanced word embedding through dialog: A case study}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084001110&doi=10.2991%2Fijcis.d.200310.002&partnerID=40&md5=16855c215ff3e89f4cd484ec5904088e},
volume = {13},
year = {2020}
}
@inproceedings{Ziegler2017200,
abstract = {The inferences of a machine learning algorithm are naturally limited by the available data. In many real-world applications, the provided internal data is domain-specific and we use external background knowledge to derive or add new features. Semantic networks, like linked open data, provide a largely unused treasure trove of background knowledge. This drives a recent surge of interest in unsupervised methods to automatically extract such semantic background knowledge and inject it into machine learning algorithms. In this work, we describe the general process of extracting knowledge from semantic networks through vector space embeddings. The locations in the vector space then reflect relations in the original semantic network. We perform this extraction for geographic background knowledge and inject it into a neural network for the complicated real-world task of credit-card fraud detection. This improves the performance by 11.2%. {\textcopyright} 2017 IEEE.},
annote = {cited By 6; Conference of 26th IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE 2017 ; Conference Date: 21 June 2017 Through 23 June 2017; Conference Code:129874},
author = {Ziegler, K and Caelen, O and Garchery, M and Granitzer, M and He-Guelton, L and Jurgovsky, J and Portier, P.-E. and Zwicklbauer, S},
booktitle = {Proceedings - 2017 IEEE 26th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE 2017},
doi = {10.1109/WETICE.2017.36},
editor = {{Cellary W. Fugini M.}, Reddy S},
isbn = {9781538617588},
keywords = {Artificial intelligence; Crime; Digital storage; L,Data mining,Fraud detection; Graph embeddings; Knowledge grap},
pages = {200--205},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Injecting semantic background knowledge into neural networks using graph embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034249758&doi=10.1109%2FWETICE.2017.36&partnerID=40&md5=b1209c121a1240b171055f7ed7339867},
year = {2017}
}
@article{8689107,
abstract = {A variety of brain areas is involved in language understanding and generation, accounting for the scope of language that can refer to many real-world matters. In this paper, we investigate how regularities among real-world entities impact emergent language representations. Specifically, we consider knowledge bases, which represent entities and their relations as structured triples, and image representations, which are obtained via deep convolutional networks. We combine these sources of information to learn representations of an image-based knowledge representation learning (IKRL) model. An attention mechanism lets more informative images contribute more to the image-based representations. Evaluation results show that the model outperforms all baselines on the tasks of knowledge graph (KG) completion and triple classification. In analyzing the learned models, we found that the structure-based and image-based representations integrate different aspects of the entities and the attention mechanism provides robustness during learning.},
author = {Xie, R and Heinrich, S and Liu, Z and Weber, C and Yao, Y and Wermter, S and Sun, M},
doi = {10.1109/TCDS.2019.2906685},
issn = {2379-8939},
journal = {IEEE Transactions on Cognitive and Developmental Systems},
keywords = {convolutional neural nets;image representation;kno},
month = {jun},
number = {2},
pages = {169--178},
title = {{Integrating Image-Based and Knowledge-Based Representation Learning}},
volume = {12},
year = {2020}
}
@inproceedings{Zhang20191031,
abstract = {Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including text classification. Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is therefore difficult and only limited previous works tackled this problem. In this paper, we propose a two-phase framework together with data augmentation and feature augmentation to solve this problem. Four kinds of semantic knowledge (word embeddings, class descriptions, class hierarchy, and a general knowledge graph) are incorporated into the proposed framework to deal with instances of unseen classes effectively. Experimental results show that each and the combination of the two phases achieve the best overall accuracy compared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario. {\textcopyright} 2019 Association for Computational Linguistics},
annote = {cited By 3; Conference of 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019 ; Conference Date: 2 June 2019 Through 7 June 2019; Conference Code:159851},
author = {Zhang, J and Lertvittayakumjorn, P and Guo, Y},
booktitle = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
isbn = {9781950737130},
keywords = {Character recognition; Computational linguistics;,Class hierarchies; Classification tasks; Data aug,Classification (of information)},
pages = {1031--1040},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Integrating semantic knowledge to tackle zero-shot text classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084312963&partnerID=40&md5=31fda81739e5648fc3effe72b16c09f9},
volume = {1},
year = {2019}
}
@inproceedings{Li2019940,
abstract = {Leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods. {\textcopyright} 2019 Association for Computational Linguistics},
annote = {cited By 3; Conference of 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019 ; Conference Date: 2 June 2019 Through 7 June 2019; Conference Code:159851},
author = {Li, D and Dadaneh, S Z and Zhang, J and Li, P},
booktitle = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
isbn = {9781950737130},
keywords = {Bayesian nonparametric modeling; Document Classif,Classification (of information),Computational linguistics; Embeddings; Information},
pages = {940--950},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Integration of knowledge graph embedding into topic modeling with hierarchical dirichlet process}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071153019&partnerID=40&md5=ee9ffcf1663cd95d39e3c0878e18d238},
volume = {1},
year = {2019}
}
@inproceedings{10.1109/ROBIO49542.2019.8961403,
abstract = {In order to enhance the user's service experience and help the robot to make more intimate service decisions, this paper proposes a service task cognition and decision model based on structured and unstructured data processing. Firstly, all kinds of information mentioned in user instructions are extracted through natural language processing, including object information, namely structured data and environment information, namely unstructured data. For structured data, it is mapped to the predefined ontology knowledge base to obtain its location attributes and state attributes, and then obtain service instructions. Adaptive fuzzy Petri net (AFPN) is constructed based on fuzzy rules of temperature, humidity and other environmental information. The unstructured data is taken as the input parameter of AFPN, and the service instruction deduced as the output. Then, according to the user's needs, the network weight can be continuously adjusted. If the user does not mention the environmental information, the environment information is periodically detected by the sensor, and the service instruction reasoning of the unstructured data is performed. Finally, back propagation neural network (BPNN) is introduced to combine the service inference of two kinds of data to eliminate the heterogeneity of different service instructions. Experimental results show that the model can provide different personalized services for users' preferences.},
author = {TIAN, Guohui and LI, Jie and ZHANG, Senyan and LU, Fei},
booktitle = {2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
doi = {10.1109/ROBIO49542.2019.8961403},
pages = {172--177},
publisher = {IEEE Press},
title = {{Intelligent Decision Model for Home Robot Based on Structured and Unstructured Data Processing}},
url = {https://doi.org/10.1109/ROBIO49542.2019.8961403},
year = {2019}
}
@inproceedings{10.1145/3289600.3291014,
abstract = {Knowledge graph embedding aims to learn distributed representations for entities and relations, and is proven to be effective in many applications. Crossover interactions -- bi-directional effects between entities and relations --- help select related information when predicting a new triple, but haven't been formally discussed before. In this paper, we propose CrossE, a novel knowledge graph embedding which explicitly simulates crossover interactions. It not only learns one general embedding for each entity and relation as most previous methods do, but also generates multiple triple specific embeddings for both of them, named interaction embeddings. We evaluate embeddings on typical link prediction tasks and find that CrossE achieves state-of-the-art results on complex and more challenging datasets. Furthermore, we evaluate embeddings from a new perspective -- giving explanations for predicted triples, which is important for real applications. In this work, an explanation for a triple is regarded as a reliable closed-path between the head and the tail entity. Compared to other baselines, we show experimentally that CrossE, benefiting from interaction embeddings, is more capable of generating reliable explanations to support its predictions.},
address = {New York, NY, USA},
author = {Zhang, Wen and Paudel, Bibek and Zhang, Wei and Bernstein, Abraham and Chen, Huajun},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
doi = {10.1145/3289600.3291014},
isbn = {9781450359405},
keywords = {crossover interactions,explanation,knowledge graph embedding,link prediction},
pages = {96--104},
publisher = {Association for Computing Machinery},
series = {WSDM '19},
title = {{Interaction Embeddings for Prediction and Explanation in Knowledge Graphs}},
url = {https://doi.org/10.1145/3289600.3291014},
year = {2019}
}
@article{Huang202011343,
abstract = {Answer selection which aims to select the most appropriate answers from a set of candidate answers plays a crucial role in various applications such as question answering (QA) and information retrieval. Recently, remarkable progress has been achieved on matching sequence pairs by deep neural networks. However, most of them focus on learning semantic representations for the contexts of QA pairs while the background information and facts beyond the context are neglected. In this paper, we propose an interactive knowledge-enhanced attention network for answer selection (IKAAS), which interactively learns the sentence representations of query–answer pairs by simultaneously considering the external knowledge from knowledge graphs and textual information of QA pairs. In this way, we can exploit the semantic compositionality of the input sequences and capture more comprehensive knowledge-enriched intra-document features within the question and answer. Specifically, we first propose a context-aware attentive mechanism to learn the knowledge representations guided by the corresponding context. The relations between the question and answer are then captured by computing the question–answer alignment matrix. We further employ self-attention to capture the global features of the input sequences, which are then used to calculate the relevance score of the question and answer. Experimental results on four real-life datasets demonstrate that IKAAS outperforms the compared methods. In addition, a series of analyses shows the robust superiority and the extensive applicability of the proposed method. {\textcopyright} 2020, Springer-Verlag London Ltd., part of Springer Nature.},
annote = {cited By 1},
author = {Huang, W and Qu, Q and Yang, M},
doi = {10.1007/s00521-019-04630-x},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Attention mechanisms; Background information; Ext,Deep learning; Deep neural networks; Knowledge rep,Query processing},
number = {15},
pages = {11343--11359},
publisher = {Springer},
title = {{Interactive knowledge-enhanced attention network for answer selection}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078243740&doi=10.1007%2Fs00521-019-04630-x&partnerID=40&md5=88bb7c83a5beba223aaeb92656be2a20},
volume = {32},
year = {2020}
}
@inproceedings{Zhou2020179,
abstract = {Interactive recommender system (IRS) has drawn huge attention because of its flexible recommendation strategy and the consideration of optimal long-term user experiences. To deal with the dynamic user preference and optimize accumulative utilities, researchers have introduced reinforcement learning (RL) into IRS. However, RL methods share a common issue of sample efficiency, i.e., huge amount of interaction data is required to train an effective recommendation policy, which is caused by the sparse user responses and the large action space consisting of a large number of candidate items. Moreover, it is infeasible to collect much data with explorative policies in online environments, which will probably harm user experience. In this work, we investigate the potential of leveraging knowledge graph (KG) in dealing with these issues of RL methods for IRS, which provides rich side information for recommendation decision making. Instead of learning RL policies from scratch, we make use of the prior knowledge of the item correlation learned from KG to (i) guide the candidate selection for better candidate item retrieval, (ii) enrich the representation of items and user states, and (iii) propagate user preferences among the correlated items over KG to deal with the sparsity of user feedback. Comprehensive experiments have been conducted on two real-world datasets, which demonstrate the superiority of our approach with significant improvements against state-of-the-arts. {\textcopyright} 2020 ACM.},
annote = {cited By 0; Conference of 43rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2020 ; Conference Date: 25 July 2020 Through 30 July 2020; Conference Code:161956},
author = {Zhou, S and Dai, X and Chen, H and Zhang, W and Ren, K and Tang, R and He, X and Yu, Y},
booktitle = {SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3397271.3401174},
isbn = {9781450380164},
keywords = {Amount of interaction; Candidate selection; Long-,Arts computing; Decision making; Knowledge managem,Recommender systems},
pages = {179--188},
publisher = {Association for Computing Machinery, Inc},
title = {{Interactive Recommender System via Knowledge Graph-enhanced Reinforcement Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090141776&doi=10.1145%2F3397271.3401174&partnerID=40&md5=b34102b09364f63a547e562a48e9415f},
year = {2020}
}
@article{10.1016/j.jbi.2013.06.012,
abstract = {Graphical abstractsDisplay Omitted This study describes a data mining algorithm for the discovery of new gene ontology relationships.This study presents new interestingness metrics for multi-ontology multi-level relationships.Cross-ontology relationships offer GO co-annotation candidates and cross-ontology connections.Our results indicate that our methods outperform QuickGO in generating co-annotation suggestions. The Gene Ontology (GO), a set of three sub-ontologies, is one of the most popular bio-ontologies used for describing gene product characteristics. GO annotation data containing terms from multiple sub-ontologies and at different levels in the ontologies is an important source of implicit relationships between terms from the three sub-ontologies. Data mining techniques such as association rule mining that are tailored to mine from multiple ontologies at multiple levels of abstraction are required for effective knowledge discovery from GO annotation data. We present a data mining approach, Multi-ontology data mining at All Levels (MOAL) that uses the structure and relationships of the GO to mine multi-ontology multi-level association rules. We introduce two interestingness measures: Multi-ontology Support (MOSupport) and Multi-ontology Confidence (MOConfidence) customized to evaluate multi-ontology multi-level association rules. We also describe a variety of post-processing strategies for pruning uninteresting rules. We use publicly available GO annotation data to demonstrate our methods with respect to two applications (1) the discovery of co-annotation suggestions and (2) the discovery of new cross-ontology relationships.},
address = {San Diego, CA, USA},
author = {Manda, Prashanti and McCarthy, Fiona and Bridges, Susan M},
doi = {10.1016/j.jbi.2013.06.012},
issn = {1532-0464},
journal = {J. of Biomedical Informatics},
keywords = {Association rule mining,Data mining,Gene ontology,Gene ontology relationships,Interestingness measures,Interpro relationships},
month = {oct},
number = {5},
pages = {849--856},
publisher = {Elsevier Science},
title = {{Interestingness Measures and Strategies for Mining Multi-Ontology Multi-Level Association Rules from Gene Ontology Annotations for the Discovery of New GO Relationships}},
url = {https://doi.org/10.1016/j.jbi.2013.06.012},
volume = {46},
year = {2013}
}
@article{Chen2020845,
abstract = {Cross-lingual knowledge alignment is the cornerstone in building a comprehensive knowledge graph (KG), which can benefit various knowledge-driven applications. As the structures of KGs are usually sparse, attributes of entities may play an important role in aligning the entities. However, the heterogeneity of the attributes across KGs prevents from accurately embedding and comparing entities. To deal with the issue, we propose to model the interactions between attributes, instead of globally embedding an entity with all the attributes. We further propose a joint framework to merge the alignments inferred from the attributes and the structures. Experimental results show that the proposed model outperforms the state-of-art baselines by up to 38.48% HitRatio@1. The results also demonstrate that our model can infer the alignments between attributes, relationships and values, in addition to entities. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2020 ; Conference Date: 11 May 2020 Through 14 May 2020; Conference Code:240129},
author = {Chen, B and Zhang, J and Tang, X and Chen, H and Li, C},
doi = {10.1007/978-3-030-47426-3_65},
editor = {{Lauw H.W. Lim E.-P.}, Wong R.C.-W. Ntoulas A Ng S.-K. Pan S J},
isbn = {9783030474256},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Alignment; Embeddings,Attribute interactions; Cross-lingual; In-buildin,Data mining},
pages = {845--856},
publisher = {Springer},
title = {{JarKA: Modeling Attribute Interactions for Cross-lingual Knowledge Alignment}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085736308&doi=10.1007%2F978-3-030-47426-3_65&partnerID=40&md5=faeb09bfdba2740c2d1ec80c7e604bad},
volume = {12084 LNAI},
year = {2020}
}
@inproceedings{9204062,
abstract = {Extracting entities and relations from unstructured text is the key to building a large-scale knowledge graph. In recent years, the relation extraction approaches based on the neural network has achieved good results. However, the existing methods cannot accurately extract overlapping entities (i.e., one entity is shared by multiple relations). In this paper, we propose a simple Electra-based joint model for relation extraction. We use the subject extractor to identify the subject first, and then use the predicate-object extractor to predict its corresponding predicates and objects based on the encoded representation vector of the detected subject. Experiments on DuIE2.0 show good performance on the extraction of Chinese dataset with overlapping triples.},
author = {Zhu, M and Xue, J and Zhou, G},
booktitle = {2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)},
doi = {10.1109/IHMSC49165.2020.10119},
keywords = {feature extraction;graph theory;information retrie},
month = {aug},
pages = {179--183},
title = {{Joint Extraction of Entity and Relation Based on Pre-trained Language Model}},
volume = {2},
year = {2020}
}
@article{Ji20201362,
abstract = {The joint representations of knowledge graph have become an important approach to improve the quality of knowledge graph, which is beneficial to machine learning, data mining, and artificial intelligence applications. However, the previous work suffers severely from the noise in text when modeling the text information. To overcome this problem, this paper mines the high-quality reference sentences of the entities in the knowledge graph, to enhance the representation ability of the entities. A novel framework for joint representation learning of knowledge graphs and text information based on reference sentence noise-reduction is proposed, which embeds the entity, the relations, and the words into a unified vector space. The proposed framework consists of knowledge graph representation learning module, textual relation representation learning module, and textual entity representation learning module. Experiments on entity prediction, relation prediction, and triple classification tasks are conducted, results show that the proposed framework can significantly improve the performance of mining and fusing the text information. Especially, compared with the state-of-the-art method [15], the proposed framework improves the metric of H@10 by 5.08% and 3.93% in entity prediction task and relation prediction task, respectively, and improves the metric of accuracy by 5.08% in triple classification task. Copyright {\textcopyright} 2020 The Institute of Electronics, Information and Communication Engineers},
annote = {cited By 0},
author = {Ji, Z and Lei, Z and Shen, T and Zhang, J},
doi = {10.1587/transinf.2019EDP7229},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Artificial intelligence; Classification (of inform,Classification tasks; High quality; Knowledge gra,Text mining},
number = {6},
pages = {1362--1370},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{Joint representations of knowledge graphs and textual information via reference sentences}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086140362&doi=10.1587%2Ftransinf.2019EDP7229&partnerID=40&md5=4bc5aab0dc8fefb17aea545447e675b6},
volume = {103},
year = {2020}
}
@inproceedings{Guo2016192,
abstract = {Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest. Most existing methods perform the embedding task using only fact triples. Logical rules, although containing rich background information, have not been well studied in this task. This paper proposes a novel method of jointly embedding knowledge graphs and logical rules. The key idea is to represent and model triples and rules in a unified framework. Specifically, triples are represented as atomic formulae and modeled by the translation assumption, while rules represented as complex formulae and modeled by t-norm fuzzy logics. Embedding then amounts to minimizing a global loss over both atomic and complex formulae. In this manner, we learn embeddings compatible not only with triples but also with rules, which will certainly be more predictive for knowledge acquisition and inference. We evaluate our method with link prediction and triple classification tasks. Experimental results show that joint embedding brings significant and consistent improvements over state-of-the-art methods. Particularly, it enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of our method to learn more predictive embeddings. {\textcopyright} 2016 Association for Computational Linguistics},
annote = {cited By 47; Conference of 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016 ; Conference Date: 1 November 2016 Through 5 November 2016; Conference Code:150070},
author = {Guo, S and Wang, Q and Wang, L and Wang, B and Guo, L},
booktitle = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
doi = {10.18653/v1/d16-1019},
isbn = {9781945626258},
keywords = {Atomic formulae; Background information; Classifi,Embeddings; Natural language processing systems; V,Fuzzy logic},
pages = {192--202},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Jointly embedding knowledge graphs and logical rules}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072828535&doi=10.18653%2Fv1%2Fd16-1019&partnerID=40&md5=63263c0bc3796d02bba30502304518a5},
year = {2016}
}
@article{Alsuhaibani2018,
abstract = {Methods for representing the meaning of words in vector spaces purely using the information distributed in text corpora have proved to be very valuable in various text mining and natural language processing (NLP) tasks. However, these methods still disregard the valuable semantic relational structure between words in co-occurring contexts. These beneficial semantic relational structures are contained in manually-created knowledge bases (KBs) such as ontologies and semantic lexicons, where the meanings of words are represented by defining the various relationships that exist among those words. We combine the knowledge in both a corpus and a KB to learn better word embeddings. Specifically, we propose a joint word representation learning method that uses the knowledge in the KBs, and simultaneously predicts the co-occurrences of two words in a corpus context. In particular, we use the corpus to define our objective function subject to the relational constrains derived from the KB. We further utilise the corpus co-occurrence statistics to propose two novel approaches, Nearest Neighbour Expansion (NNE) and Hedged Nearest Neighbour Expansion (HNE), that dynamically expand the KB and therefore derive more constraints that guide the optimisation process. Our experimental results over a wide-range of benchmark tasks demonstrate that the proposed method statistically significantly improves the accuracy of the word embeddings learnt. It outperforms a corpus-only baseline and reports an improvement of a number of previously proposed methods that incorporate corpora and KBs in both semantic similarity prediction and word analogy detection tasks. {\textcopyright} 2018 Alsuhaibani et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
annote = {cited By 7},
author = {Alsuhaibani, M and Bollegala, D and Maehara, T and Kawarabayashi, K.-I.},
doi = {10.1371/journal.pone.0193094},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Algorithms; Data Mining; Humans; Knowledge Bases;,article; embedding; human; human experiment; joint},
number = {3},
publisher = {Public Library of Science},
title = {{Jointly learning word embeddings using a corpus and a knowledge base}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043769311&doi=10.1371%2Fjournal.pone.0193094&partnerID=40&md5=6cec638b3412d089c39a79930137a40c},
volume = {13},
year = {2018}
}
@inproceedings{10.1145/3397271.3401040,
abstract = {Knowledge graph (KG) contains well-structured external information and has shown to be effective for high-quality recommendation. However, existing KG enhanced recommendation methods have largely focused on exploring advanced neural network architectures to better investigate the structural information of KG. While for model learning, these methods mainly rely on Negative Sampling (NS) to optimize the models for both KG embedding task and recommendation task. Since NS is not robust (e.g., sampling a small fraction of negative instances may lose lots of useful information), it is reasonable to argue that these methods are insufficient to capture collaborative information among users, items, and entities.In this paper, we propose a novel Jointly Non-Sampling learning model for Knowledge graph enhanced Recommendation (JNSKR). Specifically, we first design a new efficient NS optimization algorithm for knowledge graph embedding learning. The subgraphs are then encoded by the proposed attentive neural network to better characterize user preference over items. Through novel designs of memorization strategies and joint learning framework, JNSKR not only models the fine-grained connections among users, items, and entities, but also efficiently learns model parameters from the whole training data (including all non-observed data) with a rather low time complexity. Experimental results on two public benchmarks show that JNSKR significantly outperforms the state-of-the-art methods like RippleNet and KGAT. Remarkably, JNSKR also shows significant advantages in training efficiency (about 20 times faster than KGAT), which makes it more applicable to real-world large-scale systems.},
address = {New York, NY, USA},
author = {Chen, Chong and Zhang, Min and Ma, Weizhi and Liu, Yiqun and Ma, Shaoping},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3397271.3401040},
isbn = {9781450380164},
keywords = {efficient,implicit feedback,knowledge graph,non-sampling learning,recommender systems},
pages = {189--198},
publisher = {Association for Computing Machinery},
series = {SIGIR '20},
title = {{Jointly Non-Sampling Learning for Knowledge Graph Enhanced Recommendation}},
url = {https://doi.org/10.1145/3397271.3401040},
year = {2020}
}
@inproceedings{Ma2018743,
abstract = {The goal of diagnosis prediction task is to predict the future health information of patients from their historical Electronic Healthcare Records (EHR). The most important and challenging problem of diagnosis prediction is to design an accurate, robust and interpretable predictive model. Existing work solves this problem by employing recurrent neural networks (RNNs) with attention mechanisms, but these approaches suffer from the data sufficiency problem. To obtain good performance with insufficient data, graph-based attention models are proposed. However, when the training data are sufficient, they do not offer any improvement in performance compared with ordinary attention-based models. To address these issues, we propose KAME, an end-to-end, accurate and robust model for predicting patients' future health information. KAME not only learns reasonable embeddings for nodes in the knowledge graph, but also exploits general knowledge to improve the prediction accuracy with the proposed knowledge attention mechanism. With the learned attention weights, KAME allows us to interpret the importance of each piece of knowledge in the graph. Experimental results on three real world datasets show that the proposed KAME significantly improves the prediction performance compared with the state-of-the-art approaches, guarantees the robustness with both sufficient and insufficient data, and learns interpretable disease representations. {\textcopyright} 2018 Association for Computing Machinery.},
annote = {cited By 25; Conference of 27th ACM International Conference on Information and Knowledge Management, CIKM 2018 ; Conference Date: 22 October 2018 Through 26 October 2018; Conference Code:142310},
author = {Ma, F and Chitta, R and You, Q and Zhou, J and Xiao, H and Gao, J},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3269206.3271701},
editor = {{Paton N. Candan S.}, Wang H Allan J Agrawal R Labrinidis A Cuzzocrea A Zaki M Srivastava D Broder A Schuster A},
isbn = {9781450360142},
keywords = {Attention mechanisms; Electronic healthcare recor,Diagnosis,Forecasting; Graphic methods; Health care; Knowled},
pages = {743--752},
publisher = {Association for Computing Machinery},
title = {{KAME: Knowledge-based attention model for diagnosis prediction in healthcare}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058017391&doi=10.1145%2F3269206.3271701&partnerID=40&md5=fad319ae4290731f541c4e927ad64d8f},
year = {2018}
}
@article{Wang2020493,
abstract = {The goal of sequential recommendations is to capture the transitions of users' interests. Most existing methods utilize sequential neural networks to model interaction records, mapping items into latent vectors. Although such methods do explore the transitions of items in interaction sequences, they only capture the sequence dependencies of items, neglecting the deep semantic relevance between items. Such limited information contributes less to catching the complicated sequential behaviors of users accurately. In this paper, we propose a novel model Knowledge-Aware Sequential Recommendation (KASR), which captures sequence dependencies and semantic relevance of items simultaneously in an end-to-end manner. Specifically, we first convert the interaction records into a knowledge-transfer interaction sequence, which reflects the fine-grained transitions of users' interests. Next, we further recursively aggregate information in the knowledge graph based on a specific relation attention network, to explicitly capture the high-order relevance between items. A knowledge-aware GRU is later introduced to explore the sequential and semantic relevance between items automatically. We have conducted extensive experiments on three real datasets, and the results demonstrate that our method outperforms the state-of-the-art models. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 4th Asia-Pacific Web and Web-Age Information Management, Joint Conference on Web and Big Data, APWeb-WAIM 2020 ; Conference Date: 18 September 2020 Through 20 September 2020; Conference Code:250159},
author = {Wang, Q and Xiong, Y and Zhu, Y and Yu, P S},
doi = {10.1007/978-3-030-60259-8_36},
editor = {{Wang X. Zhang R.}, Lee Y.-K. Sun L Moon Y.-S.},
isbn = {9783030602581},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Big data; Graphic methods; Knowledge management; K,Knowledge graphs; Knowledge transfer; Limited inf,Semantics},
pages = {493--508},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{KASR: Knowledge-Aware Sequential Recommendation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093935051&doi=10.1007%2F978-3-030-60259-8_36&partnerID=40&md5=ec5ac5ba7fef03d84012269a5a2452f4},
volume = {12317 LNCS},
year = {2020}
}
@inproceedings{9182385,
abstract = {In recent years, with the rapid development of location-based social networks (LBSN) in the Internet, point of interest (POI) recommendation has become a hot spot. Most existing researches make use of contextual information to model users' interest preferences. However, the existing methods for extracting various auxiliary information still need to be improved, such as how to treat the users' social relations equally. In order to obtain users' actual preferences more accurately, in POI recommendation, we propose a deep learning framework KEAN (Knowledge Embedded and Attention Based Network) based on knowledge graph and attention model. The framework includes knowledge-graph embedding method, preference extraction network based on attention mechanism and recommendation network. Our study used knowledge-graph embedding method to get the embedding of each user and POI. In addition, an LSTM network based on attention mechanism was proposed, which uses LSTM network to learn the user's preferences according to the user's check-in sequence. Besides, the attention mechanism was used to extract friends' preferences and merge them with the user's preferences to generate end-user preferences. Finally, our model use fully-connected neural networks to realize recommendations. The effectiveness of the model was proved by the experimental results based on real LBSN datasets.},
author = {Zhang, C and Li, T and Gou, Y and Yang, M},
booktitle = {2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)},
doi = {10.1109/ICAICA50127.2020.9182385},
keywords = {graph theory;learning (artificial intelligence);lo},
month = {jun},
pages = {847--852},
title = {{KEAN: Knowledge Embedded and Attention-based Network for POI Recommendation}},
year = {2020}
}
@inproceedings{10.1145/3292500.3330989,
abstract = {To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism. We release the codes and datasets at https://github.com/xiangwang1223/knowledge_graph_attention_network.},
address = {New York, NY, USA},
author = {Wang, Xiang and He, Xiangnan and Cao, Yixin and Liu, Meng and Chua, Tat-Seng},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3292500.3330989},
isbn = {9781450362016},
keywords = {collaborative filtering,embedding propagation,graph neural network,higher-order connectivity,knowledge graph,recommendation},
pages = {950--958},
publisher = {Association for Computing Machinery},
series = {KDD '19},
title = {{KGAT: Knowledge Graph Attention Network for Recommendation}},
url = {https://doi.org/10.1145/3292500.3330989},
year = {2019}
}
@article{8362657,
abstract = {Motivated by the vast applications of knowledge graph and the increasing demand in education domain, we propose a system, called KnowEdu, to automatically construct knowledge graph for education. By leveraging on heterogeneous data (e.g., pedagogical data and learning assessment data) from the education domain, this system first extracts the concepts of subjects or courses and then identifies the educational relations between the concepts. More specifically, it adopts the neural sequence labeling algorithm on pedagogical data to extract instructional concepts and employs probabilistic association rule mining on learning assessment data to identify the relations with educational significance. We detail all the above mentioned efforts through an exemplary case of constructing a demonstrative knowledge graph for mathematics, where the instructional concepts and their prerequisite relations are derived from curriculum standards and concept-based performance data of students. Evaluation results show that the F1 score for concept extraction exceeds 0.70, and for relation identification, the area under the curve and mean average precision achieve 0.95 and 0.87, respectively.},
author = {Chen, P and Lu, Y and Zheng, V W and Chen, X and Yang, B},
doi = {10.1109/ACCESS.2018.2839607},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {computer aided instruction;data mining;educational},
pages = {31553--31563},
title = {{KnowEdu: A System to Construct Knowledge Graph for Education}},
volume = {6},
year = {2018}
}
@article{He20181075,
abstract = {Knowledge base completion is an important research problem in knowledge bases, which play important roles in question answering, information retrieval, and other applications. A number of relational learning algorithms have been proposed to solve this problem. However, despite their success in modeling the entity relations, they are not well founded in a Bayesian manner and thus are hard to model the prior information of the entity and relation factors. Furthermore, they under-represent the interaction between entity and relation factors. In order to avoid these disadvantages, we provide a neural-inspired approach, namely Bayesian Neural Tensor Decomposition approach for knowledge base completion based on the Stochastic Gradient Variational Bayesian framework. We employ a multivariate Bernoulli likelihood function to represent the existence of facts in knowledge graphs. We further employ a Multi-layered Perceptrons to represent more complex interactions between the latent subject, predicate, and object factors. The SGVB framework can enable us to make efficient approximate variational inference for the proposed nonlinear probabilistic tensor decomposition by a novel local reparameterization trick. This way avoids the need of expensive iterative inference schemes such as MCMC and does not make any over-simplified assumptions about the posterior distributions, in contrary to the common variational inference. In order to evaluate the proposed model, we have conducted experiments on real-world knowledge bases, i.e., FreeBase and WordNet. Experimental results have indicated the promising performance of the proposed method. {\textcopyright} 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
annote = {cited By 7},
author = {He, L and Liu, B and Li, G and Sheng, Y and Wang, Y and Xu, Z},
doi = {10.1007/s12559-018-9565-x},
issn = {18669956},
journal = {Cognitive Computation},
keywords = {Bayesian networks,Iterative methods; Knowledge based systems; Neural,Knowledge base; Likelihood functions; Multilayere},
number = {6},
pages = {1075--1084},
publisher = {Springer New York LLC},
title = {{Knowledge Base Completion by Variational Bayesian Neural Tensor Decomposition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049105657&doi=10.1007%2Fs12559-018-9565-x&partnerID=40&md5=f8a0e197678af15698a74d62d7fa5ea2},
volume = {10},
year = {2018}
}
@inproceedings{9194489,
abstract = {Topic models, such as Latent Dirichlet Allocation (LDA), are successful in learning hidden topics and has been widely applied in text mining. There are many recently developed augmented topic modeling methods to utilize metadata information. However, the effect of topic models is still not comparable to humans. We think one key point is that humans have background knowledge, which is essential for topic understanding. Inspired by this, we propose a knowledge base enhanced topic model in this paper. We take knowledge bases as good presentations of human knowledge, with huge collections of entities and their relations. We assume that documents with related entities tend to have similar topic distributions. Based on this assumption, we compute document similarity information via the linked entities and then use it as a constraint for LDA. More specifically, we embed entities in a low-dimensional space via DeepWalk and use Entity Movers Distance to efficiently and effectively measure the similarities between documents. The results of experiments over two real-world datasets show that our method boosts the LDA model on the document classification while no supervision information is needed.},
author = {Song, D and Gao, J and Pang, J and Liao, L and Qin, L},
booktitle = {2020 IEEE International Conference on Knowledge Graph (ICKG)},
doi = {10.1109/ICBK50248.2020.00061},
keywords = {data mining;document handling;learning (artificial},
month = {aug},
pages = {380--387},
title = {{Knowledge Base Enhanced Topic Modeling}},
year = {2020}
}
@inproceedings{Wang2020170,
abstract = {Knowledge Base Question Answering (KBQA) refers that questions are answered by acquiring the relationship or entity from knowledge graph. The knowledge base is being high frequent used in modern question answering systems, which can find the exact answer from the knowledge base and return it to the users. However, due to the flexibility, richness and fuzziness of natural language, it is not easy to match the semantic information of questions with the text answers. How to accurately match these natural language questions with a large number of knowledge graph and improve the accuracy of questions and answers is an urgent problem to be solved. In this paper, we propose a question answering algorithm named TransE-QA based on knowledge graph representation learning to solve Simple QA problem. This algorithm is an end-to-end algorithm for question answering of simple QA problem. TransE and TextCNN are mainly used to represent the knowledge graph and the question. Based on knowledge graph representation learning, we propose a new scoring function, which the answer can be returned directly that required by the question. Besides, based on the TransE-QA algorithm proposed in this paper, we develop a KBQA system to visualize the process. The experiment shows that the algorithm TransE-QA, which proposed in the thesis, achieves 80.2% accuracy on FB5M dataset. It achieves better performance than previous state-of-the-art methods. {\textcopyright} 2020 ACM.},
annote = {cited By 0; Conference of 4th International Conference on Innovation in Artificial Intelligence, ICIAI 2020 ; Conference Date: 8 May 2020 Through 11 May 2020; Conference Code:160591},
author = {Wang, Y and Chen, Q and He, C and Liu, H and Wu, X},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3390557.3394296},
isbn = {9781450376587},
keywords = {Artificial intelligence; Graph structures; Knowled,Graph algorithms,Knowledge graphs; Natural language questions; Nat},
pages = {170--179},
publisher = {Association for Computing Machinery},
title = {{Knowledge Base Question Answering System Based on Knowledge Graph Representation Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086498876&doi=10.1145%2F3390557.3394296&partnerID=40&md5=c6f4da8eb5225f9e7119c9d69472e9a0},
year = {2020}
}
@article{Ye2020294,
abstract = {Mining opinion is essential for consistency and persona of a chatbot. However, mining existing opinions suffers from data sparsity. Toward a given entity, we cannot always find a proper sentence that expresses desired sentiment. In this paper, we propose to generate opinion sentences for a given attitude, i.e., an entity and sentiment polarity pair. We extract attributes of a target entity from a knowledge base and specific keywords from its description. The attributes and keywords are integrated with knowledge graph embeddings, and fed into an encoder-decoder generation framework. We also propose an auxiliary task that predicts attributes using the generated sentences, aiming to avoid common opinions. Experimental results indicate that our approach significantly outperforms baselines in automatic and human evaluation. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 9th CCF International Conference on Natural Language Processing and Chinese Computing, NLPCC 2020 ; Conference Date: 14 October 2020 Through 18 October 2020; Conference Code:249929},
author = {Ye, Z and Song, R and Fu, H and Lin, P and Nie, J.-Y. and Li, F},
doi = {10.1007/978-3-030-60450-9_24},
editor = {{Zhu X. Zhang M.}, Hong Y He R},
isbn = {9783030604493},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Chatbot; Data sparsity; Encoder-decoder; Human ev,Knowledge based systems; Knowledge representation,Natural language processing systems},
pages = {294--305},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Knowledge Enhanced Opinion Generation from an Attitude}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093094874&doi=10.1007%2F978-3-030-60450-9_24&partnerID=40&md5=98dc1bcdd4b891342629e6f0cbf8a1bf},
volume = {12430 LNAI},
year = {2020}
}
@inproceedings{Wang20141591,
abstract = {We examine the embedding approach to reason new relational facts from a largescale knowledge graph and a text corpus. We propose a novel method of jointly embedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be handled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec (Skip-Gram). {\textcopyright} 2014 Association for Computational Linguistics.},
annote = {cited By 196; Conference of 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014 ; Conference Date: 25 October 2014 Through 29 October 2014; Conference Code:111414},
author = {Wang, Z and Zhang, J and Feng, J and Chen, Z},
booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/d14-1167},
isbn = {9781937284961},
keywords = {Analogical reasoning; Embedding method; Embedding,Embeddings,Linguistics; Natural language processing systems;},
pages = {1591--1601},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Knowledge graph and text jointly embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926065966&doi=10.3115%2Fv1%2Fd14-1167&partnerID=40&md5=eb21d9b18f85533cd6ff04cbef47a079},
year = {2014}
}
@inproceedings{10.1145/3300115.3309531,
abstract = {Hands-on practice is a critical component of cybersecurity education. Most of the existing hands-on exercises or labs materials are usually managed in a problem-centric fashion, while it lacks a coherent way to manage existing labs and provide productive lab exercising plans for cybersecurity learners. With the advantages of big data and natural language processing (NLP) technologies, constructing a large knowledge graph and mining concepts from unstructured text becomes possible, which motivated us to construct a machine learning based lab exercising plan for cybersecurity education. In the research presented by this paper, we have constructed a knowledge graph in the cybersecurity domain using NLP technologies including machine learning based word embedding and hyperlink-based concept mining. We then utilized the knowledge graph during the regular learning process based on the following approaches: 1. We constructed a web-based front-end to visualize the knowledge graph, which allows students to browse and search cybersecurity-related concepts and the corresponding interdependence relations; 2. We created a personalized knowledge graph for each student based on their learning progress and status; 3. We built a personalized lab recommendation system by suggesting more relevant labs based on students' past learning history to maximize their learning outcomes. To measure the effectiveness of the proposed solution, we have conducted a use case study and collected survey data from a graduate-level cybersecurity class. Our study shows that, by leveraging the knowledge graph for the cybersecurity area study, students tend to benefit more and show more interests in cybersecurity area.},
address = {New York, NY, USA},
author = {Deng, Yuli and Lu, Duo and Huang, Dijiang and Chung, Chun-Jen and Lin, Fanjie},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
doi = {10.1145/3300115.3309531},
isbn = {9781450362597},
keywords = {cybersecurity,knowledge graph,laboratory},
pages = {194--200},
publisher = {Association for Computing Machinery},
series = {CompEd '19},
title = {{Knowledge Graph Based Learning Guidance for Cybersecurity Hands-on Labs}},
url = {https://doi.org/10.1145/3300115.3309531},
year = {2019}
}
@inproceedings{10.1145/3269206.3271769,
abstract = {The main focus of relational learning for knowledge graph completion (KGC) lies in exploiting rich contextual information for facts. Many state-of-the-art models incorporate fact sequences, entity types, and even textual information. Unfortunately, most of them do not fully take advantage of rich structural information in a KG, i.e., connectivity patterns around each entity. In this paper, we propose a context-aware convolutional learning (CACL) model which jointly learns from entities and their multi-hop neighborhoods. Since we directly utilize the connectivity patterns contained in each multi-hop neighborhood, the structural role similarity among entities can be better captured, resulting in more informative entity and relation embeddings. Specifically, CACL collects entities and relations from the multi-hop neighborhood as contextual information according to their relative importance and uniquely maps them to a linear vector space. Our convolutional architecture leverages a deep learning technique to represent each entity along with its linearly mapped contextual information. Thus, we can elaborately extract the features of key connectivity patterns from the context and incorporate them into a score function which evaluates the validity of facts. Experimental results on the newest datasets show that CACL outperforms existing approaches by successfully enriching embeddings with neighborhood information.},
address = {New York, NY, USA},
author = {Oh, Byungkook and Seo, Seungmin and Lee, Kyong-Ho},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3269206.3271769},
isbn = {9781450360142},
keywords = {deep learning,graph embeddings,joint modeling,knowledge graphs,link prediction},
pages = {257--266},
publisher = {Association for Computing Machinery},
series = {CIKM '18},
title = {{Knowledge Graph Completion by Context-Aware Convolutional Learning with Multi-Hop Neighborhoods}},
url = {https://doi.org/10.1145/3269206.3271769},
year = {2018}
}
@article{Xie2020,
abstract = {Microblogging is a popular social networking tool on which people tend to express their views and opinions. As such, the massive data on microblogging platforms mean abundant research value to social science researchers. To help them better analyze these data, a framework for understanding diverse user opinions and identifying complex relationships in the form of knowledge graphs is proposed in this paper. The two main tasks in the framework are sentiment analysis and knowledge graph construction. In the first task, the Skip-gram model is employed to obtain the word embedding matrix and the Bi-LSTM model is adopted to perform stance classification. It is found in this paper that Bi-LSTM showed better performance in classifying different sentiments, compared with Naive Bayes and SnowNLP. In the second task, relations between different users are extracted from their micro-blogs through recognizing specific strings, and on this basis user attitudes are integrated into the knowledge extracted. A knowledge graph of user opinions is constructed with the Neo4J tool. With the knowledge extracted by this framework, social science researchers can more easily observe rules of perspective communication and perform further analysis of the data.},
author = {Xie, Tingyu and Yang, Yichun and Li, Qi and Liu, Xiaoying and Wang, Hongwei},
doi = {10.1007/978-3-030-34986-8_17},
issn = {23674520},
journal = {Lecture Notes on Data Engineering and Communications Technologies},
keywords = {Knowledge graph,Natural language processing,Sentiment analysis,User opinion},
pages = {236--247},
title = {{Knowledge Graph Construction for Intelligent Analysis of Social Networking User Opinion}},
volume = {41},
year = {2020}
}
@article{8778709,
abstract = {Knowledge representation is one of the critical problems in knowledge engineering and artificial intelligence, while knowledge embedding as a knowledge representation methodology indicates entities and relations in knowledge graph as low-dimensional, continuous vectors. In this way, knowledge graph is compatible with machine learning models. Major knowledge embedding methods employ geometric translation to design score function, which is weak-semantic for natural language processing. To overcome this disadvantage, in this paper, we propose our model based on multi-view clustering framework, which could generate semantic representations of knowledge elements (i.e. entities/relations). With our semantic model, we also present an empowered solution to entity retrieval with entity description. Extensive experiments show that our model achieves substantial improvements against baselines on the task of knowledge graph completion, triple classification, entity classification and entity retrieval.},
author = {Xiao, H and Chen, Y and Shi, X},
doi = {10.1109/TKDE.2019.2931548},
issn = {1558-2191},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Semantics;Numerical models;Knowledge representatio},
pages = {1},
title = {{Knowledge Graph Embedding based on Multi-View Clustering Framework}},
year = {2019}
}
@inproceedings{10.1145/3289600.3290956,
abstract = {Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.},
address = {New York, NY, USA},
author = {Huang, Xiao and Zhang, Jingyuan and Li, Dingcheng and Li, Ping},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
doi = {10.1145/3289600.3290956},
isbn = {9781450359405},
keywords = {deep learning,knowledge graph embedding,question answering},
pages = {105--113},
publisher = {Association for Computing Machinery},
series = {WSDM '19},
title = {{Knowledge Graph Embedding Based Question Answering}},
url = {https://doi.org/10.1145/3289600.3290956},
year = {2019}
}
@article{Rahman2018114,
abstract = {Knowledge graph (KG) is the most popular method for presenting knowledge in search engines and other natural-language processing (NLP) applications. However, KG remains incomplete, inconsistent, and not completely accurate. To deal with the challenges of KGs, many state-of-the-art models, such as TransE, TransH, and TransR, have been proposed. TransE and TransH use one semantic space for entities and relations, whereas TransR uses two different semantic spaces in its embedding model. An issue is that these proposed models ignore the category-specific projection of entities. For example, the entity “Washington” could belong to the person or location category depending on its context or relationships. An entity may therefore involve multiple types or aspects. Considering all entities in just one semantic space is therefore not a logical approach to building an effective model. In this paper, we propose TransET, which maps each entity based on its type. We can then apply any other existing translation-distance-based embedding models such as TransE or TransR. We evaluated our model using two tasks that involve link prediction and triple classification. Our model achieved a significant and consistent improvement over other state-of-the-art models. {\textcopyright} Springer Nature Switzerland AG 2018.},
annote = {cited By 5; Conference of 25th International Conference on Neural Information Processing, ICONIP 2018 ; Conference Date: 13 December 2018 Through 16 December 2018; Conference Code:221699},
author = {Rahman, M M and Takasu, A},
doi = {10.1007/978-3-030-04182-3_11},
editor = {{Cheng L. Ozawa S.}, Leung A C S},
isbn = {9783030041816},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Category specifics; Distance-based; Knowledge gra,Natural language processing systems,Search engines; Semantics},
pages = {114--125},
publisher = {Springer Verlag},
title = {{Knowledge graph embedding via entities' type mapping matrix}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059025607&doi=10.1007%2F978-3-030-04182-3_11&partnerID=40&md5=cd2802855ea456dafb705aaf13a583be},
volume = {11303 LNCS},
year = {2018}
}
@article{Xiong2018106,
abstract = {Knowledge graph embedding aims to embed both entities and relations into a low-dimensional space. Most existing methods of representation learning consider direct relations and some of them consider multiple-step relation paths. Although those methods achieve state-of-the-art performance, they are far from complete. In this paper, a noval path-augmented TransD (PTransD) model is proposed to improve the accuracy of knowledge graph embedding. This model uses two vectors to represent entities and relations. One of them represents the meaning of a(n) entity (relation), the other one is used to construct the dynamic mapping matrix. The PTransD model considers relation paths as translation between entities for representation learning. Experimental results on public dataset show that PTransD achieves significant and consistent improvements on knowledge graph completion. {\textcopyright} 2018, Springer Nature Switzerland AG.},
annote = {cited By 4; Conference of 37th International Conference on Conceptual Modeling, ER 2018 Workshops Emp-ER, MoBiD, MREBA, QMMQ, SCME ; Conference Date: 22 October 2018 Through 25 October 2018; Conference Code:219719},
author = {Xiong, S and Huang, W and Duan, P},
doi = {10.1007/978-3-030-01391-2_18},
editor = {{Li Z. Ling T.W.}, Li G Lu J Woo C Lee M L},
isbn = {9783030013905},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining; Mapping,Dynamic mapping; Knowledge graphs; Low-dimensiona,Graph theory},
pages = {106--118},
publisher = {Springer Verlag},
title = {{Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055441219&doi=10.1007%2F978-3-030-01391-2_18&partnerID=40&md5=c43be566d6a304575b991d38024d124d},
volume = {11158 LNCS},
year = {2018}
}
@inproceedings{Zhang20203198,
abstract = {The rapid development of knowledge graphs (KGs), such as Freebase and WordNet, has changed the paradigm for AI-related applications. However, even though these KGs are impressively large, most of them are suffering from incompleteness, which leads to performance degradation of AI applications. Most existing researches are focusing on knowledge graph embedding (KGE) models. Nevertheless, those models simply embed entities and relations into latent vectors without leveraging the rich information from the relation structure. Indeed, relations in KGs conform to a three-layer hierarchical relation structure (HRS), i.e., semantically similar relations can make up relation clusters and some relations can be further split into several fine-grained sub-relations. Relation clusters, relations and sub-relations can fit in the top, the middle and the bottom layer of three-layer HRS respectively. To this end, in this paper, we extend existing KGE models TransE, TransH and DistMult, to learn knowledge representations by leveraging the information from the HRS. Particularly, our approach is capable to extend other KGE models. Finally, the experiment results clearly validate the effectiveness of the proposed approach against baselines. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 5; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Zhang, Z and Zhuang, F and Qu, M and Lin, F and He, Q},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {AI applications; Bottom layers; Fine grained; Hie,Embeddings; Knowledge management; Knowledge repres,Graph structures},
pages = {3198--3207},
publisher = {Association for Computational Linguistics},
title = {{Knowledge graph embedding with hierarchical relation structure}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074898484&partnerID=40&md5=5c0d5bf97b50b7fc685ff275eba50455},
year = {2020}
}
@article{Du2018123,
abstract = {Existing methods for knowledge graph embedding do not ensure the high-rank triples predicted by themselves to be as consistent as possible with the logical background which is made up of a knowledge graph and a logical theory. Users must take great effort to filter consistent triples before adding new triples to the knowledge graph. To alleviate users' burden, we propose an approach to enhancing existing embedding-based methods to encode logical consistency into the learnt distributed representation for the knowledge graph, enforcing high-rank new triples as consistent as possible. To evaluate this approach, four knowledge graphs with logical theories are constructed from the four great classical masterpieces of Chinese literature. Experimental results on these datasets show that our approach is able to guarantee high-rank triples as consistent as possible while preserving a comparable performance as baseline methods in link prediction and triple classification. {\textcopyright} Springer Nature Switzerland AG 2018.},
annote = {cited By 1; Conference of 17th China National Conference on Computational Linguistics, CCL 2018 and 6th International Symposium on Natural Language Processing Based on Naturally Annotated Big Data, NLP-NABD 2018 ; Conference Date: 19 October 2018 Through 21 October 2018; Conference Code:219629},
author = {Du, J and Qi, K and Shen, Y},
doi = {10.1007/978-3-030-01716-3_11},
editor = {{Wang X. Liu T.}, Sun M Liu Z Liu Y},
isbn = {9783030017156},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Baseline methods; Chinese literature; Distributed,Big data,Classification (of information); Computational lin},
pages = {123--135},
publisher = {Springer Verlag},
title = {{Knowledge graph embedding with logical consistency}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055431437&doi=10.1007%2F978-3-030-01716-3_11&partnerID=40&md5=9e454b41bbcc12b19debc97c2e238fe6},
volume = {11221 LNAI},
year = {2018}
}
@inproceedings{8545027,
abstract = {Knowledge graphs contain rich relational structures of the world, and thus complement data-driven knowledge discovery from heterogeneous data. Relational inference between distant entities in large-scale knowledge graphs demands fast relation-specific algebraic manipulations. One of the most effective methods is to embed symbolic relations and entities into continuous spaces, where relations are approximately linear translation between projected images of entities in the relation space. However, state-of-art relation projection methods such as TransR, TransD or TransSparse do not model the correlation between relations, and thus are not scalable to complex knowledge graphs with thousands of relations, both in term of computational demand and statistical robustness. To this end we introduce TransF, a novel translation-based method which mitigates the burden of relation projection by explicitly modeling the basis subspaces of projection matrices. As a result, TransF is far more light weight than the existing projection methods, and is robust when facing a high number of relations. Experimental results on canonical link prediction and triples classification tasks show that our proposed model outperforms competing rivals by a large margin and achieves state-of-the-art performance. Especially, TransF improves by 9% (5%) on the head/tail entity prediction task with N-to-l (l-to-N) over the best performing translation-based method.},
author = {Do, K and Tran, T and Venkatesh, S},
booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8545027},
issn = {1051-4651},
keywords = {data mining;graph theory;inference mechanisms;matr},
month = {aug},
pages = {332--337},
title = {{Knowledge Graph Embedding with Multiple Relation Projections}},
year = {2018}
}
@article{Yuan2019476,
abstract = {Knowledge graphs (KGs) are large scale multi-relational directed graph, which comprise a large amount of triplets. Embedding knowledge graphs into continuous vector space is an essential problem in knowledge extraction. Many existing knowledge graph embedding methods focus on learning rich features from entities and relations with increasingly complex feature engineering. However, they pay little attention on the order information of triplets. As a result, current methods could not capture the inherent directional property of KGs fully. In this paper, we explore knowledge graphs embedding from an ingenious perspective, viewing a triplet as a fixed length sequence. Based on this idea, we propose a novel recurrent knowledge graph embedding method RKGE. It uses an order keeping concatenate operation and a shared sigmoid layer to capture order information and discriminate fine-grained relation-related information. We evaluate our method on knowledge graph completion on benchmark data sets. Extensive experiments show that our approach outperforms state-of-the-art baselines significantly with relatively much lower space complexity. Especially on sparse KGs, RKGE achieves a 86.5% improvement at Hits@1 on FB15K-237. The outstanding results demonstrate that the order information of triplets is highly beneficial for knowledge graph embedding. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 0; Conference of 23rd Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2019 ; Conference Date: 14 April 2019 Through 17 April 2019; Conference Code:225109},
author = {Yuan, J and Gao, N and Xiang, J and Tu, C and Ge, J},
doi = {10.1007/978-3-030-16142-2_37},
editor = {{Gong Z. Zhou Z.-H.}, Huang S.-J. Zhang M.-L. Yang Q},
isbn = {9783030161415},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining,Directed graphs; Embeddings; Vector spaces,Directional properties; Embedding; Essential prob},
pages = {476--488},
publisher = {Springer Verlag},
title = {{Knowledge graph embedding with order information of triplets}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065029178&doi=10.1007%2F978-3-030-16142-2_37&partnerID=40&md5=b1dfe8b7ce28ff3d4bfcca758973ae56},
volume = {11441 LNAI},
year = {2019}
}
@inproceedings{10.1145/3132847.3133119,
abstract = {Knowledge graph embedding, which aims to represent entities and relations in vector spaces, has shown outstanding performance on a few knowledge graph completion tasks. Most existing methods are based on the assumption that a knowledge graph is a set of separate triples, ignoring rich graph features, i.e., structural information in the graph. In this paper, we take advantages of structures in knowledge graphs, especially local structures around a triple, which we refer to as triple context. We then propose a Triple-Context-based knowledge Embedding model (TCE). For each triple, two kinds of structure information are considered as its context in the graph; one is the outgoing relations and neighboring entities of an entity and the other is relation paths between a pair of entities, both of which reflect various aspects of the triple. Triples along with their contexts are represented in a unified framework, in which way structural information in triple contexts can be embodied. The experimental results show that our model outperforms the state-of-the-art methods for link prediction.},
address = {New York, NY, USA},
author = {Shi, Jun and Gao, Huan and Qi, Guilin and Zhou, Zhangquan},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
doi = {10.1145/3132847.3133119},
isbn = {9781450349185},
keywords = {knowledge graph,representation learning,triple context},
pages = {2299--2302},
publisher = {Association for Computing Machinery},
series = {CIKM '17},
title = {{Knowledge Graph Embedding with Triple Context}},
url = {https://doi.org/10.1145/3132847.3133119},
year = {2017}
}
@inproceedings{8851790,
abstract = {A knowledge graph (KG) represents a collection of interlinked descriptions of entities. It has become a key focus for organising and utilising this type of data for applications. Many graph embedding techniques have been proposed to simplify the manipulation while preserving the inherent structure of the KG. However, scant attention has been given to the investigation of the importance of the entities (the nodes of KGs). In this paper, we propose a novel entities importance learning framework that investigates how to weight the entities and use them as a prior knowledge for solving multi-stream regression problems. The framework consists of KG feature extraction, multi-stream correlation analysis, and entity importance learning. To evaluate the proposed method, we implemented the framework based on Wikidata and applied it to Australian retail fuel price forecasting. The experiment results indicate that the proposed method reduces prediction error, which supports the weighted knowledge graph information as a means for improving machine learning model accuracy.},
author = {Chow, D and Liu, A and Zhang, G and Lu, J},
booktitle = {2019 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2019.8851790},
issn = {2161-4407},
keywords = {data mining;feature extraction;financial data proc},
month = {jul},
pages = {1--8},
title = {{Knowledge graph-based entity importance learning for multi-stream regression on Australian fuel price forecasting}},
year = {2019}
}
@inproceedings{10.1145/3397271.3401427,
abstract = {Event representative learning aims to embed news events into continuous space vectors for capturing syntactic and semantic information from text corpus, which is benefit to event-driven quantitative investments. However, the financial market reaction of events is also influenced by the lead-lag effect, which is driven by internal relationships. Therefore, in this paper, we present a knowledge graph-based event embedding framework for quantitative investments. In particular, we first extract structured events from raw texts, and construct the knowledge graph with the mentioned entities and relations simultaneously. Then, we leverage a joint model to merge the knowledge graph information into the objective function of an event embedding learning model. The learned representations are fed as inputs of downstream quantitative trading methods. Extensive experiments on real-world dataset demonstrate the effectiveness of the event embeddings learned from financial news and knowledge graphs. We also deploy the framework for quantitative algorithm trading. The accumulated portfolio return contributed by our method significantly outperforms other baselines.},
address = {New York, NY, USA},
author = {Cheng, Dawei and Yang, Fangzhou and Wang, Xiaoyang and Zhang, Ying and Zhang, Liqing},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3397271.3401427},
isbn = {9781450380164},
keywords = {deep learning,event embedding,financial knowledge graph},
pages = {2221--2230},
publisher = {Association for Computing Machinery},
series = {SIGIR '20},
title = {{Knowledge Graph-Based Event Embedding Framework for Financial Quantitative Investments}},
url = {https://doi.org/10.1145/3397271.3401427},
year = {2020}
}
@inproceedings{Li202019,
abstract = {Predicting the future health conditions of patients based on Electronic Health Records (EHR) is an important research topic. Due to the temporal nature of EHR data, the major challenge is how to properly model the sequences of patient visits. Recurrent Neural Networks (RNNs) with attention mechanisms are widely employed to address this challenge, but often vulnerable to data insufficiency. Lately, predictive models with the guidance of medical knowledge have been proposed to solve this problem and achieve superior performance. Although these models learn reasonable embeddings (infused with knowledge) for clinical variables, they are not able to fully make use of the underlying information in the knowledge graph. To address this, we propose an end-to-end robust solution, namely Graph Neural networks based Diagnosis Prediction (GNDP), to predict future conditions for patients. Compared with existing methods, GNDP learns the spatial and temporal patterns from patients' sequential graph, in which the domain knowledge is naturally infused. We evaluate our GNDP model against a set of state-of-the-art methods on two real-world EHR datasets and the results demonstrate that our approach significantly outperforms the baseline methods. Copyright {\textcopyright} 2020 by SIAM},
annote = {cited By 0; Conference of 2020 SIAM International Conference on Data Mining, SDM 2020 ; Conference Date: 7 May 2020 Through 9 May 2020; Conference Code:160397},
author = {Li, Y and Qian, B and Zhang, X and Liu, H},
booktitle = {Proceedings of the 2020 SIAM International Conference on Data Mining, SDM 2020},
doi = {10.1137/1.9781611976236.3},
editor = {{Demeniconi C.}, Chawla N},
isbn = {9781611976236},
keywords = {Attention mechanisms; Electronic health record; G,Data mining; Diagnosis; Forecasting; Knowledge rep,Recurrent neural networks},
pages = {19--27},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{Knowledge guided diagnosis prediction via graph spatial-temporal network}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089178652&doi=10.1137%2F1.9781611976236.3&partnerID=40&md5=e8e9eaaf461212803fa15102e5dd3301},
year = {2020}
}
@article{Zhang2020415,
abstract = {OCR is a character conversion method based on image recognition. The complexity of the character and the image quality plays a key role in the conversion accuracy. The OCR conversion process has the characteristics of irregular conversion errors and the combination between incorrect conversion words and context of original location in certain text scenarios is established in semantic. In this paper, we propose an OCR conversion error rules inference model based on Chinese character construction attribute knowledge graph to analyze and inference the structure and complexity of Chinese characters. The model integrates a variety of coding methods, extracts features of entities and relationships of different data types with different encoder in the knowledge graph, uses convolutional neural networks to learn and inference the unknown error rules in the OCR conversion. In addition, in order to enable the triple feature matrix to fully contain the construction attribute information of the Chinese characters, a feature crossover algorithm for feature diffusion of the triple feature matrix is introduced. In this algorithm, the relation matrix and the entities matrix are crossed to generate the new feature matrix which can better represent the triple of knowledge graph. The experimental results show that, compared with the current mainstream knowledge inference model, the OCR conversion error rules inference model incorporating the feature cross algorithm has achieved important improvements in MRR, Hits@1, Hits@2 and other evaluation indicators on public data sets and task-related data sets. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 9th CCF International Conference on Natural Language Processing and Chinese Computing, NLPCC 2020 ; Conference Date: 14 October 2020 Through 18 October 2020; Conference Code:249929},
author = {Zhang, X and Wang, H and Gu, W},
doi = {10.1007/978-3-030-60457-8_34},
editor = {{Zhu X. Zhang M.}, Hong Y He R},
isbn = {9783030604561},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attribute information; Chinese characters; Conver,Complex networks; Convolutional neural networks; E,Matrix algebra},
pages = {415--425},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Knowledge Inference Model of OCR Conversion Error Rules Based on Chinese Character Construction Attributes Knowledge Graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093082603&doi=10.1007%2F978-3-030-60457-8_34&partnerID=40&md5=676757b237432edafa2ff864fd2422d8},
volume = {12431 LNAI},
year = {2020}
}
@inproceedings{Kertkeidkachorn2019324,
abstract = {G-protein-coupled receptors (GPCRs) are the largest family of plasma membrane receptors, which can be activated by an external signal such as a ligand. Binding of GPCRs and ligands in the plasma membrane activates pathways that involve a sequence of events. Better understanding of GPCRs and the signal transduction pathways can help biologists to target new drugs or regulate many important cellular functions for diseases. In this paper, we introduce ontology-based GPCR signal transduction pathways, which are converted from manually collected pathways in PubMed papers. We applied network graph embedding and knowledge graph embedding algorithms on the pathway data to discover protein interactions in the GPCR signal transduction pathways. Experiments show that we could suggest missing or unknown pathways by analyzing the ontology-based GPCR signal transduction pathways. Moreover, we introduced ontology constraints on the GPCR pathway for predicting the missing interactions. The experimental results showed that using ontology constraints can boost the performance of knowledge graph embedding algorithms. {\textcopyright} 2019 IEEE.},
annote = {cited By 0; Conference of 13th IEEE International Conference on Semantic Computing, ICSC 2019 ; Conference Date: 30 January 2019 Through 1 February 2019; Conference Code:146024},
author = {Kertkeidkachorn, N and Zhao, L and Liu, X and Ichise, R},
booktitle = {Proceedings - 13th IEEE International Conference on Semantic Computing, ICSC 2019},
doi = {10.1109/ICOSC.2019.8665519},
isbn = {9781538667835},
keywords = {Bacteriophages; Cell membranes; Data mining; Embed,Cellular function; External signals; G protein co,Signal transduction},
pages = {324--329},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Knowledge Representation of G-Protein-Coupled Receptor Signal Transduction Pathways}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064127177&doi=10.1109%2FICOSC.2019.8665519&partnerID=40&md5=eb23a615d2542562e3ef4eef3a3ec29d},
year = {2019}
}
@inproceedings{10.1145/3292500.3330836,
abstract = {Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.},
address = {New York, NY, USA},
author = {Wang, Hongwei and Zhang, Fuzheng and Zhang, Mengdi and Leskovec, Jure and Zhao, Miao and Li, Wenjie and Wang, Zhongyuan},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3292500.3330836},
isbn = {9781450362016},
keywords = {graph neural networks,knowledge-aware recommendation,label propagation},
pages = {968--977},
publisher = {Association for Computing Machinery},
series = {KDD '19},
title = {{Knowledge-Aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems}},
url = {https://doi.org/10.1145/3292500.3330836},
year = {2019}
}
@inproceedings{10.1145/3357384.3358071,
abstract = {Textual entailment is a central problem of language variability, which has been attracting a lot of interest and it poses significant issues in front of systems aimed at natural language understanding. Recently, various frameworks have been proposed for textual entailment recognition, ranging from traditional computational linguistics techniques to deep learning model based methods. However, recent deep neural networks that achieve the state of the art on textual entailment task only consider the context information of the given sentences rather than the real-world background information and knowledge beyond the context. In the paper, we propose a Knowledge-Context Interactive Textual Entailment Network (KCI-TEN) that learns graph level sentence representations by harnessing external knowledge graph with graph attention network. We further propose a text-graph interaction mechanism for neural based entailment matching learning, which endows the redundancy and noise with less importance and put emphasis on the informative representations. Experiments on the SciTail dataset demonstrate that KCI-TEN outperforms the state-of-the-art methods.},
address = {New York, NY, USA},
author = {Chen, Daoyuan and Li, Yaliang and Yang, Min and Zheng, Hai-Tao and Shen, Ying},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3357384.3358071},
isbn = {9781450369763},
keywords = {graph attention network,knowledge base,textual entailment},
pages = {2145--2148},
publisher = {Association for Computing Machinery},
series = {CIKM '19},
title = {{Knowledge-Aware Textual Entailment with Graph Attention Network}},
url = {https://doi.org/10.1145/3357384.3358071},
year = {2019}
}
@article{Zhou2018222,
abstract = {Data sparsity is a common issue in recommendation systems, particularly collaborative filtering. In real recommendation scenarios, user preferences are often quantitatively sparse because of the application nature. To address the issue, we proposed a knowledge graph-based semantic information enhancement mechanism to enrich the user preferences. Specifically, the proposed Hierarchical Collaborative Embedding (HCE) model leverages both network structure and text info embedded in knowledge bases to supplement traditional collaborative filtering. The HCE model jointly learns the latent representations from user preferences, linkages between items and knowledge base, as well as the semantic representations from knowledge base. Experiment results on GitHub dataset demonstrated that semantic information from knowledge base has been properly captured, resulting improved recommendation performance. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
annote = {cited By 2; Conference of 22nd Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2018 ; Conference Date: 3 June 2018 Through 6 June 2018; Conference Code:214589},
author = {Zhou, Z and Liu, S and Xu, G and Xie, X and Yin, J and Li, Y and Zhang, W},
doi = {10.1007/978-3-319-93037-4_18},
editor = {{Ho B. Phung D.}, Webb G I Tseng V S Ganji M Rashidi L},
isbn = {9783319930367},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Collaborative filtering,Data mining; Graphic methods; Knowledge based syst,Enhancement mechanism; Knowledge basis; Knowledge},
pages = {222--234},
publisher = {Springer Verlag},
title = {{Knowledge-based recommendation with hierarchical collaborative embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049365297&doi=10.1007%2F978-3-319-93037-4_18&partnerID=40&md5=a3da221c073881788778693d8cd1ad7d},
volume = {10938 LNAI},
year = {2018}
}
@inproceedings{Ding20162133,
abstract = {Representing structured events as vectors in continuous space offers a new way for defining dense features for natural language processing (NLP) applications. Prior work has proposed effective methods to learn event representations that can capture syntactic and semantic information over text corpus, demonstrating their effectiveness for downstream tasks such as event-driven stock prediction. On the other hand, events extracted from raw texts do not contain background knowledge on entities and relations that they are mentioned. To address this issue, this paper proposes to leverage extra information from knowledge graph, which provides ground truth such as attributes and properties of entities and encodes valuable relations between entities. Specifically, we propose a joint model to combine knowledge graph information into the objective function of an event embedding learning model. Experiments on event similarity and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities. {\textcopyright} 1963-2018 ACL.},
annote = {cited By 25; Conference of 26th International Conference on Computational Linguistics, COLING 2016 ; Conference Date: 11 December 2016 Through 16 December 2016; Conference Code:136517},
author = {Ding, X and Zhang, Y and Liu, T and Duan, J},
booktitle = {COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers},
isbn = {9784879747020},
keywords = {Accurate prediction; Back-ground knowledge; Conti,Commerce; Computational linguistics; Financial mar,Knowledge management},
pages = {2133--2142},
publisher = {Association for Computational Linguistics, ACL Anthology},
title = {{Knowledge-driven event embedding for stock prediction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051146159&partnerID=40&md5=b392aa1f2bf3a29ea48ad7df6175ff10},
year = {2016}
}
@inproceedings{Deng2019678,
abstract = {Deep neural networks have achieved promising results in stock trend prediction. However, most of these models have two common drawbacks, including (i) current methods are not sensitive enough to abrupt changes of stock trend, and (ii) forecasting results are not interpretable for humans. To address these two problems, we propose a novel Knowledge-Driven Temporal Convolutional Network (KDTCN) for stock trend prediction and explanation. Firstly, we extract structured events from financial news, and utilize external knowledge from knowledge graph to obtain event embeddings. Then, we combine event embeddings and price values together to forecast stock trend. We evaluate the prediction accuracy to show how knowledge-driven events work on abrupt changes. We also visualize the effect of events and linkage among events based on knowledge graph, to explain why knowledge-driven events are common sources of abrupt changes. Experiments demonstrate that KDTCN can (i) react to abrupt changes much faster and outperform state-of-the-art methods on stock datasets, as well as (ii) facilitate the explanation of prediction particularly with abrupt changes. {\textcopyright} 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.},
annote = {cited By 4; Conference of 2019 World Wide Web Conference, WWW 2019 ; Conference Date: 13 May 2019 Through 17 May 2019; Conference Code:147967},
author = {Deng, S and Chen, J and Zhang, N and Pan, J Z and Zhang, W and Chen, H},
booktitle = {The Web Conference 2019 - Companion of the World Wide Web Conference, WWW 2019},
doi = {10.1145/3308560.3317701},
isbn = {9781450366755},
keywords = {Convolution; Deep neural networks; Embeddings; For,Data mining,Event extraction; Explanation; Knowledge-driven;},
pages = {678--685},
publisher = {Association for Computing Machinery, Inc},
title = {{Knowledge-driven stock trend prediction and explanation via temporal convolutional network}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066884024&doi=10.1145%2F3308560.3317701&partnerID=40&md5=c4e0c0e1e946a33f07e8658a9cfd927f},
year = {2019}
}
@inproceedings{10.1145/3308558.3313425,
abstract = {Representing words as embeddings in a continuous vector space has been proven to be successful in improving the performance in many natural language processing (NLP) tasks. Beyond the traditional methods that learn the embeddings from large text corpora, ensemble methods have been proposed to leverage the merits from pre-trained word embeddings as well as external semantic sources. In this paper, we propose a knowledge-enhanced ensemble method to combine both knowledge graphs and pre-trained word embedding models. Specifically, we interpret relations in knowledge graphs as linear translation from one word to another. We also propose a novel weighting scheme to further distinguish edges in the knowledge graph with same type of relation. Extensive experiments demonstrate that our proposed method is up to 20% times better than state-of-the-art in word analogy task and up to 16% times better than state-of-the-art in word similarity task.},
address = {New York, NY, USA},
author = {Fang, Lanting and Luo, Yong and Feng, Kaiyu and Zhao, Kaiqi and Hu, Aiqun},
booktitle = {The World Wide Web Conference},
doi = {10.1145/3308558.3313425},
isbn = {9781450366748},
keywords = {Ensemble model,Knowledge graph,Word embedding},
pages = {427--437},
publisher = {Association for Computing Machinery},
series = {WWW '19},
title = {{Knowledge-Enhanced Ensemble Learning for Word Embeddings}},
url = {https://doi.org/10.1145/3308558.3313425},
year = {2019}
}
@article{9143267,
abstract = {The growing demand for the meaningful use of electronic medical records has led to great interest in medical entities and relation extraction technologies. Most existing methods perform relation extraction based on manually labeled documents and rarely consider incorporating knowledge graphs that include rich, valuable structured knowledge, which will cause semantic ambiguities. To address this problem, we propose a knowledge-enhanced relation extraction (KERE) model. First, we extract knowledge information from the knowledge graph to generate a knowledge-guided word embedding. Then, the lexical features are considered supplementary information for semantic understanding. Experiments on real-world datasets show that the KERE model achieves important improvements in a biomedical relation extraction task.},
author = {Zhao, Q and Li, J and Xu, C and Yang, J and Zhao, L},
doi = {10.1109/MITP.2020.2984598},
issn = {1941-045X},
journal = {IT Professional},
keywords = {electronic health records;graph theory;information},
month = {jul},
number = {4},
pages = {57--62},
title = {{Knowledge-Enhanced Relation Extraction for Chinese EMRs}},
volume = {22},
year = {2020}
}
@article{ISI:000463654000008,
abstract = {Knowledge bases are in widespread use for aiding tasks such as
information extraction and information retrieval, for example in Web
search. However, knowledge bases are known to be inherently incomplete,
where in particular tail entities and properties are under-represented.
As a complimentary data source, embedded entity markup based on
Microdata, RDFa, and Microformats have become prevalent on the Web and
constitute an unprecedented source of data with significant potential to
aid the task of knowledge base augmentation (KBA). RDF statements
extracted from markup are fundamentally different from traditional
knowledge graphs: entity descriptions are flat, facts are highly
redundant and of varied quality, and, explicit links are missing despite
a vast amount of coreferences. Therefore, data fusion is required in
order to facilitate the use of markup data for KBA. We present a novel
data fusion approach which addresses these issues through a combination
of entity matching and fusion techniques geared towards the specific
challenges associated with Web markup. To ensure precise and
non-redundant results, we follow a supervised learning approach based on
a set of features considering aspects such as quality and relevance of
entities, facts and their sources. We perform a thorough evaluation on a
subset of the Web Data Commons dataset and show significant potential
for augmenting existing knowledge bases. A comparison with existing data
fusion baselines demonstrates superior performance of our approach when
applied to Web markup data.},
address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
author = {Yu, Ran and Gadiraju, Ujwal and Fetahu, Besnik and Lehmberg, Oliver and Ritze, Dominique and Dietze, Stefan},
doi = {10.3233/SW-180304},
issn = {1570-0844},
journal = {SEMANTIC WEB},
keywords = {Knowledge base augmentation; Web markup; microdata},
number = {1},
pages = {159--180},
publisher = {IOS PRESS},
title = {{KnowMore - knowledge base augmentation with structured web markup}},
type = {Article},
volume = {10},
year = {2019}
}
@inproceedings{8970948,
abstract = {Thanks to the widespread adoption of Electronic Health Record (EHR) systems, a variety of data-driven clinical risk prediction approaches have been spawned in recent years. However, there remain three challenges, which if addressed would improve the performance and applicability of such models. (i) Due to the limited data sharing between different health care institutions, the EHR data collected by a single institution is often inadequate or missing some visits records. The limited number of data cannot meet the large sample required of recent approaches especially deep learning models. In addition, the missing records (due to visiting different institution) may contain important health condition of the patient, which if ignored would cause prediction bias. (ii) Few existing approaches take clinical knowledge into account. The auxiliary knowledge if included can greatly reduce the data dependency of many modern learning algorithms. (iii) Most existing deep learning based methods are unable to identify the contribution of each medical event to the final results, which prohibits such models from being widely accepted in practical clinical applications. In this paper, we propose an interpretable and knowledge-guided deep model to address these challenges. Specifically, we distill knowledge from a clinical knowledge graph both explicitly and implicitly, which can not only supplement inadequate patient records but also guide the predicting process of the model. Furthermore, skip-connections and attention mechanisms are adopted to improve the interpretability of our model. In the context of heart failure prediction task, our model outperforms several state-of-the-art methods. Finally, a series of case studies are presented to prove the interpretability of our model.},
author = {Zhang, X and Qian, B and Li, Y and Yin, C and Wang, X and Zheng, Q},
booktitle = {2019 IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2019.00196},
issn = {2374-8486},
keywords = {diseases;electronic health records;graph theory;he},
month = {nov},
pages = {1492--1497},
title = {{KnowRisk: An Interpretable Knowledge-Guided Model for Disease Risk Prediction}},
year = {2019}
}
@inproceedings{Wang20202246,
abstract = {Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 12; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Wang, G and Zhang, W and Wang, R and Zhou, Y and Chen, X and Zhang, W and Zhu, H and Chen, H},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {Data mining,Embeddings; Extraction; Labeled data; Natural lang,Knowledge graphs; Label free; Prior knowledge; Re},
pages = {2246--2255},
publisher = {Association for Computational Linguistics},
title = {{Label-free distant supervision for relation extraction via knowledge graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081738408&partnerID=40&md5=a42733c5b20e34263c5f089f881fbb43},
year = {2020}
}
@inproceedings{10.1007/978-3-319-18818-8_21,
abstract = {Learning cross-lingual semantic representations of relations from textual data is useful for tasks like cross-lingual information retrieval and question answering. So far, research has been mainly focused on cross-lingual entity linking, which is confined to linking between phrases in a text document and their corresponding entities in a knowledge base but cannot link to relations. In this paper, we present an approach for inducing clusters of semantically related relations expressed in text, where relation clusters i can be extracted from text of different languages, ii are embedded in a semantic representation of the context, and iii can be linked across languages to properties in a knowledge base. This is achieved by combining multi-lingual semantic role labeling SRL with cross-lingual entity linking followed by spectral clustering of the annotated SRL graphs. With our initial implementation we learned a cross-lingual lexicon of relation expressions from English and Spanish Wikipedia articles. To demonstrate its usefulness we apply it to cross-lingual question answering over linked data.},
address = {Berlin, Heidelberg},
author = {Rettinger, Achim and Schumilin, Artem and Thoma, Steffen and Ell, Basil},
booktitle = {Proceedings of the 12th European Semantic Web Conference on The Semantic Web. Latest Advances and New Domains - Volume 9088},
doi = {10.1007/978-3-319-18818-8_21},
isbn = {9783319188171},
keywords = {Cross-lingual relation clustering,Relation linking,Unsupervised relation extraction},
pages = {337--352},
publisher = {Springer-Verlag},
title = {{Learning a Cross-Lingual Semantic Representation of Relations Expressed in Text}},
url = {https://doi.org/10.1007/978-3-319-18818-8_21},
year = {2015}
}
@inproceedings{9006589,
abstract = {For sales and marketing organizations within large enterprises, identifying and understanding new markets, customers and partners is a key challenge. Intel's Sales and Marketing Group (SMG) faces similar challenges while growing in new markets and domains and evolving its existing business. In today's complex technological and commercial landscape, there is need for intelligent automation supporting a fine-grained understanding of businesses in order to help SMG sift through millions of companies across many geographies and languages and identify relevant directions. We present a system developed in our company that mines millions of public business web pages, and extracts a faceted customer representation. We focus on two key customer aspects that are essential for finding relevant opportunities: industry segments (ranging from broad verticals such as healthcare, to more specific fields such as “video analytics”) and functional roles (e.g., “manufacturer” or “retail”). To address the challenge of labeled data collection, we enrich our data with external information gleaned from Wikipedia, and develop a semi-supervised multi-label, multi-lingual deep learning model that parses customer website texts and classifies them into their respective facets. Our system scans and indexes companies as part of a large-scale knowledge graph that currently holds tens of millions of connected entities with thousands being fetched, enriched and connected to the graph by the hour in real time, and also supports knowledge and insight discovery. In experiments conducted in our company, we are able to significantly boost the performance of sales personnel in the task of discovering new customers and commercial partnership opportunities.},
author = {Lieder, I and Segal, M and Avidan, E and Cohen, A and Hope, T},
booktitle = {2019 IEEE International Conference on Big Data (Big Data)},
doi = {10.1109/BigData47090.2019.9006589},
keywords = {business data processing;consumer behaviour;data m},
month = {dec},
pages = {6136--6138},
title = {{Learning a Faceted Customer Segmentation for Discovering new Business Opportunities at Intel}},
year = {2019}
}
@inproceedings{Annervaz2018313,
abstract = {Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data. Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolutionbased model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) \& DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base. {\textcopyright} 2018 The Association for Computational Linguistics.},
annote = {cited By 7; Conference of 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2018 ; Conference Date: 1 June 2018 Through 6 June 2018; Conference Code:158892},
author = {Annervaz, K M and Chowdhury, S B R and Dukkipati, A},
booktitle = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
isbn = {9781948087278},
keywords = {Attention mechanisms; Enhance learning; Knowledge,Computational linguistics; Deep learning; Knowledg,Learning systems},
pages = {313--322},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Learning beyond datasets: Knowledge graph augmented neural networks for natural language processing}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075751697&partnerID=40&md5=53881b9376f21b7c2dd77a987ac41ebb},
volume = {1},
year = {2018}
}
@inproceedings{Garcia-Silva201927,
abstract = {Natural language processing can assist scientists to leverage the increasing amount of information contained in scientific bibliography. The current trend, based on deep learning and embeddings, uses representations at the (sub)word level that require large amounts of training data and neural architectures with millions of parameters to learn successful language models, like BERT. However, these representations may not be well suited for the scientific domain, where it is common to find complex terms, e.g. multi-word, with a domain-specific meaning in a very specific context. In this paper we propose an approach based on a linguistic analysis of the corpus using a knowledge graph to learn representations that can unambiguously capture such terms and their meaning. We learn embeddings from different linguistic annotations on the text and evaluate them through a classification task over the SciGraph taxonomy, showing that our representations outperform (sub)word-level approaches. Copyright {\textcopyright} 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
annote = {cited By 0; Conference of 3rd International Workshop on Capturing Scientific Knowledge, SciKnow 2019 ; Conference Date: 19 November 2019; Conference Code:156026},
author = {Garcia-Silva, A and Denaux, R and Gomez-Perez, J M},
booktitle = {CEUR Workshop Proceedings},
editor = {{Garijo D. Markovic M.}, Groth P Perez I S Belhajjame K},
issn = {16130073},
keywords = {Amount of information; Classification tasks; Conv,Classification (of information); Deep learning; Em,Text processing},
pages = {27--32},
publisher = {CEUR-WS},
title = {{Learning embeddings from scientific corpora using lexical, grammatical and semantic information}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077823532&partnerID=40&md5=532805ed6a552718a6e8de3f1b4c3abc},
volume = {2526},
year = {2019}
}
@inproceedings{Sawant20131099,
abstract = {Thanks to information extraction and semantic Web efforts, search on unstructured text is increasingly refined using semantic annotations and structured knowledge bases. However, most users cannot become familiar with the schema of knowledge bases and ask structured queries. Interpreting free-format queries into a more structured representation is of much current interest. The dominant paradigm is to segment or partition query tokens by purpose (references to types, entities, attribute names, attribute values, relations) and then launch the interpreted query on structured knowledge bases. Given that structured knowledge extraction is never complete, here we choose a less trodden path: a data representation that retains the unstructured text corpus, along with structured annotations (mentions of entities and relationships) on it. We propose two new, natural formulations for joint query interpretation and response ranking that exploit bidirectional flow of information between the knowledge base and the corpus. One, inspired by probabilistic language models, computes expected response scores over the uncertainties of query interpretation. The other is based on max-margin discriminative learning, with latent variables representing those uncertainties. In the context of typed entity search, both formulations bridge a considerable part of the accuracy gap between a generic query that does not constrain the type at all, and the upper bound where the "perfect" target entity type of each query is provided by humans. Our formulations are also superior to a two-stage approach of first choosing a target type using recent query type prediction techniques, and then launching a type-restricted entity search query. Copyright is held by the International World Wide Web Conference Committee (IW3C2).},
address = {Rio de Janeiro},
annote = {cited By 23; Conference of 22nd International Conference on World Wide Web, WWW 2013 ; Conference Date: 13 May 2013 Through 17 May 2013; Conference Code:102216},
author = {Sawant, U and Chakrabarti, S},
booktitle = {WWW 2013 - Proceedings of the 22nd International Conference on World Wide Web},
doi = {10.1145/2488388.2488484},
isbn = {9781450320351},
keywords = {Data mining,Data representations; Discriminative learning; En,Knowledge based systems; World Wide Web},
pages = {1099--1109},
publisher = {Association for Computing Machinery},
title = {{Learning joint query interpretation and response ranking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893125385&doi=10.1145%2F2488388.2488484&partnerID=40&md5=8341b00d26ce29796781e1edcc4873fb},
year = {2013}
}
@inproceedings{10.1145/3132847.3132939,
abstract = {In knowledge graph embedding models, the margin-based ranking loss as the common loss function is usually used to encourage discrimination between golden triplets and incorrect triplets, which has proved effective in many translation-based models for knowledge graph embedding. However, we find that the loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation. In this paper, we present a limit-based scoring loss to provide lower scoring of a golden triplet, and then to extend two basic translation models TransE and TransH, separately to TransE-RS and TransH-RS by combining limit-based scoring loss with margin-based ranking loss. Both the presented models have low complexities of parameters benefiting for application on large scale graphs. In experiments, we evaluate our models on two typical tasks including triplet classification and link prediction, and also analyze the scoring distributions of positive and negative triplets by different models. Experimental results show that the introduced limit-based scoring loss is effective to improve the capacities of knowledge graph embedding.},
address = {New York, NY, USA},
author = {Zhou, Xiaofei and Zhu, Qiannan and Liu, Ping and Guo, Li},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
doi = {10.1145/3132847.3132939},
isbn = {9781450349185},
keywords = {data mining,image recognition,machine learning,nlp},
pages = {1009--1018},
publisher = {Association for Computing Machinery},
series = {CIKM '17},
title = {{Learning Knowledge Embeddings by Combining Limit-Based Scoring Loss}},
url = {https://doi.org/10.1145/3132847.3132939},
year = {2017}
}
@inproceedings{Jin2019669,
abstract = {A large number of spatial knowledge graphs (SKGs) are available from spatially enriched knowledge bases, e.g., DBpedia and YAGO2. This provides a great chance to understand valuable information about the regions surrounding us. However, it is hard to comprehend SKGs due to the explosively growing volume and the complication of the graph structures. Thus we study the problem of similar region search (SRS), which is an easy-to-use but effective way to explore spatial data. The effectiveness of SRS highly depends on how to measure the region similarity. However, existing approaches cannot make use of the rich information contained in SKGs thus may lead to incorrect results. In this paper, we propose a spatial knowledge representation learning method for region similarity, namely SKRL4RS. SKRL4RS firstly encodes the spatial entities of an SKG into a vector space to make it easier to extract useful features. Then regions are represented by 3-D tensors using the spatial entity embeddings together with geographical information. Finally, region tensors are fed into the conventional triplet network to learn the feature vectors of regions. The region similarity measure learned by SKRL4RS can capture the hierarchical types, semantic relatedness, and relative locations of spatial entities inside a region. Experimental results on two real-world datasets show that our SKRL4RS outperforms the state-of-the-art by a significant margin in terms of the accuracy of measuring region similarity. {\textcopyright} 2019 Association for Computing Machinery.},
annote = {cited By 0; Conference of 28th ACM International Conference on Information and Knowledge Management, CIKM 2019 ; Conference Date: 3 November 2019 Through 7 November 2019; Conference Code:154362},
author = {Jin, X and Lee, D and Oh, B and Lee, K.-H. and Lee, S and Chen, L},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3357384.3358008},
isbn = {9781450369763},
keywords = {Deep learning,Embeddings; Geographical regions; Knowledge manage,Entity embedding; Geographical information; Real-},
pages = {669--678},
publisher = {Association for Computing Machinery},
title = {{Learning region similarity over spatial knowledge graphs with hierarchical types and semantic relations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075457450&doi=10.1145%2F3357384.3358008&partnerID=40&md5=c09057adeb3cb5a01f8a1f0d5d4b8f17},
year = {2019}
}
@inproceedings{Chen20151869,
abstract = {We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems, in particular, learning the hierarchical semantic structure from the data. Given unlabelled conversations, we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots (e.g., both slots "direction" and "locale" convey location-related information) for building a coherent semantic hierarchy, and then estimate the slot importance at different levels. The high-level semantic estimation involves not only within-slot but also crossslot relations. The experiments show that high-level semantic information can accurately estimate the prominence of slots, significantly improving the slot induction performance; furthermore, a semantic decoder trained on the data with automatically extracted slots achieves about 68% F-measure, which is close to the one from hand-crafted grammars. Copyright {\textcopyright} 2015 ISCA.},
annote = {cited By 4; Conference of 16th Annual Conference of the International Speech Communication Association, INTERSPEECH 2015 ; Conference Date: 6 September 2015 Through 10 September 2015; Conference Code:118697},
author = {Chen, Y.-N. and Wang, W Y and Rudnicky, A I},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
editor = {{Noth E. Steidl S.}, Moller S Ney H Mobius B},
issn = {2308457X},
keywords = {Cluster analysis; Computational linguistics; Hiera,Embeddings; Hierarchical agglomerative clustering,Semantics},
pages = {1869--1873},
publisher = {International Speech and Communication Association},
title = {{Learning semantic hierarchy with distributed representations for unsupervised spoken language understanding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959179155&partnerID=40&md5=8d6c010f2f6b225f84001ba4cd8fc1c1},
volume = {2015-Janua},
year = {2015}
}
@inproceedings{García-Durán20204816,
abstract = {Research on link prediction in knowledge graphs has mainly focused on static multi-relational data. In this work we consider temporal knowledge graphs where relations between entities may only hold for a time interval or a specific point in time. In line with previous work on static knowledge graphs, we propose to address this problem by learning latent entity and relation type representations. To incorporate temporal information, we utilize recurrent neural networks to learn time-aware representations of relation types which can be used in conjunction with existing latent factorization methods. The proposed approach is shown to be robust to common challenges in real-world KGs: the sparsity and heterogeneity of temporal expressions. Experiments show the benefits of our approach on four temporal KGs. The data sets are available under a permissive BSD-3 license1 {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 0; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Garc{\'{i}}a-Dur{\'{a}}n, A and Duman{\v{c}}i{\'{c}}, S and Niepert, M},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {Factorization methods; Knowledge graphs; Learning,Natural language processing systems,Recurrent neural networks; Signal encoding},
pages = {4816--4821},
publisher = {Association for Computational Linguistics},
title = {{Learning sequence encoders for temporal knowledge graph completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081738126&partnerID=40&md5=0b0e9e55ea302d4aa95bd69b4753a0a8},
year = {2020}
}
@inproceedings{Bordes2011301,
abstract = {Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigid symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like natural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning methods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text. Copyright {\textcopyright} 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.},
address = {San Francisco, CA},
annote = {cited By 301; Conference of 25th AAAI Conference on Artificial Intelligence and the 23rd Innovative Applications of Artificial Intelligence Conference, AAAI-11 / IAAI-11 ; Conference Date: 7 August 2011 Through 11 August 2011; Conference Code:87049},
author = {Bordes, A and Weston, J and Collobert, R and Bengio, Y},
booktitle = {Proceedings of the National Conference on Artificial Intelligence},
isbn = {9781577355083},
keywords = {Collaborative filtering; Collaborative process; Em,Computational linguistics; Information retrieval;,Natural language processing systems},
pages = {301--306},
title = {{Learning structured embeddings of knowledge bases}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055048322&partnerID=40&md5=b617bd2bfea9fee07032ffc2fdb56e77},
volume = {1},
year = {2011}
}
@article{Gu2020257,
abstract = {Text-based end-to-end question answering (QA) systems have attracted more attention for their good robustness and excellent performance in dealing with complex questions. However, this kind of method lacks certain interpretability, which is essential for the QA system. For instance, the interpretability of answers is particularly significant in the medical field, in that interpretable answers are more credible and apt to acception. The methods based on knowledge graph (KG) can improve the interpretability, but suffer from the problems of incompleteness and sparseness of KG. In this paper, we propose a novel method (EGQA) to solve complex question answering via combining text and KG. We use Wikipedia as a text source to extract documents related to the question and extract triples from the documents to construct a raw graph (i.e., a small-scale KG). Then, we extract the evidence graphs from the raw graph and adopt Attention-based Graph Neural Network (AGNN) to embed them to find the answer. Our experiments conduct on a real medical dataset Head-QA, which shows that our approach can effectively improve the interpretability and performance of complex question answering. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 4th Asia-Pacific Web and Web-Age Information Management, Joint Conference on Web and Big Data, APWeb-WAIM 2020 ; Conference Date: 18 September 2020 Through 20 September 2020; Conference Code:250159},
author = {Gu, G and Li, B and Gao, H and Wang, M},
doi = {10.1007/978-3-030-60259-8_20},
editor = {{Wang X. Zhang R.}, Lee Y.-K. Sun L Moon Y.-S.},
isbn = {9783030602581},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Big data; Complex networks; Information management,Complex questions; Evidence graph; Graph neural n,Natural language processing systems},
pages = {257--269},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Learning to Answer Complex Questions with Evidence Graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093980534&doi=10.1007%2F978-3-030-60259-8_20&partnerID=40&md5=8ebc5a63d8c66de31deabcbd81e092e1},
volume = {12317 LNCS},
year = {2020}
}
@inproceedings{8953301,
abstract = {The recent advances in instance-level detection tasks lay a strong foundation for automated visual scenes understanding. However, the ability to fully comprehend a social scene still eludes us. In this work, we focus on detecting human-object interactions (HOIs) in images, an essential step towards deeper scene understanding. HOI detection aims to localize human and objects, as well as to identify the complex interactions between them. Innate in practical problems with large label space, HOI categories exhibit a long-tail distribution, i.e., there exist some rare categories with very few training samples. Given the key observation that HOIs contain intrinsic semantic regularities despite they are visually diverse, we tackle the challenge of long-tail HOI categories by modeling the underlying regularities among verbs and objects in HOIs as well as general relationships. In particular, we construct a knowledge graph based on the ground-truth annotations of training dataset and external source. In contrast to direct knowledge incorporation, we address the necessity of dynamic image-specific knowledge retrieval by multi-modal learning, which leads to an enhanced semantic embedding space for HOI comprehension. The proposed method shows improved performance on V-COCO and HICO-DET benchmarks, especially when predicting the rare HOI categories.},
author = {Xu, B and Wong, Y and Li, J and Zhao, Q and Kankanhalli, M S},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2019.00212},
issn = {2575-7075},
keywords = {graph theory;image classification;image representa},
month = {jun},
pages = {2019--2028},
title = {{Learning to Detect Human-Object Interactions With Knowledge}},
year = {2019}
}
@inproceedings{8970688,
abstract = {Knowledge graph (KG) embedding techniques represent entities and relations as low-dimensional, continuous vectors, and thus enables machine learning models to be easily adapted to KG completion and querying tasks. However, learned dense vectors are inefficient for large-scale similarity computations. Learning-to-hash is to learn compact binary codes from high-dimensional input data and provides a promising way to accelerate efficiency by measuring Hamming distance instead of Euclidean distance or dot-product. Unfortunately, most of learning-to-hash methods cannot be directly applied to KG structure encoding. In this paper, we introduce a novel framework for encoding incomplete KGs and graph queries in Hamming space. To preserve KG structure information from embeddings to hash codes and address the ill-posed gradient issue in optimization, we utilize a continuation method with convergence guarantees to jointly encode queries and KG entities with geometric operations. The hashed embedding of a query can be utilized to discover target answers from incomplete KGs whilst the efficiency has been greatly improved.We compared our model with state-of-the-art methods on real-world KGs. Experimental results show that our framework not only significantly speeds up the searching process, but also provides good results for unanswerable queries caused by incomplete information.},
author = {Wang, M and Shen, H and Wang, S and Yao, L and Jiang, Y and Qi, G and Chen, Y},
booktitle = {2019 IEEE International Conference on Data Mining (ICDM)},
doi = {10.1109/ICDM.2019.00174},
issn = {2374-8486},
keywords = {binary codes;graph theory;learning (artificial int},
month = {nov},
pages = {1360--1365},
title = {{Learning to Hash for Efficient Search Over Incomplete Knowledge Graphs}},
year = {2019}
}
@article{9091850,
abstract = {Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in an input-text sequence to their correct references in a knowledge graph. We tackle NED problem by leveraging two novel objectives for pre-training framework, and propose a novel pre-training NED model. Especially, the proposed pre-training NED model consists of: (i) concept-enhanced pre-training, aiming at identifying valid lexical semantic relations with the concept semantic constraints derived from external resource Probase; and (ii) masked entity language model, aiming to train the contextualized embedding by predicting randomly masked entities based on words and non-masked entities in the given input-text. Therefore, the proposed pre-training NED model could merge the advantage of pre-training mechanism for generating contextualized embedding with the superiority of the lexical knowledge (e.g., concept knowledge emphasized here) for understanding language semantic. We conduct experiments on the CoNLL dataset and TAC dataset, and various datasets provided by GERBIL platform. The experimental results demonstrate that the proposed model achieves significantly higher performance than previous models.},
author = {Ji, Z and Dai, L and Pang, J and Shen, T},
doi = {10.1109/ACCESS.2020.2994247},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {knowledge based systems;learning (artificial intel},
pages = {100469--100484},
title = {{Leveraging Concept-Enhanced Pre-Training Model and Masked-Entity Language Model for Named Entity Disambiguation}},
volume = {8},
year = {2020}
}
@article{Rahman2020958,
abstract = {Knowledge graph embedding aims to embed entities and relations of multi-relational data in low dimensional vector spaces. Knowledge graphs are useful for numerous artificial intelligence (AI) applications. However, they (KGs) are far from completeness and hence KG embedding models have quickly gained massive attention. Nevertheless, the state-of-the-art KG embedding models ignore the category specific projection of entities and the impact of entity types in relational aspect. For example, the entity “Washington” could belong to the person or location category depending on its appearance in a specific relation. In a KG, an entity usually holds many type properties. It leads us to a very interesting question: are all the type properties of an entity are meaningful for a specific relation? In this paper, we propose a KG embedding model TPRC that leverages entity-type properties in the relational context. To show the effectiveness of our model, we apply our idea to the TransE, TransR and TransD. Our approach outperforms other state-of-the-art approaches as TransE, TransD, DistMult and ComplEx. Another, important observation is: introducing entity type properties in the relational context can improve the performances of the original translation distance based models. Copyright {\textcopyright} 2020 The Institute of Electronics, Information and Communication Engineers},
annote = {cited By 0},
author = {Rahman, M M and Takasu, A},
doi = {10.1587/transinf.2019DAP0007},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Category specifics; Distance-based models; Entity,Embeddings; Vector spaces,Knowledge management},
number = {5},
pages = {958--968},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{Leveraging entity-type properties in the relational context for knowledge graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084844612&doi=10.1587%2Ftransinf.2019DAP0007&partnerID=40&md5=25ac87c25ee941557e58c837c3177246},
year = {2020}
}
@article{Wang2019659,
abstract = {A promising pathway for natural language question answering over knowledge graphs (KG-QA) is to translate natural language questions into graph-structured queries. During the translation, a vital process is to map entity/relation phrases of natural language questions to the vertices/edges of underlying knowledge graphs which can be used to construct target graph-structured queries. However, due to linguistic flexibility and ambiguity of natural language, the mapping process is challenging and has been a bottleneck of KG-QA models. In this paper, we propose a novel framework, called KemQA, which stands on recent advances in relation phrase dictionaries and knowledge graph embedding techniques to address the mapping problem and construct graph-structured queries of natural language questions. Extensive experiments were conducted on question answering benchmark datasets. The results demonstrate that our framework outperforms state-of-the-art baseline models in terms of effectiveness and efficiency. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 6; Conference of 24th International Conference on Database Systems for Advanced Applications, DASFAA 2019 ; Conference Date: 22 April 2019 Through 25 April 2019; Conference Code:225579},
author = {Wang, R and Wang, M and Liu, J and Chen, W and Cochez, M and Decker, S},
doi = {10.1007/978-3-030-18576-3_39},
editor = {{Natwichai J. Yang J.}, Li G Gama J Tong Y},
isbn = {9783030185756},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Benchmark datasets; Effectiveness and efficiencie,Database systems; Embeddings; Knowledge management,Natural language processing systems},
pages = {659--675},
publisher = {Springer Verlag},
title = {{Leveraging knowledge graph embeddings for natural language question answering}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065549868&doi=10.1007%2F978-3-030-18576-3_39&partnerID=40&md5=847d4a23bd13671acb5577eaec959c16},
volume = {11446 LNCS},
year = {2019}
}
@article{Wang2019382,
abstract = {Knowledge graphs (KGs) are important resources for a variety of natural language processing tasks but suffer from incompleteness. To address this challenge, a number of knowledge graph completion (KGC) methods have been developed using low-dimensional graph embeddings. Most existing methods focus on the structured information of triples in encyclopaedia KG and maximize the likelihood of them. However, they neglect semantic information contained in lexical KG. To overcome this drawback, we propose a novel KGC method (named as TransC), that integrates the structured information in encyclopaedia KG and the entity concepts in lexical KG, which describe the categories of entities. Since all entities appearing in the head (or tail) position with the same relation have some common concepts, we introduce a novel semantic similarity to measure the distinction of entity semantics with the concept information. And then TransC utilizes concept-based semantic similarity of the related entities and relations to capture prior distributions of entities and relations. With the concept-based prior distributions, TransC generates multiple embedding representations of each entity in different contexts and estimates the posterior probability of entity and relation prediction. Experimental results demonstrate the efficiency of the proposed method on two benchmark datasets. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 2; Conference of 3rd APWeb and WAIM Joint Conference on Web and Big Data, APWeb-WAIM 2019 ; Conference Date: 1 August 2019 Through 3 August 2019; Conference Code:229129},
author = {Wang, Y and Liu, Y and Zhang, H and Xie, H},
doi = {10.1007/978-3-030-26072-9_28},
editor = {{Shao J. Yiu M.L.}, Toyoda M Zhang D Wang W Cui B},
isbn = {9783030260712},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Big data; Embeddings; Natural language processing,Concept information; Knowledge graphs; NAtural la,Knowledge management},
pages = {382--397},
publisher = {Springer Verlag},
title = {{Leveraging lexical semantic information for learning concept-based multiple embedding representations for knowledge graph completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069979622&doi=10.1007%2F978-3-030-26072-9_28&partnerID=40&md5=cd6cd31782a1a4a013236ca86cb45dfe},
volume = {11641 LNCS},
year = {2019}
}
@inproceedings{Dasgupta20163476,
abstract = {Fine-grained classification is an extremely challenging problem in computer vision, compounded by subtle differences in shape, pose, illumination and appearance. While convolutional neural networks have become the versatile jack-of-all-trades tool in modern computer vision, approaches for fine-grained recognition still rely on localization of keypoints and parts to learn discriminative features for recognition. In order to achieve this, most approaches use a localization module and subsequently learn classifiers for the inferred locations, thus necessitating large amounts of manual annotations for bounding boxes and keypoints. In order to tackle this problem, we aim to leverage the (taxonomic and/or semantic) relationships present among fine-grained classes. The ontology tree is a free source of labels that can be used as auxiliary tasks to train a multi-task loss. Additional tasks can act as regularizers, and increase the generalization capabilities of the network. Multiple tasks try to take the network in diverging directions, and the network has to reach a common minimum by adapting and learning features common to all tasks in its shared layers. We train a multi-task network using auxiliary tasks extracted from taxonomical or semantic hierarchies, using a novel method to update task-wise learning rates to ensure that the related tasks aid and unrelated tasks does not hamper performance on the primary task. Experiments on the popular CUB-200-2011 dataset show that employing super-classes in an end-to-end model improves performance, compared to methods employing additional expensive annotations such as keypoints and bounding boxes and/or using multi-stage pipelines. {\textcopyright} 2016 IEEE.},
annote = {cited By 3; Conference of 23rd International Conference on Pattern Recognition, ICPR 2016 ; Conference Date: 4 December 2016 Through 8 December 2016; Conference Code:127420},
author = {Dasgupta, R and Namboodiri, A M},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7900172},
isbn = {9781509048472},
issn = {10514651},
keywords = {Computer vision,Convolutional neural network; Discriminative feat,Neural networks; Semantics},
pages = {3476--3481},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Leveraging multiple tasks to regularize fine-grained classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019080688&doi=10.1109%2FICPR.2016.7900172&partnerID=40&md5=228ab8b1efbb47e0ce14190dec404320},
volume = {0},
year = {2016}
}
@inproceedings{Liu20196746,
abstract = {Much recent work focuses on leveraging semantic lexicons like WordNet to enhance word representation learning (WRL) and achieves promising performance on many NLP tasks. However, most existing methods might have limitations because they require high-quality, manually created, semantic lexicons or linguistic structures. In this paper, we propose to leverage semantic knowledge automatically mined from web structured data to enhance WRL. We first construct a semantic similarity graph, which is referred as semantic knowledge, based on a large collection of semantic lists extracted from the web using several pre-defined HTML tag patterns. Then we introduce an efficient joint word representation learning model to capture semantics from both semantic knowledge and text corpora. Compared with recent work on improving WRL with semantic resources, our approach is more general, and can be easily scaled with no additional effort. Extensive experimental results show that our approach outperforms the state-of-the-art methods on word similarity, word sense disambiguation, text classification and textual similarity tasks. {\textcopyright} 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
annote = {cited By 0; Conference of 33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019 ; Conference Date: 27 January 2019 Through 1 February 2019; Conference Code:160302},
author = {Liu, H and Fang, L and Lou, J.-G. and Li, Z},
booktitle = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
isbn = {9781577358091},
keywords = {Classification (of information); Knowledge managem,Linguistic structure; Semantic knowledge; Semanti,Semantic Web},
pages = {6746--6753},
publisher = {AAAI Press},
title = {{Leveraging web semantic knowledge in word representation learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090801870&partnerID=40&md5=7d9d94397de65a99fb6815ae9dd5614c},
year = {2019}
}
@inproceedings{DeAssisCosta2018226,
abstract = {Entity Alignment is a research topic that has drawn a lot of attention from practitioners and researchers from many different areas in the last years. Also referred to as Entity Resolution or Instance Matching, it involves aligning different entity representations that refer to the same real-world entity. However, it is still a challenging solution and existing works are based on iterative methods, blocking schemes and learning approaches. Among these latter, recent works have been using embedding models, focusing only on structural information. In this paper, we present an approach for entity alignment that enrich entity embeddings with different literal information. To be able to express complex relationships between properties and involving multiple entities we employ FrameBase Schema, which is based on linguistic frames to create complex mappings from external knowledge bases to a unique schema. Preliminary experiments showed competitive results to the performance of entity alignment task. {\textcopyright} 2018 Association for Computing Machinery.},
annote = {cited By 2; Conference of 20th International Conference on Information Integration and Web-Based Applications and Services, iiWAS 2018 ; Conference Date: 19 November 2018 Through 21 November 2018; Conference Code:144290},
author = {{De Assis Costa}, G and de Oliveira, J M},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3282373.3282415},
editor = {{Anderst-Kotsis G. Pardede E.}, Steinbauer M Indrawan-Santiago M Salvadori I L Salvadori I L Khalil I},
isbn = {9781450364799},
keywords = {Complex relationships; Entity resolutions; Extern,Embeddings; Information retrieval; Integration; Li,Iterative methods},
pages = {226--229},
publisher = {Association for Computing Machinery},
title = {{Linguistic frames as support for entity alignment in knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061147433&doi=10.1145%2F3282373.3282415&partnerID=40&md5=aa8be8ab2da1165fb6fc6aef867aa0f4},
year = {2018}
}
@article{Karadeniz2019,
abstract = {Background: Although there is an enormous number of textual resources in the biomedical domain, currently, manually curated resources cover only a small part of the existing knowledge. The vast majority of these information is in unstructured form which contain nonstandard naming conventions. The task of named entity recognition, which is the identification of entity names from text, is not adequate without a standardization step. Linking each identified entity mention in text to an ontology/dictionary concept is an essential task to make sense of the identified entities. This paper presents an unsupervised approach for the linking of named entities to concepts in an ontology/dictionary. We propose an approach for the normalization of biomedical entities through an ontology/dictionary by using word embeddings to represent semantic spaces, and a syntactic parser to give higher weight to the most informative word in the named entity mentions. Results: We applied the proposed method to two different normalization tasks: the normalization of bacteria biotope entities through the Onto-Biotope ontology and the normalization of adverse drug reaction entities through the Medical Dictionary for Regulatory Activities (MedDRA). The proposed method achieved a precision score of 65.9%, which is 2.9 percentage points above the state-of-the-art result on the BioNLP Shared Task 2016 Bacteria Biotope test data and a macro-averaged precision score of 68.7% on the Text Analysis Conference 2017 Adverse Drug Reaction test data. Conclusions: The core contribution of this paper is a syntax-based way of combining the individual word vectors to form vectors for the named entity mentions and ontology concepts, which can then be used to measure the similarity between them. The proposed approach is unsupervised and does not require labeled data, making it easily applicable to different domains. {\textcopyright} 2019 The Author(s).},
annote = {cited By 4},
author = {Karadeniz, I and {\"{O}}zg{\"{u}}r, A},
doi = {10.1186/s12859-019-2678-8},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Adverse drug reactions; Entity categorization; En,Algorithms; Bacteria; Data Mining; Drug-Related S,Bacteria; Character recognition; Data mining; Embe,Syntactics,adverse drug reaction; algorithm; bacterium; data},
number = {1},
publisher = {BioMed Central Ltd.},
title = {{Linking entities through an ontology using word embeddings and syntactic re-ranking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063581742&doi=10.1186%2Fs12859-019-2678-8&partnerID=40&md5=76f8a8bce9040a541b0d364ab16d39dd},
volume = {20},
year = {2019}
}
@inproceedings{Zhang20193016,
abstract = {We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate”few-shot” models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that relational knowledge into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other baselines, especially for long-tail relations. {\textcopyright} 2019 Association for Computational Linguistics},
annote = {cited By 9; Conference of 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019 ; Conference Date: 2 June 2019 Through 7 June 2019; Conference Code:159851},
author = {Zhang, N and Deng, S and Sun, Z and Wang, G and Chen, X and Zhang, W and Chen, H},
booktitle = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
isbn = {9781950737130},
keywords = {Attention mechanisms; Benchmark datasets; Class d,Computational linguistics; Convolution; Embeddings,Data mining},
pages = {3016--3025},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Long-tail relation extraction via knowledge graph embeddings and graph convolution networks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085555333&partnerID=40&md5=c86d6bb110af1f2b70bac3b2301cc74f},
volume = {1},
year = {2019}
}
@article{Young2017299,
abstract = {Intelligent Autonomous Robots deployed in human environments must have understanding of the wide range of possible semantic identities associated with the spaces they inhabit – kitchens, living rooms, bathrooms, offices, garages, etc. We believe robots should learn this information through their own exploration and situated perception in order to uncover and exploit structure in their environments – structure that may not be apparent to human engineers, or that may emerge over time during a deployment. In this work, we combine semantic web-mining and situated robot perception to develop a system capable of assigning semantic categories to regions of space. This is accomplished by looking at web-mined relationships between room categories and objects identified by a Convolutional Neural Network trained on 1000 categories. Evaluated on real-world data, we show that our system exhibits several conceptual and technical advantages over similar systems, and uncovers semantic structure in the environment overlooked by ground-truth annotators. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 3; Conference of 14th International Conference on Semantic Web, ESWC 2017 ; Conference Date: 28 May 2017 Through 1 June 2017; Conference Code:204389},
author = {Young, J and Basile, V and Suchi, M and Kunze, L and Hawes, N and Vincze, M and Caputo, B},
doi = {10.1007/978-3-319-70407-4_39},
editor = {{Blomqvist E. Hartig O.}, Paulheim H Hose K Ciravegna F Lawrynowicz A},
isbn = {9783319704067},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Artificial intelligence; Convolution; Data mining;,Convolutional neural network; Imagenet; Semantic,Semantic Web},
pages = {299--313},
publisher = {Springer Verlag},
title = {{Making Sense of Indoor Spaces Using Semantic Web Mining and Situated Robot Perception}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034217729&doi=10.1007%2F978-3-319-70407-4_39&partnerID=40&md5=2ade63c47089b72b8d24abfad73b20c7},
volume = {10577 LNCS},
year = {2017}
}
@article{Li2019676,
abstract = {Measuring semantic relatedness between two words is a fundamental task for many applications in both databases and natural language processing domains. Conventional methods mainly utilize the latent semantic information hidden in lexical databases (WordNet) or text corpus (Wikipedia). They have made great achievements based on the distance computation in lexical tree or co-occurrence principle in Wikipedia. However these methods suffer from low coverage and low precision because (1) lexical database contains abundant lexical information but lacks semantic information; (2) in Wikipedia, two related words (e.g. synonyms) may not appear in a window size or a sentence, and unrelated ones may be mentioned together by chance. To compute semantic relatedness more accurately, some other approaches have made great efforts based on free association network and achieved a significant improvement on relatedness measurement. Nevertheless, they need complex preprocessing in Wikipedia. Besides, the fixed score functions they adopt cause the lack of flexibility and expressiveness of model. In this paper, we leverage DBPedia and Wikipedia to construct a Knowledge Association Network (KAN) which avoids the information extraction of Wikipedia. We propose a flexible and expressive model to represent entities behind the words, in which attribute and topological structure information of entities are embedded in vector space simultaneously. The experiment results based on standard datasets show the better effectiveness of our model compared to previous models. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 0; Conference of 24th International Conference on Database Systems for Advanced Applications, DASFAA 2019 ; Conference Date: 22 April 2019 Through 25 April 2019; Conference Code:225579},
author = {Li, J and Chen, W and Gu, B and Fang, J and Li, Z and Zhao, L},
doi = {10.1007/978-3-030-18576-3_40},
editor = {{Li G. Gama J.}, Natwichai J Yang J Tong Y},
isbn = {9783030185756},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Conventional methods; Distance computation; Knowl,Data mining; Database systems; Knowledge managemen,Semantics},
pages = {676--691},
publisher = {Springer Verlag},
title = {{Measuring semantic relatedness with knowledge association network}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065517054&doi=10.1007%2F978-3-030-18576-3_40&partnerID=40&md5=9bd4729745c05f86d140f4da03c5694d},
volume = {11446 LNCS},
year = {2019}
}
@inproceedings{Bai20194897,
abstract = {Representing words as low dimensional vectors is very useful in many natural language processing tasks. This idea has been extended to medical domain where medical codes listed in medical claims are represented as vectors to facilitate exploratory analysis and predictive modeling. However, depending on a type of a medical provider, medical claims can use medical codes from different ontologies or from a combination of ontologies, which complicates learning of the representations. To be able to properly utilize such multi-source medical claim data, we propose an approach that represents medical codes from different ontologies in the same vector space. We first modify the Pointwise Mutual Information (PMI) measure of similarity between the codes. We then develop a new negative sampling method for word2vec model that implicitly factorizes the modified PMI matrix. The new approach was evaluated on the code cross-reference problem, which aims at identifying similar codes across different ontologies. In our experiments, we evaluated cross-referencing between ICD-9 and CPT medical code ontologies. Our results indicate that vector representations of codes learned by the proposed approach provide superior cross-referencing when compared to several existing approaches. {\textcopyright} 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.},
annote = {cited By 2; Conference of 28th International Joint Conference on Artificial Intelligence, IJCAI 2019 ; Conference Date: 10 August 2019 Through 16 August 2019; Conference Code:153611},
author = {Bai, T and Egleston, B L and Bleicher, R and Vucetic, S},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2019/680},
editor = {S., Kraus},
isbn = {9780999241141},
issn = {10450823},
pages = {4897--4903},
publisher = {International Joint Conferences on Artificial Intelligence},
title = {{Medical concept representation learning from multi-source data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074926942&doi=10.24963%2Fijcai.2019%2F680&partnerID=40&md5=baaf598bff9dc0c6d0998efe8288deeb},
volume = {2019-Augus},
year = {2019}
}
@inproceedings{Xu2018903,
abstract = {This paper addresses the problem of multi-instance entity typing from corpus. Current approaches mainly rely on the structured features (attributes, attribute-value pairs and tags) of the entities. However, their effectiveness is largely dependent on the completeness of structured features, which unfortunately is not guaranteed in KBs. In this paper, we therefore propose to use the text corpus of an entity to infer its types, and propose a multi-instance method to tackle this problem. We take each mention of an entity in KBs as an instance of the entity, and learn the types of these entities from multiple instances. Specifically, we first use an end-to-end neural network model to type each instance of an entity, and then use an integer linear programming (ILP) method to aggregate the predicted type results from multiple instances. Experimental results show the effectiveness of our method. {\textcopyright} 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
annote = {cited By 4; Conference of 27th ACM International Conference on Information and Knowledge Management, CIKM 2018 ; Conference Date: 22 October 2018 Through 26 October 2018; Conference Code:142310},
author = {Xu, B and Luo, Z and Huang, L and Liang, B and Xiao, Y and Yang, D and Wang, W},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3269206.3271804},
editor = {{Paton N. Candan S.}, Wang H Allan J Agrawal R Labrinidis A Cuzzocrea A Zaki M Srivastava D Broder A Schuster A},
isbn = {9781450360142},
keywords = {Attribute-value pairs; Entity typing; Integer Lin,Integer programming,Knowledge management},
pages = {903--912},
publisher = {Association for Computing Machinery},
title = {{METIC: Multi-instance entity typing from corpus}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058052432&doi=10.1145%2F3269206.3271804&partnerID=40&md5=ef16b712264128d9ef9db8d42b24d6f0},
year = {2018}
}
@article{Kaffee2018319,
abstract = {While Wikipedia exists in 287 languages, its content is unevenly distributed among them. It is therefore of utmost social and cultural importance to focus efforts on languages whose speakers only have access to limited Wikipedia content. We investigate supporting communities by generating summaries for Wikipedia articles in underserved languages, given structured data as an input. We focus on an important support for such summaries: ArticlePlaceholders, a dynamically generated content pages in underserved Wikipedias. They enable native speakers to access existing information in Wikidata. To extend those ArticlePlaceholders, we provide a system, which processes the triples of the KB as they are provided by the ArticlePlaceholder, and generate a comprehensible textual summary. This data-driven approach is employed with the goal of understanding how well it matches the communities' needs on two underserved languages on the Web: Arabic, a language with a big community with disproportionate access to knowledge online, and Esperanto, an easily-acquainted, artificial language whose Wikipedia content is maintained by a small but devoted community. With the help of the Arabic and Esperanto Wikipedians, we conduct a study which evaluates not only the quality of the generated text, but also the usefulness of our end-system to any underserved Wikipedia version. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
annote = {cited By 4; Conference of 15th International Conference on Extended Semantic Web Conference, ESWC 2018 ; Conference Date: 3 June 2018 Through 7 June 2018; Conference Code:214029},
author = {Kaffee, L.-A. and Elsahar, H and Vougiouklis, P and Gravier, C and Laforest, F and Hare, J and Simperl, E},
doi = {10.1007/978-3-319-93417-4_21},
editor = {{Gangemi A. Troncy R.}, Navigli R Hollink L Vidal M Hitzler P Tordai A Alam M},
isbn = {9783319934167},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Arabic; Esperanto; Multilinguality; Natural langu,Natural language processing systems; Neural networ,Semantic Web},
pages = {319--334},
publisher = {Springer Verlag},
title = {{Mind the (Language) Gap: Generation of Multilingual Wikipedia Summaries from Wikidata for ArticlePlaceholders}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048490979&doi=10.1007%2F978-3-319-93417-4_21&partnerID=40&md5=c88ee076522d7b1d985eaea23938e4bb},
volume = {10843 LNCS},
year = {2018}
}
@inproceedings{8983022,
abstract = {The Disease Ontology (DO) is standardized, controlled vocabulary that contains information about inherited, developmental and acquired human diseases. Each DO term is associated with disease concepts through an annotation process. The relevance and the specificity of DO terms are often evaluated by its Information Content (IC). An important research area focus on the analysis of annotated data with the goal to extract knowledge. For example, the analysis of annotated data using Association Rules (AR) may supply meaningful knowledge, discovering relevant associations. Classical association rules methods consider all annotation equally, do not taking into account that the DO terms have different Information Content, i.e. different relevance. This implies the generation of association rules with low IC. In this paper we presents WARDO (Weighted Association Rule mining from Disease Ontology), a methodology based on the extraction od Weighted Association Rules from the DO Ontology considering the IC of terms. To assess our methodology, we tested WARDO on DO annotation datasets. WARDO is publicly available at https://gitlab.com/giuseppeagapito/wardo.},
author = {Agapito, G and Milano, M and Guzzi, P H and Cannataro, M},
booktitle = {2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
keywords = {data mining;diseases;medical computing;ontologies},
month = {nov},
pages = {2239--2243},
title = {{Mining Association Rules From Disease Ontology}},
year = {2019}
}
@inproceedings{8217917,
abstract = {An ontology is a framework for describing domain-specific knowledge in a structured format. It is comprised of a set of terms as nodes and a set of relationships between terms as directed edges to form a directed acyclic graph. Gene Ontology (GO) and Human Phenotype Ontology (HPO) are widely referred biological and biomedical ontology databases. They also provide extensive annotations of human genes. Recent studies have applied association rule mining techniques to these ontology and annotation data in order to predict cellular functions of genes and determine gene-to-disease relations. We present a new approach to extract pairwise association rules between specific terms. Our approach selects significant, specific rules by weighted measures of support, confidence and coverage which incorporate weighting terms by integration of the ontology structure and their information contents. In our experiment, the cross-ontology association rules generated from GO and HPO by our approach were compared to those by two previous methods. The results show that our approach discovers association rules between more specific terms than the previous methods.},
author = {Huang, J and Rapp, C and Cho, Y},
booktitle = {2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
doi = {10.1109/BIBM.2017.8217917},
keywords = {biology computing;data mining;directed graphs;dise},
month = {nov},
pages = {1706--1711},
title = {{Mining cross-ontology weighted association rules between GO and HPO}},
year = {2017}
}
@inproceedings{10.1109/BIBE.2012.6399704,
abstract = {Ontology has become a very vital issue to solve important issues regarding human diseases through data integration of chemical and biological data. Mining such data discovers highly important knowledge about diseases can give an important insight to arrive to new drug targets and assist in personalized medicine. In the current paper, a mining technique for diseases is developed based on integrated ontology and association rule mining algorithm. To perform mining, the semantic web, as a knowledge representation methodology is used to integrate data. In addition, an Ontology Association Rule Mining algorithm (OARM) is developed since existing algorithms cannot be applied because of the ontology nature of data containing several types of relations. To test our performance, prostate cancer data is obtained from NCI, which is related to 279 genes and 89 genes (from prostate cancer pathway).},
address = {USA},
author = {El-Sharkawi, Mohamed and Hussein, Marwa and Soliman, Taysir Hassan A},
booktitle = {Proceedings of the 2012 IEEE 12th International Conference on Bioinformatics \& Bioengineering (BIBE)},
doi = {10.1109/BIBE.2012.6399704},
isbn = {9781467343572},
keywords = {Association Rule Mining,Association rules,Databases,Disease-related information ontology,Diseases,Gene Ontology,Humans,Ontologies,Semantic Web},
pages = {40--45},
publisher = {IEEE Computer Society},
series = {BIBE '12},
title = {{Mining Disease Integrated Ontology}},
url = {https://doi.org/10.1109/BIBE.2012.6399704},
year = {2012}
}
@article{Faria2012,
abstract = {Despite the structure and objectivity provided by the Gene Ontology (GO), the annotation of proteins is a complex task that is subject to errors and inconsistencies. Electronically inferred annotations in particular are widely considered unreliable. However, given that manual curation of all GO annotations is unfeasible, it is imperative to improve the quality of electronically inferred annotations. In this work, we analyze the full GO molecular function annotation of UniProtKB proteins, and discuss some of the issues that affect their quality, focusing particularly on the lack of annotation consistency. Based on our analysis, we estimate that 64% of the UniProtKB proteins are incompletely annotated, and that inconsistent annotations affect 83% of the protein functions and at least 23% of the proteins. Additionally, we present and evaluate a data mining algorithm, based on the association rule learning methodology, for identifying implicit relationships between molecular function terms. The goal of this algorithm is to assist GO curators in updating GO and correcting and preventing inconsistent annotations. Our algorithm predicted 501 relationships with an estimated precision of 94%, whereas the basic association rule learning methodology predicted 12,352 relationships with a precision below 9%. {\textcopyright} 2012 Faria et al.},
annote = {cited By 26},
author = {Faria, D and Schlicker, A and Pesquita, C and Bastos, H and FerreiraAnt{\'{o}}, A E N and Albrecht, M and Falc{\~{a}}o, A O},
doi = {10.1371/journal.pone.0040519},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Databases,Protein; Molecular Sequence Annotation; Sequence,Protein; Software,accuracy; algorithm; article; association rule lea},
number = {7},
title = {{Mining GO annotations for improving annotation consistency}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864674779&doi=10.1371%2Fjournal.pone.0040519&partnerID=40&md5=b7506ee4d7054981facda179689fcc50},
volume = {7},
year = {2012}
}
@inproceedings{Wong2011707,
abstract = {Ontology consists of concepts, taxonomic relations and non-taxonomic relations. The majority of the ontology learning tools focus on discovering concepts and taxonomic relations. Very little effort has been put on discovering non-taxonomic relations. In this paper, we present a concept correlation search framework to discover non-taxonomic concept pairs from unstructured text. Our framework features the (a) extraction of correlated concepts beyond ordinary search window size of a single sentence; (b) use of lift as interestingness measure for association rule mining; (c) harness of 2-itemsets association rules from n-itemsets association rules where n>2; and (d) identification of non-taxonomic concept pairs based on existing domain ontology. The proposed framework has been tested with the Fisheries Oceanography journals, and the results demonstrate significant improvements over traditional association rule approach in search of non-taxonomic concept pairs.},
address = {Noordwijkerhout},
annote = {cited By 2; Conference of 7th International Conference on Web Information Systems and Technologies, WEBIST 2011 ; Conference Date: 6 May 2011 Through 9 May 2011; Conference Code:86383},
author = {Wong, M K and Abidi, S S R and Jonsen, I D},
booktitle = {WEBIST 2011 - Proceedings of the 7th International Conference on Web Information Systems and Technologies},
isbn = {9789898425515},
keywords = {Association rule mining; Concept correlation; Doma,Association rules; Data mining; Information syste,Search engines},
pages = {707--716},
title = {{Mining non-taxonomic concept pairs from unstructured text: A concept correlation search framework}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052589578&partnerID=40&md5=4a5aeda2b6a7f5749970bcc020368cf6},
year = {2011}
}
@inproceedings{Buscaldi201921,
abstract = {Knowledge graphs (KG) are large networks of entities and relationships, typically expressed as RDF triples, relevant to a specific domain or an organization. Scientific Knowledge Graphs (SKGs) focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. The next big challenge in this field regards the generation of SKGs that also contain an explicit representation of the knowledge presented in research publications. In this paper, we present a preliminary approach that uses a set of NLP and Deep Learning methods for extracting entities and relationships from research publications, and then integrates them in a KG. More specifically, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) analyze an automatically generated Knowledge Graph including 10 425 entities and 25 655 relationships derived from 12 007 publications in the field of Semantic Web, and iv) discuss some open problems that have not been solved yet. {\textcopyright} 2019 CEUR-WS. All rights reserved.},
annote = {cited By 0; Conference of 2019 Workshop on Deep Learning for Knowledge Graphs, DL4KG 2019 ; Conference Date: 2 June 2019; Conference Code:148550},
author = {Buscaldi, D and Dess{\`{i}}, D and Motta, E and Osborne, F and Recupero, D R},
booktitle = {CEUR Workshop Proceedings},
editor = {{Alam M. Buscaldi D.}, Cochez M Osborne F Recupero D R Sack H},
issn = {16130073},
keywords = {Automatically generated; Explicit representation;,Data mining,Data reduction; Deep learning; Extraction; Graphic},
pages = {21--30},
publisher = {CEUR-WS},
title = {{Mining scholarly data for fine-grained knowledge graph construction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067893695&partnerID=40&md5=967dee119fdc7b631fbde9a4e7380e9f},
volume = {2377},
year = {2019}
}
@article{10.1016/j.knosys.2017.07.009,
abstract = {The Semantic Web opens up new opportunities for the data mining research. Semantic Web data is usually represented in the RDF triple format (subject, predicate, object). Large RDF-style Knowledge Bases contain hundreds of millions of RDF triples that represent knowledge in a machine-understandable format. Association rule mining is one of the most effective techniques for detecting frequent patterns. In the context of Semantic Web data mining, most existing methods rely on users intervention that is time-consuming and error-prone due to a large amount of data. Meanwhile, rule quality factors (e.g. support and confidence) usually consider knowledge at the instance-level. Namely, these factors disregard the knowledge embedded at the schema-level. In this paper, we demonstrate that ignoring knowledge encoded at the schema-level negatively impacts the interpretation of discovered rules. We introduce an approach called SWARM (Semantic Web Association Rule Mining) that automatically mines Semantic Association Rules from RDF data. The main achievement of SWARM is to reveal common behavioural patterns associated with knowledge at the instance-level and schema-level. We discuss how to utilize knowledge encoded at the schema-level to add more semantics to the rules. We compare the semantic of rules discovered by SWRAM with one of the latest approaches in this field to show the importance of considering schema-level knowledge. Initial experiments performed on RDF-style Knowledge Bases demonstrate the effectiveness of the proposed approach.},
address = {NLD},
author = {Barati, Molood and Bai, Quan and Liu, Qing},
doi = {10.1016/j.knosys.2017.07.009},
issn = {0950-7051},
journal = {Know.-Based Syst.},
keywords = {Association rule mining,Knowledge discovery,Ontology,Semantic Web data},
month = {oct},
number = {C},
pages = {183--196},
publisher = {Elsevier Science Publishers B. V.},
title = {{Mining Semantic Association Rules from RDF Data}},
url = {https://doi.org/10.1016/j.knosys.2017.07.009},
volume = {133},
year = {2017}
}
@inproceedings{Sethi201873,
abstract = {Association Rule Mining has so far focused on generating and pruning positive rules using various interestingness measures. However, there are very few studies that explore the mining process of substitution rules. These studies have incorporated a limited definition of substitution, either in statistical terms or based on manager's static knowledge. Here we attempt to provide a customer-centric model of substitution rule mining using the lens of affordance. We adopt a knowledge-based approach involving a dynamic ontology wherein objects are positioned based on the affordances they are preferred for. This contrasts with the traditional static ontology approach that highlights manager's static knowledge base. We develop an Expected-Actual Substitution Framework to compare relatedness between items in the static and dynamic ontologies. We present Affordance-Based Substitution (ABS) algorithm to mine substitution rules based on the proposed approach. We also come up with a novel interestingness measure that enhances the quality of our substitution rules thus leading to effective knowledge discovery. Empirical analyses are performed on a real-life supermarket dataset to show the efficacy of ABS algorithm. We compare the generated rules with those generated by another substitution rule mining algorithm from the literature. Our results show that substitution rules generated through ABS algorithm capture customer perceptions that are generally missed by alternate approaches. Copyright 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.},
annote = {cited By 0; Conference of 10th International Conference on Agents and Artificial Intelligence, ICAART 2018 ; Conference Date: 16 January 2018 Through 18 January 2018; Conference Code:134807},
author = {Sethi, R and Shekar, B},
booktitle = {ICAART 2018 - Proceedings of the 10th International Conference on Agents and Artificial Intelligence},
doi = {10.5220/0006577400730084},
editor = {{Rocha A.P.}, van den Herik J},
isbn = {9789897582752},
keywords = {Affordances; Alternate approaches; Customer perce,Artificial intelligence; Association rules; Data m,Knowledge based systems},
pages = {73--84},
publisher = {SciTePress},
title = {{Mining substitution rules: A knowledge-based approach using dynamic ontologies}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046683225&doi=10.5220%2F0006577400730084&partnerID=40&md5=ea832e8fb588ee20d701bc9d60699f17},
volume = {2},
year = {2018}
}
@article{Schlichtkrull2018593,
abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
annote = {cited By 220; Conference of 15th International Conference on Extended Semantic Web Conference, ESWC 2018 ; Conference Date: 3 June 2018 Through 7 June 2018; Conference Code:214029},
author = {Schlichtkrull, M and Kipf, T N and Bloem, P and van den Berg, R and Titov, I and Welling, M},
doi = {10.1007/978-3-319-93417-4_38},
editor = {{Gangemi A. Troncy R.}, Navigli R Hollink L Vidal M Hitzler P Tordai A Alam M},
isbn = {9783319934167},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Computer system recovery; Convolution; Knowledge b,Convolutional networks; Factorization model; Know,Semantic Web},
pages = {593--607},
publisher = {Springer Verlag},
title = {{Modeling Relational Data with Graph Convolutional Networks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048485418&doi=10.1007%2F978-3-319-93417-4_38&partnerID=40&md5=ee4cdbdbd1539c2f24888dc14a7cc339},
volume = {10843 LNCS},
year = {2018}
}
@article{Zhu2018323,
abstract = {Knowledge graph embedding, which maps the entities and relations into low-dimensional vector spaces, has demonstrated its effectiveness in many tasks such as link prediction and relation extraction. Typical methods include TransE, TransH, and TransR. All these methods map different relations into the vector space separately and the intrinsic correlations of these relations are ignored. It is obvious that there exist some correlations among relations because different relations may connect to a common entity. For example, the triples (Steve Jobs, PlaceOfBrith, California) and (Apple Inc., Location, California) share the same entity California as their tail entity. We analyze the embedded relation matrices learned by TransE/TransH/TransR, and find that the correlations of relations do exist and they are showed as low-rank structure over the embedded relation matrix. It is natural to ask whether we can leverage these correlations to learn better embeddings for the entities and relations in a knowledge graph. In this paper, we propose to learn the embedded relation matrix by decomposing it as a product of two low-dimensional matrices, for characterizing the low-rank structure. The proposed method, called TransCoRe (Translation-Based Method via Modeling the Correlations of Relations), learns the embeddings of entities and relations with translation-based framework. Experimental results based on the benchmark datasets of WordNet and Freebase demonstrate that our method outperforms the typical baselines on link prediction and triple classification tasks. {\textcopyright} 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
annote = {cited By 7},
author = {Zhu, J.-Z. and Jia, Y.-T. and Xu, J and Qiao, J.-Z. and Cheng, X.-Q.},
doi = {10.1007/s11390-018-1821-8},
issn = {10009000},
journal = {Journal of Computer Science and Technology},
keywords = {Benchmark datasets; Classification tasks; Knowled,Classification (of information); Decomposition; Ve,Knowledge management},
number = {2},
pages = {323--334},
publisher = {Springer New York LLC},
title = {{Modeling the Correlations of Relations for Knowledge Graph Embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044450883&doi=10.1007%2Fs11390-018-1821-8&partnerID=40&md5=a7bdf10d08fd693411ca25386bd702e0},
volume = {33},
year = {2018}
}
@article{Tan2020424,
abstract = {Knowledge graph embedding models aim to represent entities and relations in continuous low-dimensional vector space, benefiting many research areas such as knowledge graph completion and web searching. However, previous works do not consider controlling information flow, which makes them hard to obtain useful latent information and limits model performance. Specifically, as human beings, predictions are usually made in multiple steps with every step filtering out irrelevant information and targeting at helpful information. In this paper, we first integrate iterative mechanism into knowledge graph embedding and propose a multi-step gated model which utilizes relations as queries to extract useful information from coarse to fine in multiple steps. First gate mechanism is adopted to control information flow by the interaction between entity and relation with multiple steps. Then we repeat the gate cell for several times to refine the information incrementally. Our model achieves state-of-the-art performance on most benchmark datasets compared to strong baselines. Further analyses demonstrate the effectiveness of our model and its scalability on large knowledge graphs. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2020 ; Conference Date: 11 May 2020 Through 14 May 2020; Conference Code:240129},
author = {Tan, C and Yang, K and Dai, X and Huang, S and Chen, J},
doi = {10.1007/978-3-030-47426-3_33},
editor = {{Lauw H.W. Lim E.-P.}, Wong R.C.-W. Ntoulas A Ng S.-K. Pan S J},
isbn = {9783030474256},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Benchmark datasets; Control information; Informat,Benchmarking; Embeddings; Information filtering; V,Data mining},
pages = {424--435},
publisher = {Springer},
title = {{MSGE: A Multi-step Gated Model for Knowledge Graph Completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085730699&doi=10.1007%2F978-3-030-47426-3_33&partnerID=40&md5=254a020f48a9721cf8100d5e7ff5e58b},
volume = {12084 LNAI},
year = {2020}
}
@inproceedings{Lin20203243,
abstract = {Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 11; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Lin, X V and Socher, R and Xiong, C},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {Action sequences; Benchmark datasets; Effective a,Embeddings; Reinforcement learning,Natural language processing systems},
pages = {3243--3253},
publisher = {Association for Computational Linguistics},
title = {{Multi-hop knowledge graph reasoning with reward shaping}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081715825&partnerID=40&md5=e1a964431bd28a2432bd7b51c780140a},
year = {2020}
}
@inproceedings{Lee20181576,
abstract = {In this paper, we propose a novel deep learning architecture for multi-label zero-shot learning (ML-ZSL), which is able to predict multiple unseen class labels for each input instance. Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a framework that incorporates knowledge graphs for describing the relationships between multiple labels. Our model learns an information propagation mechanism from the semantic label space, which can be applied to model the interdependencies between seen and unseen class labels. With such investigation of structured knowledge graphs for visual reasoning, we show that our model can be applied for solving multi-label classification and ML-ZSL tasks. Compared to state-of-the-art approaches, comparable or improved performances can be achieved by our method. {\textcopyright} 2018 IEEE.},
annote = {cited By 46; Conference of 31st Meeting of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2018 ; Conference Date: 18 June 2018 Through 22 June 2018; Conference Code:143811},
author = {Lee, C.-W. and Fang, W and Yeh, C.-K. and Wang, Y.-C.F.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00170},
isbn = {9781538664209},
issn = {10636919},
keywords = {Classification (of information); Computer vision;,Deep learning,Information propagation; Knowledge graphs; Learni},
pages = {1576--1585},
publisher = {IEEE Computer Society},
title = {{Multi-label Zero-Shot Learning with Structured Knowledge Graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061190897&doi=10.1109%2FCVPR.2018.00170&partnerID=40&md5=61a3202509558396f7c5021bc8aaccd0},
year = {2018}
}
@inproceedings{8725669,
abstract = {Deep neural networks can obtain effective hierarchical representations, which make them perform well in image recognition and speech recognition. However, the disadvantages of deep learning relying on large-scale annotation data also limit its development. To this end, a multi-layer convolutional neural network based on knowledge graph prior knowledge is proposed in this paper, which acquires text features from finer granularity, and using the existing prior knowledge which increases granularity information to enhance the semantic information of the text, and to reduce the dependence of the model on large-scale samples. At the same time, it can reduce the over-fitting problem of the model, and improve classification accuracy. This model performs well in several large data sets. By introducing prior knowledge to TextCNN, a classical deep neural model, it has also verified the validity of using prior knowledge through experiments.},
author = {Meng, Y and Wang, G and Liu, Q},
booktitle = {2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)},
doi = {10.1109/ICCCBDA.2019.8725669},
keywords = {convolutional neural nets;feature extraction;image},
month = {apr},
pages = {618--624},
title = {{Multi-layer Convolutional Neural Network Model Based on Prior Knowledge of Knowledge Graph for Text Classification}},
year = {2019}
}
@article{Jiang2020,
abstract = {Objective: Currently, a major limitation for natural language processing (NLP) analyses in clinical applications is that concepts are not effectively referenced in various forms across different texts. This paper introduces Multi-Ontology Refined Embeddings (MORE), a novel hybrid framework that incorporates domain knowledge from multiple ontologies into a distributional semantic model, learned from a corpus of clinical text. Materials and Methods: We use the RadCore and MIMIC-III free-text datasets for the corpus-based component of MORE. For the ontology-based part, we use the Medical Subject Headings (MeSH) ontology and three state-of-the-art ontology-based similarity measures. In our approach, we propose a new learning objective, modified from the sigmoid cross-entropy objective function. Results and Discussion: We used two established datasets of semantic similarities among biomedical concept pairs to evaluate the quality of the generated word embeddings. On the first dataset with 29 concept pairs, with similarity scores established by physicians and medical coders, MORE's similarity scores have the highest combined correlation (0.633), which is 5.0% higher than that of the baseline model, and 12.4% higher than that of the best ontology-based similarity measure. On the second dataset with 449 concept pairs, MORE's similarity scores have a correlation of 0.481, based on the average of four medical residents' similarity ratings, and that outperforms the skip-gram model by 8.1%, and the best ontology measure by 6.9%. Furthermore, MORE outperforms three pre-trained transformer-based word embedding models (i.e., BERT, ClinicalBERT, and BioBERT) on both datasets. Conclusion: MORE incorporates knowledge from several biomedical ontologies into an existing corpus-based distributional semantics model, improving both the accuracy of the learned word embeddings and the extensibility of the model to a broader range of biomedical concepts. MORE allows for more accurate clustering of concepts across a wide range of applications, such as analyzing patient health records to identify subjects with similar pathologies, or integrating heterogeneous clinical data to improve interoperability between hospitals. {\textcopyright} 2020},
annote = {cited By 0},
author = {Jiang, S and Wu, W and Tomita, N and Ganoe, C and Hassanpour, S},
doi = {10.1016/j.jbi.2020.103581},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Biomedical engineering; Embeddings; Knowledge mana,Biomedical ontologies; Clinical application; Dist,Ontology,adult; article; embedding; entropy; human; learni},
publisher = {Academic Press Inc.},
title = {{Multi-Ontology Refined Embeddings (MORE): A hybrid multi-ontology and corpus-based semantic representation model for biomedical concepts}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092201431&doi=10.1016%2Fj.jbi.2020.103581&partnerID=40&md5=fa18e50dcf000043c046e0c07bff31a5},
volume = {111},
year = {2020}
}
@inproceedings{Tay20171029,
abstract = {Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete a.ributes for each entity. Intuitively, these a.ributes such as height, price or population count are able to richly characterize entities in knowledge graphs. .is additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete a.ribute information in a relational setting. Specifically, we train a neural network for triplet prediction along with a separate network for a.ribute value regression. Via multi-task learning, we are able to learn representations of entities, relations and a.ributes that encode information about both tasks. Moreover, such a.ributes are not only central to many predictive tasks as an information source but also as a prediction target. Therefore, models that are able to encode, incorporate and predict such information in a relational learning context are highly a.ractive as well. We show that our approach outperforms many state-of Theart methods for the tasks of relational triplet classi.cation and a.ribute value prediction. {\textcopyright} 2017 Copyright held by the owner/author(s).},
annote = {cited By 5; Conference of 26th ACM International Conference on Information and Knowledge Management, CIKM 2017 ; Conference Date: 6 November 2017 Through 10 November 2017; Conference Code:131841},
author = {Tay, Y and Tuan, L A and Phan, M C and Hui, S C},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3132847.3132937},
isbn = {9781450349185},
keywords = {Deep learning; Encoding (symbols); Forecasting; Kn,Discrete attributes; Discrete data; Information s,Graphic methods},
pages = {1029--1038},
publisher = {Association for Computing Machinery},
title = {{Multi-task neural network for non-discrete attribute prediction in knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037348309&doi=10.1145%2F3132847.3132937&partnerID=40&md5=1d59aabd761e27563048b9545350046e},
volume = {Part F1318},
year = {2017}
}
@article{Wu2019185,
abstract = {Named Entity Recognition (NER) for open domain data is a critical task for the natural language process applications and attracts many research attention. However, the complexity of semantic dependencies and the sparsity of the context information make it difficult for identifying correct entities from the corpus. In addition, the lack of annotated training data makes impossible the prediction of fine-grained entity types for detected entities. To solve the above-mentioned problems in NER, we propose an extractor which takes both the near arguments and long dependencies of relations into consideration for the entities and relations mention discovery. We then employ distant-supervision methods to automatically label mention types of training data sets and a neural network model is proposed for learning the type classifier. Empirical studies on two real-world raw text corpus, NYT and YELP, demonstrate that our proposed NER approach outperforms the existing models. {\textcopyright} Springer Nature Singapore Pte Ltd 2019.},
annote = {cited By 0; Conference of 4th China Conference on Knowledge Graph and Semantic Computing, CCKS 2019 ; Conference Date: 24 August 2019 Through 27 August 2019; Conference Code:235759},
author = {Wu, J and Zhang, R and Deng, T and Huai, J},
doi = {10.1007/978-981-15-1956-7_17},
editor = {{Zhu X. Qin B.}, Liu M Zhu X Qian L},
isbn = {9789811519550},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Annotated training data; Context information; Dis,Classification (of information); Information retri,Open Data},
pages = {185--197},
publisher = {Springer},
title = {{Named Entity Recognition for Open Domain Data Based on Distant Supervision}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078502380&doi=10.1007%2F978-981-15-1956-7_17&partnerID=40&md5=65c5b8404c1b0970de81728c76836161},
volume = {1134 CCIS},
year = {2019}
}
@article{Jin2019537,
abstract = {Named entity recognition in Traditional Chinese Medicine (TCM) clinical cases is a fundamental and crucial task for follow-up work. In recent years, deep learning approaches have achieved remarkable results in named entity recognition and other natural language processing tasks. However, these methods cannot effectively solve the problem of low recognition rate of rare words, which is common in TCM field. In this paper, we propose TCMKG-LSTM-CRF model that utilizes knowledge graph information to strength the learning ability and recognize rare words. This model introduces knowledge attention vector model to implement attention mechanism between hidden vector of neural networks and knowledge graph candidate vectors and consider influence from previous word. The experiment results prove the effectiveness of our model. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 1; Conference of 12th International Conference on Knowledge Science, Engineering and Management, KSEM 2019 ; Conference Date: 28 August 2019 Through 30 August 2019; Conference Code:230379},
author = {Jin, Z and Zhang, Y and Kuang, H and Yao, L and Zhang, W and Pan, Y},
doi = {10.1007/978-3-030-29551-6_48},
editor = {{Douligeris C. Apostolou D.}, Karagiannis D},
isbn = {9783030295509},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attention mechanisms; Hidden vectors; Knowledge g,Deep learning; Medicine; Natural language processi,Long short-term memory},
pages = {537--548},
publisher = {Springer},
title = {{Named Entity Recognition in Traditional Chinese Medicine Clinical Cases Combining BiLSTM-CRF with Knowledge Graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081618149&doi=10.1007%2F978-3-030-29551-6_48&partnerID=40&md5=bd7a0b9b206d53f25998ca9ffb7f9554},
volume = {11775 LNAI},
year = {2019}
}
@inproceedings{Li201743,
abstract = {This work focuses on the relation matching problem in knowledge based question answering systems. Finding the right relation a natural question asks is a key step in current knowledge based question answering systems, while also being the most difficult one, because of the mismatch between natural language question and formal relation type definitions. In this paper, we present two approaches to tackle this problem. The first approach tries to directly learn the soft match between the question and the relations from the training data using neural networks. The second approach enriches the relation name with natural language support sentences generated from Wikipedia, which provide additional matches with the question. Experiments on the WebQuestions dataset demonstrate that both of our approaches improve the relation matching accuracy of a prior state-of-the-art. Our further analysis reveals the high quality of support sentences and suggests the rich potential of support sentences in question answering and semantic parsing tasks. {\textcopyright} Copyright by the paper's authors.},
annote = {cited By 1; Conference of 1st Workshop on Knowledge Graphs and Semantics for Text Retrieval and Analysis, KG4IR 2017 ; Conference Date: 11 August 2017; Conference Code:129401},
author = {Li, H and Xiong, C and Callan, J},
booktitle = {CEUR Workshop Proceedings},
editor = {{Dietz L. Xiong C.}, Meij E},
issn = {16130073},
keywords = {Artificial intelligence; Information retrieval; Kn,Knowledge graphs; Matching problems; Natural lang,Natural language processing systems},
pages = {43--48},
publisher = {CEUR-WS},
title = {{Natural language supported relation matching for question answering with knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027838167&partnerID=40&md5=2f652c6cd299ee904003f9938791ea98},
volume = {1883},
year = {2017}
}
@article{Lashkari2019733,
abstract = {Traditional information retrieval techniques that primarily rely on keyword-based linking of the query and document spaces face challenges such as the vocabulary mismatch problem where relevant documents to a given query might not be retrieved simply due to the use of different terminology for describing the same concepts. As such, semantic search techniques aim to address such limitations of keyword-based retrieval models by incorporating semantic information from standard knowledge bases such as Freebase and DBpedia. The literature has already shown that while the sole consideration of semantic information might not lead to improved retrieval performance over keyword-based search, their consideration enables the retrieval of a set of relevant documents that cannot be retrieved by keyword-based methods. As such, building indices that store and provide access to semantic information during the retrieval process is important. While the process for building and querying keyword-based indices is quite well understood, the incorporation of semantic information within search indices is still an open challenge. Existing work have proposed to build one unified index encompassing both textual and semantic information or to build separate yet integrated indices for each information type but they face limitations such as increased query process time. In this paper, we propose to use neural embeddings-based representations of term, semantic entity, semantic type and documents within the same embedding space to facilitate the development of a unified search index that would consist of these four information types. We perform experiments on standard and widely used document collections including Clueweb09-B and Robust04 to evaluate our proposed indexing strategy from both effectiveness and efficiency perspectives. Based on our experiments, we find that when neural embeddings are used to build inverted indices; hence relaxing the requirement to explicitly observe the posting list key in the indexed document: (a) retrieval efficiency will increase compared to a standard inverted index, hence reduces the index size and query processing time, and (b) while retrieval efficiency, which is the main objective of an efficient indexing mechanism improves using our proposed method, retrieval effectiveness also retains competitive performance compared to the baseline in terms of retrieving a reasonable number of relevant documents from the indexed corpus. {\textcopyright} 2018 Elsevier Ltd},
annote = {cited By 3},
author = {Lashkari, F and Bagheri, E and Ghorbani, A A},
doi = {10.1016/j.ipm.2018.10.015},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Competitive performance; Effectiveness and effici,Efficiency; Indexing (materials working); Indexing,Semantic Web},
number = {3},
pages = {733--755},
publisher = {Elsevier Ltd},
title = {{Neural embedding-based indices for semantic search}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059802431&doi=10.1016%2Fj.ipm.2018.10.015&partnerID=40&md5=4cd69a79d88cb88a739dea4627e15cc4},
volume = {56},
year = {2019}
}
@article{Vougiouklis20181,
abstract = {Most people need textual or visual interfaces in order to make sense of Semantic Web data. In this paper, we investigate the problem of generating natural language summaries for Semantic Web data using neural networks. Our end-to-end trainable architecture encodes the information from a set of triples into a vector of fixed dimensionality and generates a textual summary by conditioning the output on the encoded vector. We explore a set of different approaches that enable our models to verbalise entities from the input set of triples in the generated text. Our systems are trained and evaluated on two corpora of loosely aligned Wikipedia snippets with triples from DBpedia and Wikidata, with promising results. {\textcopyright} 2018 The Authors},
annote = {cited By 11},
author = {Vougiouklis, P and Elsahar, H and Kaffee, L.-A. and Gravier, C and Laforest, F and Hare, J and Simperl, E},
doi = {10.1016/j.websem.2018.07.002},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Dbpedia; End to end; Input set; Knowledge base; N,Knowledge based systems; Natural language processi,Semantic Web},
pages = {1--15},
publisher = {Elsevier B.V.},
title = {{Neural Wikipedian: Generating Textual Summaries from Knowledge Base Triples}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054144728&doi=10.1016%2Fj.websem.2018.07.002&partnerID=40&md5=68f4db457b1d7359539bc6c5f60aed4f},
volume = {52-53},
year = {2018}
}
@inproceedings{10.1145/3209978.3210150,
abstract = {Beyond word embeddings, continuous representations of knowledge graph (KG) components, such as entities, types and relations, are widely used for entity mention disambiguation, relation inference and deep question answering. Great strides have been made in modeling general, asymmetric or antisymmetric KG relations using Gaussian, holographic, and complex embeddings. None of these directly enforce transitivity inherent in the is-instance-of and is-subtype-of relations. A recent proposal, called order embedding (OE), demands that the vector representing a subtype elementwise dominates the vector representing a supertype. However, the manner in which such constraints are asserted and evaluated have some limitations. In this short research note, we make three contributions specific to representing and inferring transitive relations. First, we propose and justify a significant improvement to the OE loss objective. Second, we propose a new representation of types as hyper-rectangular regions, that generalize and improve on OE. Third, we show that some current protocols to evaluate transitive relation inference can be misleading, and offer a sound alternative. Rather than use black-box deep learning modules off-the-shelf, we develop our training networks using elementary geometric considerations.},
address = {New York, NY, USA},
author = {Subramanian, Sandeep and Chakrabarti, Soumen},
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
doi = {10.1145/3209978.3210150},
isbn = {9781450356572},
keywords = {continuous type embeddings,ordered embeddings},
pages = {1037--1040},
publisher = {Association for Computing Machinery},
series = {SIGIR '18},
title = {{New Embedded Representations and Evaluation Protocols for Inferring Transitive Relations}},
url = {https://doi.org/10.1145/3209978.3210150},
year = {2018}
}
@article{Chu2019221,
abstract = {A news recommendation system aims to predict the next news based on users' interaction histories. In general, the clicking sequences from the interaction histories indicate users' latent preference, which plays an important role in predicting their future interest. Besides, news articles consist of considerable knowledge entities which have deep connections from common sense of human. In this paper, we propose a Self-Attention Sequential Knowledge-aware Recommendation (Saskr) system consisting of sequential-aware and knowledge-aware modelling. We use the self-attention mechanism to uncover sequential patterns in the sequential-aware modelling. The knowledge-aware modelling leverage the knowledge graph as side information to mine deep connections between news, thus improving diversity and extensibility of recommendation. Content-based news embeddings help to address the item cold-start problem. Through extensive experiments on the real-world news dataset, we demonstrate that the proposed model outperforms state-of-the-art deep neural sequential recommendation systems. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 18th China National Conference on Computational Linguistics, CCL 2019 ; Conference Date: 18 October 2019 Through 20 October 2019; Conference Code:233289},
author = {Chu, Q and Liu, G and Sun, H and Zhou, C},
doi = {10.1007/978-3-030-32381-3_18},
editor = {{Sun M. Liu Y.}, Liu Z Huang X Ji H},
isbn = {9783030323806},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attention mechanisms; Cold start problems; Intera,Computational linguistics; Recommender systems,Knowledge management},
pages = {221--232},
publisher = {Springer},
title = {{Next News Recommendation via Knowledge-Aware Sequential Model}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075723740&doi=10.1007%2F978-3-030-32381-3_18&partnerID=40&md5=f7eb9d532941c6a398d404e356e498a3},
volume = {11856 LNAI},
year = {2019}
}
@inproceedings{Szubert2019172,
abstract = {Combining two graphs requires merging the nodes which are counterparts of each other. In this process errors occur, resulting in incorrect merging or incorrect failure to merge. We find a high prevalence of such errors when using AskNET, an algorithm for building Knowledge Graphs from text corpora. AskNET node matching method uses string similarity, which we propose to replace with vector embedding similarity. We explore graph-based and wordbased embedding models and show an overall error reduction of from 56% to 23.6%, with a reduction of over a half in both types of incorrect node matching. {\textcopyright} 2019 EMNLP-IJCNLP 2019 - Graph-Based Methods for Natural Language Processing - Proceedings of the 13th Workshop. All rights reserved.},
annote = {cited By 0; Conference of 13th Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs 2019, in conjunction with the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019 ; Conference Date: 4 November 2019 Through 4 November 2019; Conference Code:159691},
author = {Szubert, I and Steedman, M},
booktitle = {EMNLP-IJCNLP 2019 - Graph-Based Methods for Natural Language Processing - Proceedings of the 13th Workshop},
isbn = {9781950737864},
keywords = {Embeddings; Errors; Graph algorithms; Graphic meth,Error reduction; Graph-based; Knowledge graphs; M,Graph theory},
pages = {172--176},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Node embeddings for graph merging: Case of knowledge graph construction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085036865&partnerID=40&md5=28bd67adc5310fa8700c9bfc54d763b0},
year = {2019}
}
@inproceedings{Zhang20192585,
abstract = {Linking entities from different sources is a fundamental task in building open knowledge graphs. Despite much research conducted in related fields, the challenges of linking large-scale heterogeneous entity graphs are far from resolved. Employing two billion-scale academic entity graphs (Microsoft Academic Graph and AMiner) as sources for our study, we propose a unified framework - LinKG - to address the problem of building a large-scale linked entity graph. LinKG is coupled with three linking modules, each of which addresses one category of entities. To link word-sequence-based entities (e.g., venues), we present a long short-term memory network-based method for capturing the dependencies. To link large-scale entities (e.g., papers), we leverage locality-sensitive hashing and convolutional neural networks for scalable and precise linking. To link entities with ambiguity (e.g., authors), we propose heterogeneous graph attention networks to model different types of entities. Our extensive experiments and systematical analysis demonstrate that LinKG can achieve linking accuracy with an F1-score of 0.9510, significantly outperforming the state-of-the-art. LinKG has been deployed to Microsoft Academic Search and AMiner to integrate the two large graphs. We have published the linked results-the Open Academic Graph (OAG)1, making it the largest publicly available heterogeneous academic graph to date. {\textcopyright} 2019 Association for Computing Machinery.},
annote = {cited By 4; Conference of 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2019 ; Conference Date: 4 August 2019 Through 8 August 2019; Conference Code:149966},
author = {Zhang, F and Liu, X and Tang, J and Dong, Y and Yao, P and Zhang, J and Gu, X and Wang, Y and Shao, B and Li, R and Wang, K},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330785},
isbn = {9781450362016},
keywords = {Convolutional neural network; Entity Linking; Het,Data mining,Graphic methods; Heterogeneous networks; Neural ne},
pages = {2585--2595},
publisher = {Association for Computing Machinery},
title = {{OAG: Toward linking large-scale heterogeneous entity graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071180128&doi=10.1145%2F3292500.3330785&partnerID=40&md5=dc1e6ebdc0957eab7299c4498983caab},
year = {2019}
}
@inproceedings{Fang20171661,
abstract = {Object detection in images is a crucial task in computer vision, with important applications ranging from security surveillance to autonomous vehicles. Existing state-of-the-art algorithms, including deep neural networks, only focus on utilizing features within an image itself, largely neglecting the vast amount of background knowledge about the real world. In this paper, we propose a novel framework of knowledge-aware object detection, which enables the integration of external knowledge such as knowledge graphs into any object detection algorithm. The framework employs the notion of semantic consistency to quantify and generalize knowledge, which improves object detection through a re-optimization process to achieve better consistency with background knowledge. Finally, empirical evaluation on two benchmark datasets show that our approach can significantly increase recall by up to 6.3 points without compromising mean average precision, when compared to the state-of-the-art baseline.},
annote = {cited By 23; Conference of 26th International Joint Conference on Artificial Intelligence, IJCAI 2017 ; Conference Date: 19 August 2017 Through 25 August 2017; Conference Code:130864},
author = {Fang, Y and Kuan, K and Lin, J and Tan, C and Chandrasekhar, V},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/230},
editor = {C., Sierra},
isbn = {9780999241103},
issn = {10450823},
keywords = {Artificial intelligence; Deep neural networks; Obj,Back-ground knowledge; Benchmark datasets; Empiri,Object detection},
pages = {1661--1667},
publisher = {International Joint Conferences on Artificial Intelligence},
title = {{Object detection meets knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031918851&doi=10.24963%2Fijcai.2017%2F230&partnerID=40&md5=044a29b9a60d2f08995aa854aa4914c6},
volume = {0},
year = {2017}
}
@inproceedings{Chen2018315,
abstract = {Populating ontology graphs represents a long-standing problem for the Semantic Web community. Recent ad-vances in translation-based graph embedding methods for populating instance-level knowledge graphs lead to promising new approaching for the ontology population problem. However, unlike instance-level graphs, the ma-jority of relation facts in ontology graphs come with comprehensive semantic relations, which often include the properties of transitivity and symmetry, as well as hierarchical relations. These comprehensive relation-s are often too complex for existing graph embedding methods, and direct application of such methods is not feasible. Hence, we propose On2Vec, a novel translation-based graph embedding method for ontology popula-tion. On2Vec integrates two model components that effectively characterize comprehensive relation facts in ontology graphs. The first is the Component-specific Model that encodes concepts and relations into low-dimensional embedding spaces without a loss of rela-tional properties; the second is the Hierarchy Model that performs focused learning of hierarchical relation facts. Experiments on several well-known ontol-ogy graphs demonstrate the promising capabilities of On2Vec in predicting and verifying new relation facts. These promising results also make possible significant improvements in related methods. {\textcopyright} 2018 by SIAM.},
annote = {cited By 10; Conference of 2018 SIAM International Conference on Data Mining, SDM 2018 ; Conference Date: 3 May 2018 Through 5 May 2018; Conference Code:136552},
author = {Chen, M and Tian, Y and Chen, X and Xue, Z and Zaniolo, C},
booktitle = {SIAM International Conference on Data Mining, SDM 2018},
doi = {10.1137/1.9781611975321.36},
keywords = {Data mining,Embeddings; Graphic methods; Ontology,Graph embeddings; Hierarchical relations; Hierarc},
pages = {315--323},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{On2Vec: Embedding-based relation prediction for ontology population}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048321977&doi=10.1137%2F1.9781611975321.36&partnerID=40&md5=97750e9f0283b6ef8ad8ee516fde9b73},
year = {2018}
}
@inproceedings{Xiong20201980,
abstract = {Knowledge graphs (KGs) are the key components of various natural language processing applications. To further expand KGs' coverage, previous studies on knowledge graph completion usually require a large number of training instances for each relation. However, we observe that long-tail relations are actually more common in KGs and those newly added relations often do not have many known triples for training. In this work, we aim at predicting new facts under a challenging setting where only one training instance is available. We propose a one-shot relational learning framework, which utilizes the knowledge extracted by embedding models and learns a matching metric by considering both the learned embeddings and one-hop graph structures. Empirically, our model yields considerable performance improvements over existing embedding models, and also eliminates the need of retraining the embedding models when dealing with newly added relations. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 11; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Xiong, W and Yu, M and Chang, S and Guo, X and Wang, W Y},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {Embeddings; Graph structures,Knowledge graphs; Long tail; Matching metric; Mod,Natural language processing systems},
pages = {1980--1990},
publisher = {Association for Computational Linguistics},
title = {{One-shot relational learning for knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081725315&partnerID=40&md5=7af11d65a332daf56d085ae3ff3cc00b},
year = {2020}
}
@inproceedings{Alkhatib201717,
abstract = {This paper introduces Onto.KOM: a minimally supervised ontology learning system which minimizes the reliance on complicated feature engineering and supervised linguistic modules for constructing the different consecutive components of an ontology, potentially providing domain independent and fully automatic ontology learning system. The focus here is to fill in the gap between automatically identifying the different ontological categories reflecting the domain of interest and the extraction and classification of semantic relations between the concepts under the different categories. In Onto.KOM, we depart from traditional approaches with intensive linguistic analysis and manual feature engineering for relation classification by introducing a convolutional neural network (CNN) that automatically learns features from word-pair offset in the vector space. The experimental results show that our system outperforms the state-of-the-art systems for relation classification in terms of F1-measure. {\textcopyright} 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
annote = {cited By 1; Conference of 9th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2017 ; Conference Date: 1 November 2017 Through 3 November 2017; Conference Code:133034},
author = {Alkhatib, W and Herrmann, L A and Rensing, C},
booktitle = {IC3K 2017 - Proceedings of the 9th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
doi = {10.5220/0006483000170026},
editor = {{Aveiro D. Dietz J.}, Filipe J Filipe J},
isbn = {9789897582721},
keywords = {Classification (of information); Convolution; Deep,Convolutional neural network; Feature engineering,Ontology},
pages = {17--26},
publisher = {SciTePress},
title = {{Onto.KOM: Towards a minimally supervised ontology learning system based on word embeddings and convolutional neural networks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055721909&doi=10.5220%2F0006483000170026&partnerID=40&md5=2bf7be496b7e8cb72737ba12b3c9ec45},
volume = {2},
year = {2017}
}
@inproceedings{Adán-Coello2012,
abstract = {The size of the Web and its dynamic nature in addition to the fact that stored documents are written in natural language, and therefore intended to be read by people and not to be processed by computers, present major challenges to build automatic personalized information filtering systems. This article presents the architecture of an information filtering agent based on an implementation of a Hopfield neural network (HNN). Network nodes (neurons) represent relevant terms in the domain of interest and neuronal links represent asymmetric probabilities of term co-occurrences in the domain, or the relevance weight between a pair of terms. Relevant terms are automatically derived from a corpus related to the domain of interest using automatic indexing and an ontology. Co-occurrence probabilities are computed by a cluster function that produces asymmetric links between terms. At the moment of document filtering, input neurons are activated on the basis of the presence of terms in the document that are identical or semantically similar to the terms stored in the net. The semantic similarity between terms is calculated using a hierarchical ontology that describes concepts that exist in the domain of interest. Experiments conducted to evaluate the precision and recall of the agent with and without the use of ontologies show that ontology use tends to favor recall over precision. The degree to which this bias occurs can be adjusted by setting the minimum level of similarity required to consider a document and a network term similar. {\textcopyright} 2012 IEEE.},
address = {Brisbane, QLD},
annote = {cited By 0; Conference of 2012 Annual International Joint Conference on Neural Networks, IJCNN 2012, Part of the 2012 IEEE World Congress on Computational Intelligence, WCCI 2012 ; Conference Date: 10 June 2012 Through 15 June 2012; Conference Code:92036},
author = {Ad{\'{a}}n-Coello, J M and Tobar, C M},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2012.6252796},
isbn = {9781467314909},
keywords = {Asymmetric links; Cluster functions; Co-occurrence,Automatic indexing; Cobalt; Cobalt compounds; Hop,Neural networks},
title = {{OntoHop: An information filtering agent using hopfield nets and ontologies}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865100697&doi=10.1109%2FIJCNN.2012.6252796&partnerID=40&md5=ed4f1249db51d402128159e9a0935ab0},
year = {2012}
}
@inproceedings{Filali2019124,
abstract = {Bag-of-Viusal-Words (BoVW) model has been widely used in the area of image classification, which rely on building visual vocabulary. Recently, attention has been shifted to the use of advanced architectures which are characterized by multilevel processing. HMAX model (Hierarchical Max-pooling model) has attracted a great deal of attention in image classification. Recent works, in image classification, consider the integration of ontologies and semantic structures is useful to improve image classification. In this paper, we propose an approach of image classification based on ontology and HMAX features using merged classifiers. Our contribution resides in exploiting ontological relationships between image categories in line with training visual-feature classifiers, and by merging the outputs of hypernym-hyponym classifiers to lead to a better discrimination between classes. Our purpose is to improve image classification by using ontologies. Several strategies have been experimented and the obtained results have shown that our proposal improves image classification. Results based our ontology outperform results obtained by baseline methods without ontology. Moreover, the deep learning network Inception-v3 is experimented and compared with our method, classification results obtained by our method outperform Inception-v3 for some image classes. Copyright {\textcopyright} 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.},
annote = {cited By 4; Conference of 14th International Conference on Computer Vision Theory and Applications, VISAPP 2019 - Part of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2019 ; Conference Date: 25 February 2019 Through 27 February 2019; Conference Code:146941},
author = {Filali, J and Zghal, H B and Martinet, J},
booktitle = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
doi = {10.5220/0007444101240134},
editor = {{Kerren A. Hurter C.}, Braz J},
isbn = {9789897583544},
keywords = {Advanced architecture; Baseline methods; Classifi,Classification (of information); Computer graphics,Image classification},
pages = {124--134},
publisher = {SciTePress},
title = {{Ontology and HMAX features-based image classification using merged classifiers}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068258902&doi=10.5220%2F0007444101240134&partnerID=40&md5=a15fb8744491cb141462846df0a92ac9},
volume = {5},
year = {2019}
}
@article{Pertsas2018162,
abstract = {We address the automatic extraction from publications of two key concepts for representing research processes: the concept of research activity and the sequence relation between successive activities. These representations are driven by the Scholarly Ontology, specifically conceived for documenting research processes. Unlike usual named entity recognition and relation extraction tasks, we are facing textual descriptions of activities of widely variable length, while pairs of successive activities often span multiple sentences. We developed and experimented with several sliding window classifiers using Logistic Regression, SVMs, and Random Forests, as well as a two-stage pipeline classifier. Our classifiers employ task-specific features, as well as word, part-of-speech and dependency embeddings, engineered to exploit distinctive traits of research publications written in English. The extracted activities and sequences are associated with other relevant information from publication metadata and stored as RDF triples in a knowledge base. Evaluation on datasets from three disciplines, Digital Humanities, Bioinformatics, and Medicine, shows very promising performance. {\textcopyright} Springer Nature Switzerland AG 2018.},
annote = {cited By 4; Conference of 17th International Semantic Web Conference, ISWC 2018 ; Conference Date: 8 October 2018 Through 12 October 2018; Conference Code:219319},
author = {Pertsas, V and Constantopoulos, P and Androutsopoulos, I},
doi = {10.1007/978-3-030-00671-6_10},
editor = {{Suarez-Figueroa M.C. Presutti V.}, Kaffee L Simperl E Sabou M Vrandecic D Celino I Bontcheva K},
isbn = {9783030006709},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Artificial intelligence; Data mining; Decision tre,Automatic extraction; Digital humanities; Logisti,Semantic Web},
pages = {162--178},
publisher = {Springer Verlag},
title = {{Ontology driven extraction of research processes}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054822199&doi=10.1007%2F978-3-030-00671-6_10&partnerID=40&md5=6cfc8f2b5e3d78d981230a26f8b0ff28},
volume = {11136 LNCS},
year = {2018}
}
@inproceedings{Idoudi2016345,
abstract = {Medical association rules induction is used to discover useful correlations between pertinent concepts from large medical databases. Nevertheless, ARs algorithms produce huge amount of delivered rules and do not guarantee the usefulness and interestingness of the generated knowledge. To overcome this drawback, we propose an ontology based interestingness measure for ARs ranking. According to domain expert, the goal of the use of ARs is to discover implicit relationships between items of different categories such as 'clinical features and disorders','clinical features and radiological observations', etc. That's to say, the itemsets which are composed of "similar" items are uninteresting. Therefore, the dissimilarity between the rule's items can be used to judge the interestingness of association rules; the more different are the items, the more interesting the rule is. In this paper, we design a distinct approach for ranking semantically interesting association rules involving the use of an ontology knowledge mining approach. The basic idea is to organize the ontology's concepts into a hierarchical structure of conceptual clusters of targeted subjects, where each cluster encapsulates "similar" concepts suggesting a specific category of the domain knowledge. The interestingness of association rules is, then, defined as the dissimilarity between corresponding clusters. That's to say, the further are the clusters of the items in the AR, the more interesting the rule is. We apply the method in our domain of interest-mammographic domain-using an existing mammographic ontology called Mammo∗, with the goal of deriving interesting rules from past experiences, to discover implicit relationships between concepts modeling the domain. {\textcopyright} 2016 The Authors. Published by Elsevier B.V.},
annote = {cited By 8; Conference of 20th International Conference on Knowledge Based and Intelligent Information and Engineering Systems, KES 2016 ; Conference Date: 5 September 2016 Through 7 September 2016; Conference Code:131578},
author = {Idoudi, R and Ettabaa, K S and Solaiman, B and Hamrouni, K},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2016.08.147},
editor = {{Howlett R.J. Gabrys B.}, Jain L C Toro C Lim C P},
issn = {18770509},
keywords = {Association rules,Clinical features; Conceptual clusters; Hierarchi,Data mining; Knowledge based systems; Mammography;},
pages = {345--354},
publisher = {Elsevier B.V.},
title = {{Ontology Knowledge Mining Based Association Rules Ranking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988882834&doi=10.1016%2Fj.procs.2016.08.147&partnerID=40&md5=285552f3afee5faba534bc8d4d7841b9},
volume = {96},
year = {2016}
}
@article{10.1016/j.procs.2019.09.212,
address = {NLD},
author = {Ayadi, Ali and Samet, Ahmed and de Beuvron, Fran{\c{c}}ois de Bertrand and Zanni-Merk, Cecilia},
doi = {10.1016/j.procs.2019.09.212},
issn = {1877-0509},
journal = {Procedia Comput. Sci.},
keywords = {Biomolecular Network Ontology,Deep learning,Knowledge acquisition,Natural language processing,Ontology population},
month = {jan},
number = {C},
pages = {572--581},
publisher = {Elsevier Science Publishers B. V.},
title = {{Ontology Population with Deep Learning-Based NLP: A Case Study on the Biomolecular Network Ontology}},
url = {https://doi.org/10.1016/j.procs.2019.09.212},
volume = {159},
year = {2019}
}
@article{Batbaatar2019,
abstract = {Named Entity Recognition (NER) in the healthcare domain involves identifying and categorizing disease, drugs, and symptoms for biosurveillance, extracting their related properties and activities, and identifying adverse drug events appearing in texts. These tasks are important challenges in healthcare. Analyzing user messages in social media networks such as Twitter can provide opportunities to detect and manage public health events. Twitter provides a broad range of short messages that contain interesting information for information extraction. In this paper, we present a Health-Related Named Entity Recognition (HNER) task using healthcare-domain ontology that can recognize health-related entities from large numbers of user messages from Twitter. For this task, we employ a deep learning architecture which is based on a recurrent neural network (RNN) with little feature engineering. To achieve our goal, we collected a large number of Twitter messages containing health-related information, and detected biomedical entities from the Unified Medical Language System (UMLS). A bidirectional long short-term memory (BiLSTM) model learned rich context information, and a convolutional neural network (CNN) was used to produce character-level features. The conditional random field (CRF) model predicted a sequence of labels that corresponded to a sequence of inputs, and the Viterbi algorithm was used to detect health-related entities from Twitter messages. We provide comprehensive results giving valuable insights for identifying medical entities in Twitter for various applications. The BiLSTM-CRF model achieved a precision of 93.99%, recall of 73.31%, and F1-score of 81.77% for disease or syndrome HNER; a precision of 90.83%, recall of 81.98%, and F1-score of 87.52% for sign or symptom HNER; and a precision of 94.85%, recall of 73.47%, and F1-score of 84.51% for pharmacologic substance named entities. The ontology-based manual annotation results show that it is possible to perform high-quality annotation despite the complexity of medical terminology and the lack of context in tweets. {\textcopyright} 2019 by the authors. Licensee MDPI, Basel, Switzerland.},
annote = {cited By 1},
author = {Batbaatar, E and Ryu, K H},
doi = {10.3390/ijerph16193628},
issn = {16617827},
journal = {International Journal of Environmental Research and Public Health},
keywords = {Algorithms; Biological Ontologies; Humans; Inform,Alzheimer disease; Article; asthma; blight; condi,Computer; Social Media; Unified Medical Language,algorithm; artificial neural network; complexity;,californium; cannabis; digitalis},
number = {19},
publisher = {MDPI AG},
title = {{Ontology-based healthcare named entity recognition from twitter messages using a recurrent neural network approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072764196&doi=10.3390%2Fijerph16193628&partnerID=40&md5=cab9aeb3942b6e695ee87f57a9675592},
volume = {16},
year = {2019}
}
@article{Trappey20131992,
abstract = {In order to stimulate innovation during the collaborative process of new product and production development, especially to avoid duplicating existing techniques or infringing upon others patents and intellectual property rights, the collaborative team of research and development, and patent engineers must accurately identify relevant patent knowledge in a timely manner. This research develops a novel knowledge management approach using ontology-based artificial neural network (ANN) algorithm to automatically classify and search knowledge documents stored in huge online patent corpuses. This research focuses on developing a smart and semantic oriented classification and search from the sources of the most critical and well-structured knowledge publications, i.e. patents, to gain valuable and practical references for the collaborative networks of technology-centric product and production development teams. The research uses the domain ontology schema created using Prot{\'{e}}g{\'{e}} and derives the semantic concept probabilities of key phrases that frequently occur in domain relevant patent documents. Then, by combining the term frequencies and the concept probabilities of key phrases as the ANN inputs, the method shows significant improvement in classification accuracy. In addition, this research provides an advanced semantic-oriented search algorithm to accurately identify related patent documents in the patent knowledge base. The case demonstration analyses 343 chemical mechanical polishing and 150 radio-frequency identification patents sample sets to verify and measure the performance of the proposed approach. The results are compared with the previous automatic classification methods demonstrating much improved outcomes. {\textcopyright} 2013 Taylor \& Francis Group, LLC.},
annote = {cited By 23},
author = {Trappey, A J C and Trappey, C V and Chiang, T.-A. and Huang, Y.-H.},
doi = {10.1080/00207543.2012.701775},
issn = {00207543},
journal = {International Journal of Production Research},
keywords = {Algorithms; Chemical mechanical polishing; Innova,Automatic classification; Classification accuracy;,Patents and inventions},
number = {7},
pages = {1992--2005},
title = {{Ontology-based neural network for patent knowledge management in design collaboration}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873433576&doi=10.1080%2F00207543.2012.701775&partnerID=40&md5=f95409843176e59b1494b85755a604a4},
volume = {51},
year = {2013}
}
@inproceedings{Balasubramani2016,
abstract = {Cities are actively creating open data portals to enable predictive analytics of urban data. However, the large num- ber of observable patterns that can be extracted as rules by techniques such as Association Rule Mining (ARM) makes the task of sifting through patterns a tedious and time- consuming task. In this paper, we explore the use of domain ontologies to: (i) ffilter and prune rules that are variations of a more general concept in the ontology, and (ii) replace groups of rules by a single general rule with the intent of downsizing the number of initial rules while preserving the semantics. We show how the combination of several methods reduces signifficantly the number of rules thus effiectively allowing city administrators to use open data to generate patterns, use them for decision making, and better direct limited government resources. {\textcopyright} 2016 ACM.},
annote = {cited By 8; Conference of 2nd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics, UrbanGIS 2016 ; Conference Date: 31 October 2016; Conference Code:124714},
author = {Balasubramani, B S and Shivaprabhu, V R and Krishnamurthy, S and Cruz, I F and Malik, T},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics, UrbanGIS 2016},
doi = {10.1145/3007540.3007550},
isbn = {9781450345835},
keywords = {Association rule minings (ARM); Data exploration;,Data mining,Decision making; Semantics},
publisher = {Association for Computing Machinery, Inc},
title = {{Ontology-based urban data exploration}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002410804&doi=10.1145%2F3007540.3007550&partnerID=40&md5=6adc98659f9941f720046290d709eaf6},
year = {2016}
}
@article{Yang2019,
abstract = {Background: Padua linear model is widely used for the risk assessment of venous thromboembolism (VTE), a common but preventable complication for inpatients. However, genetic and environmental differences between Western and Chinese population limit the validity of Padua model in Chinese patients. Medical records which contain rich information about disease progression, are useful in mining new risk factors related to Chinese VTE patients. Furthermore, machine learning (ML) methods provide new opportunities to build precise risk prediction model by automatic selection of risk factors based on original medical records. Methods: Medical records of 3,106 inpatients including 224 VTE patients were collected and various types of ontologies were integrated to parse unstructured text. A workflow of ontology-based VTE risk prediction model, that combines natural language processing (NLP) and machine learning (ML) technologies, was proposed. Firstly ontology terms were extracted from medical records, then sorted according to their calculated weights. Next importance of each term in the unit of section was evaluated and finally a ML model was built based on a subset of terms. Four ML methods were tested, and the best model was decided by comparing area under the receiver operating characteristic curve (AUROC). Results: Medical records were first split into different sections and subsequently, terms from each section were sorted by their weights calculated by multiple types of information. Greedy selection algorithm was used to obtain significant sections and terms. Top terms in each section were selected to construct patients' distributed representations by word embedding technique. Using top 300 terms of two important sections, namely the 'Progress Note' section and 'Admitting Diagnosis' section, the model showed relatively better predictive performance. Then ML model which utilizes a subset of terms from two sections, about 110 terms, achieved the best AUC score, of 0.973 ± 0.006, which was significantly better compared to the Padua's performance of 0.791 ± 0.022. Terms found by the model showed their potential to help clinicians explore new risk factors. Conclusions: In this study, a new VTE risk assessment model based on ontologies extraction from raw medical records is developed and its performance is verified on real clinical dataset. Results of selected terms can help clinicians to discover meaningful risk factors. {\textcopyright} 2019 The Author(s).},
annote = {cited By 3},
author = {Yang, Y and Wang, X and Huang, Y and Chen, N and Shi, J and Chen, T},
doi = {10.1186/s12911-019-0856-2},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
keywords = {Adult; Aged; Algorithms; Area Under Curve; Female,adult; aged; algorithm; area under the curve; fema},
publisher = {BioMed Central Ltd.},
title = {{Ontology-based venous thromboembolism risk assessment model developing from medical records}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072016236&doi=10.1186%2Fs12911-019-0856-2&partnerID=40&md5=117056ee0c7694d8e4d9cb9e20521a90},
volume = {19},
year = {2019}
}
@article{ISI:000567825900012,
abstract = {Infodemiology is the process of mining unstructured and textual data so
as to provide public health officials and policymakers with valuable
information regarding public health. The appearance of this new data
source, which was previously unimaginable, has opened up a new way in
which to improve public health systems, resulting in better
communication policies and better detection systems. However, the
unstructured nature of the Internet, along with the complexity of the
infectious disease domain, prevents the information extracted from being
easily understood. Moreover, when dealing with languages other than
English, for which some of the most common Natural Language Processing
resources are not available, the correct exploitation of this data
becomes even more difficult. We intend to fill these gaps proposing an
ontology-driven aspect-based sentiment analysis with which to measure
the general public's opinions as regards infectious diseases when
expressed in Spanish by employing a case study of tweets concerning the
Zika, Dengue and Chikungunya viruses in Latin America. Our proposal is
based on two technologies. We first use ontologies in order to model the
infectious disease domain with concepts such as risks, symptoms,
transmission methods or drugs, among other concepts. We then measure the
relationship between these concepts in order to determine the degree to
which one concept influences other concepts. This new information is
subsequently applied in order to build an aspect-based sentiment
analysis model based on statistical and linguistic features. This is
done by applying deep-learning models. Our proposal is available on a
web platform, where users can see the sentiment for each concept at a
glance and analyse how each concept influences the sentiment of the
others. (c) 2020 Elsevier B.V. All rights reserved.},
address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
author = {Garcia-Diaz, Jose Antonio and Canovas-Garcia, Mar and Valencia-Garcia, Rafael},
doi = {10.1016/j.future.2020.06.019},
issn = {0167-739X},
journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
keywords = {Aspect-based sentiment analysis; Infodemiology; De},
month = {nov},
pages = {641--657},
publisher = {ELSEVIER},
title = {{Ontology-driven aspect-based sentiment analysis classification: An infodemiological case study regarding infectious diseases in Latin America}},
type = {Article},
volume = {112},
year = {2020}
}
@article{Abdollahi202078,
abstract = {Extracting meaningful features from unstructured text is one of the most challenging tasks in medical document classification. The various domain specific expressions and synonyms in the clinical discharge notes make it more challenging to analyse them. The case becomes worse for short texts such as abstract documents. These challenges can lead to poor classification accuracy. As the medical input data is often not enough in the real world, in this work a novel ontology-guided method is proposed for data augmentation to enrich input data. Then, three different deep learning methods are employed to analyse the performance of the suggested approach for classification. The experimental results show that the suggested approach achieved substantial improvement in the targeted medical documents classification. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 18th International Conference on Artificial Intelligence in Medicine, AIME 2020 ; Conference Date: 25 August 2020 Through 28 August 2020; Conference Code:249409},
author = {Abdollahi, M and Gao, X and Mei, Y and Ghosh, S and Li, J},
doi = {10.1007/978-3-030-59137-3_8},
editor = {{Michalowski M.}, Moskovitch R},
isbn = {9783030591366},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Classification accuracy; Data augmentation; Domai,Deep learning; Input output programs; Learning sys,Information retrieval systems},
pages = {78--88},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Ontology-Guided Data Augmentation for Medical Document Classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092233463&doi=10.1007%2F978-3-030-59137-3_8&partnerID=40&md5=abb4d6cb6eda484a7638f7fab522d33e},
volume = {12299 LNAI},
year = {2020}
}
@article{Garla2012992,
abstract = {In this study we present novel feature engineering techniques that leverage the biomedical domain knowledge encoded in the Unified Medical Language System (UMLS) to improve machine-learning based clinical text classification. Critical steps in clinical text classification include identification of features and passages relevant to the classification task, and representation of clinical text to enable discrimination between documents of different classes. We developed novel information-theoretic techniques that utilize the taxonomical structure of the Unified Medical Language System (UMLS) to improve feature ranking, and we developed a semantic similarity measure that projects clinical text into a feature space that improves classification. We evaluated these methods on the 2008 Integrating Informatics with Biology and the Bedside (I2B2) obesity challenge. The methods we developed improve upon the results of this challenge's top machine-learning based system, and may improve the performance of other machine-learning based clinical text classification systems. We have released all tools developed as part of this study as open source, available at http://code.google.com/p/ytex. {\textcopyright} 2012 Elsevier Inc..},
annote = {cited By 44},
author = {Garla, V N and Brandt, C},
doi = {10.1016/j.jbi.2012.04.010},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Algorithms; Cardiovascular Diseases; Data Mining;,Computational linguistics; Feature extraction; In,Information contents; Information gain; Kernel met,Medical information systems,Theoretical; Natural Language Processing; Obesity,article; clinical text classification; coding and},
number = {5},
pages = {992--998},
title = {{Ontology-guided feature engineering for clinical text classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865991155&doi=10.1016%2Fj.jbi.2012.04.010&partnerID=40&md5=e9abb00497db683e6b800d38bca10346},
volume = {45},
year = {2012}
}
@inproceedings{Shi20181957,
abstract = {Knowledge Graphs (KGs) have been applied to many tasks including Web search, link prediction, recommendation, natural language processing, and entity linking. However, most KGs are far from complete and are growing at a rapid pace. To address these problems, Knowledge Graph Completion (KGC) has been proposed to improve KGs by filling in its missing connections. Unlike existing methods which hold a closed-world assumption, i.e., where KGs are fixed and new entities cannot be easily added, in the present work we relax this assumption and propose a new open-world KGC task. As a first attempt to solve this task we introduce an open-world KGC model called ConMask. This model learns embeddings of the entity's name and parts of its text-description to connect unseen entities to the KG. To mitigate the presence of noisy text descriptions, ConMask uses a relationship-dependent content masking to extract relevant snippets and then trains a fully convolutional neural network to fuse the extracted snippets with entities in the KG. Experiments on large data sets, both old and new, show that ConMask performs well in the open-world KGC task and even outperforms existing KGC models on the standard closed-world KGC task. Copyright {\textcopyright} 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
annote = {cited By 39; Conference of 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference Date: 2 February 2018 Through 7 February 2018; Conference Code:143510},
author = {Shi, B and Weninger, T},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
isbn = {9781577358008},
keywords = {Closed world assumption; Convolutional neural net,Natural language processing systems,Neural networks},
pages = {1957--1964},
publisher = {AAAI press},
title = {{Open-world knowledge graph completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056498188&partnerID=40&md5=63fcef4c8de3a789f08f50abbb4eca9e},
year = {2018}
}
@inproceedings{10.1145/3357384.3357965,
abstract = {Medicine Combination Prediction (MCP) based on Electronic Health Record (EHR) can assist doctors to prescribe medicines for complex patients. Previous studies on MCP either ignore the correlations between medicines (i.e., MCP is formulated as a binary classifcation task), or assume that there is a sequential correlation between medicines (i.e., MCP is formulated as a sequence prediction task). The latter is unreasonable because the correlations between medicines should be considered in an order-free way. Importantly, MCP must take additional medical knowledge (e.g., Drug-Drug Interaction (DDI)) into consideration to ensure the safety of medicine combinations. However, most previous methods for MCP incorporate DDI knowledge with a post-processing scheme, which might undermine the integrity of proposed medicine combinations. In this paper, we propose a graph convolutional reinforcement learning model for MCP, named Combined Order-free Medicine Prediction Network (CompNet), that addresses the issues listed above. CompNet casts the MCP task as an order-free Markov Decision Process (MDP) problem and designs a Deep Q Learning (DQL) mechanism to learn correlative and adverse interactions between medicines. Specifcally, we frst use a Dual Convolutional Neural Network (Dual-CNN) to obtain patient representations based on EHRs. Then, we introduce the medicine knowledge associated with predicted medicines to create a dynamic medicine knowledge graph, and use a Relational Graph Convolutional Network (R-GCN) to encode it. Finally, CompNet selects medicines by fusing the combination of patient information and the medicine knowledge graph. Experiments on a benchmark dataset, i.e., MIMIC-III, demonstrate that CompNet signifcantly outperforms state-of-the-art methods and improves a recently proposed model by 3.74%pt, 6.64%pt in terms of Jaccard and F1 metrics.},
address = {New York, NY, USA},
author = {Wang, Shanshan and Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Ma, Jun and de Rijke, Maarten},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3357384.3357965},
isbn = {9781450369763},
keywords = {medicine combination prediction,medicine knowledge graph,reinforcement learning,relational graph convolutional network},
pages = {1623--1632},
publisher = {Association for Computing Machinery},
series = {CIKM '19},
title = {{Order-Free Medicine Combination Prediction with Graph Convolutional Reinforcement Learning}},
url = {https://doi.org/10.1145/3357384.3357965},
year = {2019}
}
@inproceedings{10.1145/3219819.3219899,
abstract = {Entity linking is the task of mapping potentially ambiguous terms in text to their constituent entities in a knowledge base like Wikipedia. This is useful for organizing content, extracting structured data from textual documents, and in machine learning relevance applications like semantic search, knowledge graph construction, and question answering. Traditionally, this work has focused on text that has been well-formed, like news articles, but in common real world datasets such as messaging, resumes, or short-form social media, non-grammatical, loosely-structured text adds a new dimension to this problem. This paper presents Pangloss, a production system for entity disambiguation on noisy text. Pangloss combines a probabilistic linear-time key phrase identification algorithm with a semantic similarity engine based on context-dependent document embeddings to achieve better than state-of-the-art results (>5% in F1) compared to other research or commercially available systems. In addition, Pangloss leverages a local embedded database with a tiered architecture to house its statistics and metadata, which allows rapid disambiguation in streaming contexts and on-device disambiguation in low-memory environments such as mobile phones.},
address = {New York, NY, USA},
author = {Conover, Michael and Hayes, Matthew and Blackburn, Scott and Skomoroch, Pete and Shah, Sam},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3219819.3219899},
isbn = {9781450355520},
keywords = {entity linking,knowledge bases,natural language understanding},
pages = {168--176},
publisher = {Association for Computing Machinery},
series = {KDD '18},
title = {{Pangloss: Fast Entity Linking in Noisy Text Environments}},
url = {https://doi.org/10.1145/3219819.3219899},
year = {2018}
}
@inproceedings{8217753,
abstract = {The large availability of biomedical data brings opportunities and challenges to health care. Representation of medical concepts has been well studied in many applications, such as medical informatics, cohort selection, risk prediction, and health care quality measurement. In this paper, we propose an efficient multichannel convolutional neural network (CNN) model based on multi-granularity embeddings of medical concepts named MG-CNN, to examine the effect of individual patient characteristics including demographic factors and medical comorbidities on total hospital costs and length of stay (LOS) by using the Hospital Quality Monitoring System (HQMS) data. The proposed embedding method leverages prior medical hierarchical ontology and improves the quality of embedding for rare medical concepts. The embedded vectors are further visualized by the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique to demonstrate the effectiveness of grouping related medical concepts. Experimental results demonstrate that our MG-CNN model outperforms traditional regression methods based on the one-hot representation of medical concepts, especially in the outcome prediction tasks for patients with low-frequency medical events. In summary, MG-CNN model is capable of mining potential knowledge from the clinical data and will be broadly applicable in medical research and inform clinical decisions.},
author = {Feng, Y and Min, X and Chen, N and Chen, H and Xie, X and Wang, H and Chen, T},
booktitle = {2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
doi = {10.1109/BIBM.2017.8217753},
keywords = {bioinformatics;data mining;electronic health recor},
month = {nov},
pages = {770--777},
title = {{Patient outcome prediction via convolutional neural networks based on multi-granularity medical concept embedding}},
year = {2017}
}
@article{9178312,
abstract = {With the prevalence and growing volume of Electronic Health Records (EHRs), there has been increasing interest in mining EHRs for improving clinical decision support. The accurate identification of patients with similar conditions based on EHRs is a key step in personalized healthcare. Existing studies model EHRs by medical knowledge graph embedding to learn the latent embeddings of medical entities (e.g., patients, medications, diagnoses and procedures). However, such precisely structured data is usually limited in quantity and in scope. Therefore, to enhance the quality of the embeddings it is important to consider more widely available medical information such as medical entity descriptions. In this paper we propose a novel framework, called Deep Patient Similarity (DeepPS). Specifically, DeepPS incorporates medical entity descriptions by augmenting the embeddings of medical entities and relations with the embeddings of words, which leverages both information from medical knowledge graph structures and the contexts of medical entity descriptions. Furthermore, DeepPS employs the embeddings to patient similarity learning by leveraging Siamese Convolutional Neural Network (CNN) with Spatial Pyramid Pooling (SPP). Extensive experiments on real datasets are conducted to show superior performance of our proposed framework.},
author = {Lin, Z and Yang, D and Yin, X},
doi = {10.1109/ACCESS.2020.3019577},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {convolutional neural nets;data mining;decision sup},
pages = {156663--156676},
title = {{Patient Similarity via Joint Embeddings of Medical Knowledge Graph and Medical Entity Descriptions}},
volume = {8},
year = {2020}
}
@article{Yang2020539,
abstract = {Artificial intelligence technology has been actively researched in the areas of image processing and natural language processing. Recently, with the release of Google's language model BERT, the importance of artificial intelligence models has attracted attention in the field of natural language processing. In this paper, we propose a knowledge graph to build a model that can extract people in a document using BERT, and to grasp the relationship between people based on the model. In addition, to verify the applicability of person extraction techniques using BERT based knowledge graphs, we conduct a performance comparison experiment with other person extraction models and apply our proposed method to the case study. {\textcopyright} 2020, ICIC International.},
annote = {cited By 0},
author = {Yang, S M and Yoo, S Y and Ahn, Y S and Jeong, O R},
doi = {10.24507/icicelb.11.06.539},
issn = {21852766},
journal = {ICIC Express Letters, Part B: Applications},
number = {6},
pages = {539--544},
publisher = {ICIC International},
title = {{Person-relation extraction using bert based knowledge graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084368302&doi=10.24507%2Ficicelb.11.06.539&partnerID=40&md5=8a7e5c7967001245abbdb062c2ed5ea0},
volume = {11},
year = {2020}
}
@article{8361057,
abstract = {As a significant determinant in the development of named entity recognition, phenotypic descriptions are normally presented differently in biomedical literature with the use of complicated semantics. In this paper, a novel approach has been proposed to identify plant phenotypes by adopting word embedding to sentence embedding cascaded approach. We make use of a word embedding method to find high-frequency phenotypes with original sentences used as input in a sentence embedding method. In doing so, a variety of complicated phenotypic expressions can be recognized accurately. Besides, the state-of-the-art word representation models have been compared and among them, skip-gram with negative sampling was selected with the best performance. To evaluate the performance of our approach, we applied it to the dataset composed of 56 748 PubMed abstracts of model organism Arabidopsis thaliana. The experiment results showed that our approach yielded the best performance, as it achieved a 2.588-fold increase in terms of the number of new phenotypic descriptions when compared to the original phenotype ontology.},
author = {Xing, W and Yuan, X and Li, L and Hu, L and Peng, J},
doi = {10.1109/TNB.2018.2838137},
issn = {1558-2639},
journal = {IEEE Transactions on NanoBioscience},
keywords = {Automated;Phenotype,Factual;Natural Language Processing;Pattern Recog,biology computing;botany;natural language processi},
month = {jul},
number = {3},
pages = {172--180},
title = {{Phenotype Extraction Based on Word Embedding to Sentence Embedding Cascaded Approach}},
volume = {17},
year = {2018}
}
@article{Min2017,
abstract = {Background: Bio-ontologies are becoming increasingly important in knowledge representation and in the machine learning (ML) fields. This paper presents a ML approach that incorporates bio-ontologies and its application to the SEER-MHOS dataset to discover patterns of patient characteristics that impact the ability to perform activities of daily living (ADLs). Bio-ontologies are used to provide computable knowledge for ML methods to "understand" biomedical data. Results: This retrospective study included 723 cancer patients from the SEER-MHOS dataset. Two ML methods were applied to create predictive models for ADL disabilities for the first year after a patient's cancer diagnosis. The first method is a standard rule learning algorithm; the second is that same algorithm additionally equipped with methods for reasoning with ontologies. The models showed that a patient's race, ethnicity, smoking preference, treatment plan and tumor characteristics including histology, staging, cancer site, and morphology were predictors for ADL performance levels one year after cancer diagnosis. The ontology-guided ML method was more accurate at predicting ADL performance levels (P < 0.1) than methods without ontologies. Conclusions: This study demonstrated that bio-ontologies can be harnessed to provide medical knowledge for ML algorithms. The presented method demonstrates that encoding specific types of hierarchical relationships to guide rule learning is possible, and can be extended to other types of semantic relationships present in biomedical ontologies. The ontology-guided ML method achieved better performance than the method without ontologies. The presented method can also be used to promote the effectiveness and efficiency of ML in healthcare, in which use of background knowledge and consistency with existing clinical expertise is critical. {\textcopyright} 2017 The Author(s).},
annote = {cited By 3},
author = {Min, H and Mobahi, H and Irvin, K and Avramovic, S and Wojtusiak, J},
doi = {10.1186/s13326-017-0149-6},
issn = {20411480},
journal = {Journal of Biomedical Semantics},
keywords = {80 and over; Biological Ontologies; Cohort Studie,Activities of Daily Living; Aged; Aged,aged; biological ontology; cohort analysis; daily},
number = {1},
publisher = {BioMed Central Ltd.},
title = {{Predicting activities of daily living for cancer patients using an ontology-guided machine learning methodology}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029495342&doi=10.1186%2Fs13326-017-0149-6&partnerID=40&md5=06b0a8f424e0a7560094d2ff3d7f50c2},
volume = {8},
year = {2017}
}
@article{Lukovnikov2019470,
abstract = {Answering simple questions over knowledge graphs is a well-studied problem in question answering. Previous approaches for this task built on recurrent and convolutional neural network based architectures that use pretrained word embeddings. It was recently shown that finetuning pretrained transformer networks (e.g. BERT) can outperform previous approaches on various natural language processing tasks. In this work, we investigate how well BERT performs on SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based models in limited-data scenarios. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 1; Conference of 18th International Semantic Web Conference, ISWC 2019 ; Conference Date: 26 October 2019 Through 30 October 2019; Conference Code:233309},
author = {Lukovnikov, D and Fischer, A and Lehmann, J},
doi = {10.1007/978-3-030-30793-6_27},
editor = {{Ghidini C. Hartig O.}, Maleshkova M Svatek V Cruz I Hogan A Song J Lefrancois M Gandon F},
isbn = {9783030307929},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network; Knowledge graphs; L,Natural language processing systems; Petroleum res,Semantic Web},
pages = {470--486},
publisher = {Springer},
title = {{Pretrained Transformers for Simple Question Answering over Knowledge Graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075712407&doi=10.1007%2F978-3-030-30793-6_27&partnerID=40&md5=197a5fdbbe17f3d86e1e2a0243b6ee16},
volume = {11778 LNCS},
year = {2019}
}
@article{Fan20161087,
abstract = {Background: To populate knowledge repositories, such as WordNet, Freebase and NELL, two branches of research have grown separately for decades. On the one hand, corpus-based methods which leverage unstructured free texts have been explored for years; on the other hand, some recently emerged embedding-based approaches use structured knowledge graphs to learn distributed representations of entities and relations. But there are still few comprehensive and elegant models that can integrate those large-scale heterogeneous resources to satisfy multiple subtasks of knowledge population including entity inference, relation prediction and triplet classification. Methods: This paper contributes a novel embedding model which estimates the probability of each candidate belief <h,r,t,m> in a large-scale knowledge repository via simultaneously learning distributed representations for entities (h and t), relations (r) and the words in relation mentions (m). It facilitates knowledge population by means of simple vector operations to discover new beliefs. Given an imperfect belief, we can not only infer the missing entities and predict the unknown relations, but also identify the plausibility of the belief, just by leveraging the learned embeddings of remaining evidence. Results: To demonstrate the scalability and the effectiveness of our model, experiments have been conducted on several large-scale repositories which contain millions of beliefs from WordNet, Freebase and NELL, and the results are compared with other cutting-edge approaches via comparing the performance assessed by the tasks of entity inference, relation prediction and triplet classification with their respective metrics. Extensive experimental results show that the proposed model outperforms the state of the arts with significant improvements. Conclusions: The essence of the improvements comes from the capability of our model that encodes not only structured knowledge graph information, but also unstructured relation mentions, into continuous vector spaces, so that we can bridge the gap of one-hot representations, and expect to discover certain relevance among entities, relations and even words in relation mentions. {\textcopyright} 2016, Springer Science+Business Media New York.},
annote = {cited By 5},
author = {Fan, M and Zhou, Q and Abel, A and Zheng, T F and Grishman, R},
doi = {10.1007/s12559-016-9425-5},
issn = {18669956},
journal = {Cognitive Computation},
keywords = {Belief embedding; Distributed representation; Ent,Forecasting; Ontology; Probability; Vector spaces,Knowledge management},
number = {6},
pages = {1087--1102},
publisher = {Springer New York LLC},
title = {{Probabilistic Belief Embedding for Large-Scale Knowledge Population}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981163507&doi=10.1007%2Fs12559-016-9425-5&partnerID=40&md5=660e3c9aab7be1bf215f6b44092b1615},
volume = {8},
year = {2016}
}
@inproceedings{Masseroli2012,
abstract = {Consistency and completeness of biomolecular annotations is a keypoint of correct interpretation of biological experiments. Yet, the associations between genes (or proteins) and features correctly annotated are just some of all the existing ones. As time goes by, they increase in number and become more useful, but they remain incomplete and some of them incorrect. To support and quicken their time-consuming curation procedure and to improve consistence of available annotations, computational methods that are able to supply a ranked list of predicted annotations are hence extremely useful. Starting from a previous work on the automatic prediction of Gene Ontology (GO) annotations based on the Singular Value Decomposition of the annotation matrix, where every matrix element corresponds to the association of a gene with a feature, we propose the use of a modified Probabilistic Latent Semantic Analysis (pLSA) algorithm, named pLSAnorm, to better perform such prediction. pLSA is a statistical technique from the natural language processing field, which has not been used in bioinformatics annotation prediction yet; it takes advantage of the latent information contained in the analyzed data co-occurrences. We proved the effectiveness of the pLSAnorm prediction method by performing k-fold cross-validation of the GO annotations of two organisms, Gallus gallus and Bos taurus. Obtained results demonstrate the efficacy of our approach. {\textcopyright} 2012 IEEE.},
address = {Brisbane, QLD},
annote = {cited By 29; Conference of 2012 Annual International Joint Conference on Neural Networks, IJCNN 2012, Part of the 2012 IEEE World Congress on Computational Intelligence, WCCI 2012 ; Conference Date: 10 June 2012 Through 15 June 2012; Conference Code:92036},
author = {Masseroli, M and Chicco, D and Pinoli, P},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2012.6252767},
isbn = {9781467314909},
keywords = {Automatic prediction; Bio-molecular; Biological ex,Bioinformatics; Computational linguistics; Genes;,Forecasting},
title = {{Probabilistic Latent Semantic Analysis for prediction of Gene Ontology annotations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865084338&doi=10.1109%2FIJCNN.2012.6252767&partnerID=40&md5=d50297ba1076745fc4c1e5f5d1522cd2},
year = {2012}
}
@inproceedings{Qu2019,
abstract = {Knowledge graph reasoning, which aims at predicting the missing facts through reasoning with the observed facts, is critical to many applications. Such a problem has been widely explored by traditional logic rule-based approaches and recent knowledge graph embedding methods. A principled logic rule-based approach is the Markov Logic Network (MLN), which is able to leverage domain knowledge with first-order logic and meanwhile handle the uncertainty. However, the inference in MLNs is usually very difficult due to the complicated graph structures. Different from MLNs, knowledge graph embedding methods (e.g. TransE, DistMult) learn effective entity and relation embeddings for reasoning, which are much more effective and efficient. However, they are unable to leverage domain knowledge. In this paper, we propose the probabilistic Logic Neural Network (pLogicNet), which combines the advantages of both methods. A pLogicNet defines the joint distribution of all possible triplets by using a Markov logic network with first-order logic, which can be efficiently optimized with the variational EM algorithm. In the E-step, a knowledge graph embedding model is used for inferring the missing triplets, while in the M-step, the weights of logic rules are updated based on both the observed and predicted triplets. Experiments on multiple knowledge graphs prove the effectiveness of pLogicNet over many competitive baselines. {\textcopyright} 2019 Neural information processing systems foundation. All rights reserved.},
annote = {cited By 0; Conference of 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019 ; Conference Date: 8 December 2019 Through 14 December 2019; Conference Code:161263},
author = {Qu, M and Tang, J},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
keywords = {Computer circuits,Domain knowledge; First order logic; Joint distri,Embeddings; Graph structures; Knowledge management},
publisher = {Neural information processing systems foundation},
title = {{Probabilistic logic neural networks for reasoning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090171033&partnerID=40&md5=223e64827c5303f18f9c9d5f5072396c},
volume = {32},
year = {2019}
}
@article{Alekseychuk201968,
abstract = {In this paper, we present a project on the analysis of an extensive corpus of strategic planning documents, devoted to various aspects of the development of Russian regions. The main purposes of the project are: (1) to extract different aspects of goal setting and planning, (2) to form an ontology of goals and criteria of achieving these goals, (3) to measure the similarity between goals declared by federal and municipal subjects. Such unsupervised Natural Language Processing (NLP) methods as phrase chunking, word embeddings, and latent topic modeling are used for information extraction and ontology construction as well as similarity computation. The resulting ontology should serve in short-term as a helper tool for writing strategic planning documents and in long-term resolve the need to compose strategic planning documents completely by navigating through the ontology and selecting relevant goals and criteria. The resulting similarity measure between federal and municipal goals will serve as a navigation tool for further analysis. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 1; Conference of 4th International Conference on Digital Transformation and Global Society, DTGS 2019 ; Conference Date: 19 June 2019 Through 21 June 2019; Conference Code:235749},
author = {Alekseychuk, N and Sarkisyan, V and Emelyanov, A and Artemova, E},
doi = {10.1007/978-3-030-37858-5_6},
editor = {{Alexandrov D.A. Kabanov Y.}, Koltsova O Musabirov I Boukhanovsky A V Chugunov A V},
isbn = {9783030378578},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Data mining; Modeling languages; Natural language,Distributional semantics; Latent topic model; NAt,Strategic planning},
pages = {68--81},
publisher = {Springer},
title = {{Processing and Analysis of Russian Strategic Planning Programs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078542274&doi=10.1007%2F978-3-030-37858-5_6&partnerID=40&md5=a6eec9f87b8a4d2125d60689ccd2767a},
volume = {1038 CCIS},
year = {2019}
}
@inproceedings{10.1145/3336191.3371778,
abstract = {In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The {\P}oincare embedding is also employed to handle complex entity structures. We use a real-world dataset from textslgrocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.},
address = {New York, NY, USA},
author = {Xu, Da and Ruan, Chuanwei and Korpeoglu, Evren and Kumar, Sushant and Achan, Kannan},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
doi = {10.1145/3336191.3371778},
isbn = {9781450368223},
keywords = {information retrieval,knowledge graph,recommendation,relation learning,representation learning,search ranking},
pages = {672--680},
publisher = {Association for Computing Machinery},
series = {WSDM '20},
title = {{Product Knowledge Graph Embedding for E-Commerce}},
url = {https://doi.org/10.1145/3336191.3371778},
year = {2020}
}
@article{Zhang2018145,
abstract = {Knowledge graph completion aims to find new true links between entities. In this paper, we consider an approach to embed a knowledge graph into a continuous vector space. Embedding methods, such as TransE, TransR and ProjE, are proposed in recent years and have achieved promising predictive performance. We discuss that a lot of substructures related with different relation properties in knowledge graph should be considered during embedding. We list 8 kinds of substructures and find that none of the existing embedding methods could encode all the substructures at the same time. Considering the structure diversity, we propose that a knowledge graph embedding method should have diverse representations for entities in different relation contexts and different entity positions. And we propose a new embedding method ProjR which combines TransR and ProjE together to achieve diverse representations by defining a unique combination operator for each relation. In ProjR, the input head entity-relation pairs with different relations will go through a different combination process. We conduct experiments with link prediction task on benchmark datasets for knowledge graph completion and the experiment results show that, with diverse representations, ProjR performs better compared with TransR and ProjE. We also analyze the performance of ProjR in the 8 different substructures listed in this paper and the results show that ProjR achieves better performance in most of the substructures. {\textcopyright} 2018, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 7th CCF International Conference on Natural Language Processing and Chinese Computing, NLPCC 2018 ; Conference Date: 26 August 2018 Through 30 August 2018; Conference Code:217149},
author = {Zhang, W and Li, J and Chen, H},
doi = {10.1007/978-3-319-99495-6_13},
editor = {{Zhao D. Li S.}, Zhang M Ng V Zan H},
isbn = {9783319994949},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Benchmark datasets; Combination operators; Combin,Natural language processing systems,Vector spaces},
pages = {145--157},
publisher = {Springer Verlag},
title = {{ProjR: Embedding Structure Diversity for Knowledge Graph Completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052918844&doi=10.1007%2F978-3-319-99495-6_13&partnerID=40&md5=1634c0b19ea3de176d017dde79e37bca},
volume = {11108 LNAI},
year = {2018}
}
@inproceedings{Xu20141219,
abstract = {Representing words into vectors in continuous space can form up a potentially powerful basis to generate high-quality textual features for many text mining and natural language processing tasks. Some recent efforts, such as the skip-gram model, have attempted to learn word representations that can capture both syntactic and semantic information among text corpus. However, they still lack the capability of encoding the properties of words and the complex relationships among words very well, since text itself often contains incomplete and ambiguous information. Fortunately, knowledge graphs provide a golden mine for enhancing the quality of learned word representations. In particular, a knowledge graph, usually composed by entities (words, phrases, etc.), relations between entities, and some corresponding meta information, can supply invaluable relational knowledge that encodes the relationship between entities as well as categorical knowledge that encodes the attributes or properties of entities. Hence, in this paper, we introduce a novel framework called RC-NET to leverage both the relational and categorical knowledge to produce word representations of higher quality. Specifically, we build the relational knowledge and the categorical knowledge into two separate reg-ularization functions, and combine both of them with the original objective function of the skip-gram model. By solving this combined optimization problem using back propagation neural networks, we can obtain word representations enhanced by the knowledge graph. Experiments on popular text mining and natural language processing tasks, including analogical reasoning, word similarity, and topic prediction, have all demonstrated that our model can significantly improve the quality of word representations. Copyright 2014 ACM.},
annote = {cited By 102; Conference of 23rd ACM International Conference on Information and Knowledge Management, CIKM 2014 ; Conference Date: 3 November 2014 Through 7 November 2014; Conference Code:109104},
author = {Xu, C and Bai, Y and Bian, J and Gao, B and Wang, G and Liu, X and Liu, T.-Y.},
booktitle = {CIKM 2014 - Proceedings of the 2014 ACM International Conference on Information and Knowledge Management},
doi = {10.1145/2661829.2662038},
isbn = {9781450325981},
keywords = {Analogical reasoning; Back propagation neural net,Backpropagation; Complex networks; Computational l,Knowledge management},
pages = {1219--1228},
publisher = {Association for Computing Machinery, Inc},
title = {{RC-NET: A general framework for incorporating knowledge into word representations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937567033&doi=10.1145%2F2661829.2662038&partnerID=40&md5=b6fdb47bf7af72dfb02f96491b291832},
year = {2014}
}
@inproceedings{Eddamiri2018367,
abstract = {With the increasing amount of Linked Data on the Web in the past decade, there is a growing desire for machine learning community to bring this type of data into the fold. However, while Linked Data and Machine Learning have seen an explosive growth in popularity, relatively little attention has been paid in the literature to the possible union of both Linked Data and Machine Learning. The best way to collaborate these two fields is to focus on RDF data. After a thorough overview of Machine learning pipeline on RDF data, the paper presents an unsupervised feature extraction technique named Walks and two language modeling approaches, namely Word2vec and Doc2vec. In order to adapt the RDF graph to the clustering mechanism, we first applied the Walks technique on several sequences of entities by combining it with the Word2Vec approach. However, the application of the Doc2vec approach to a set of walks gives better results on two different datasets. Copyright 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
annote = {cited By 2; Conference of 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2018 ; Conference Date: 18 September 2018 Through 20 September 2018; Conference Code:143001},
author = {Eddamiri, S and Zemmouri, E M and Benghabrit, A},
booktitle = {IC3K 2018 - Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
doi = {10.5220/0007228903670373},
editor = {{Fred A.}, Filipe J},
isbn = {9789897583308},
keywords = {Cluster analysis; Knowledge engineering; Knowledge,Clustering; Clustering mechanism; Doc2vec; Explos,K-means clustering},
pages = {367--373},
publisher = {SciTePress},
title = {{RDF data clustering based on resource and predicate embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059018629&doi=10.5220%2F0007228903670373&partnerID=40&md5=39e63af40c5d8253773386702c12afe3},
volume = {1},
year = {2018}
}
@article{Ristoski2019721,
abstract = {Linked Open Data has been recognized as a valuable source for background information in many data mining and information retrieval tasks. However, most of the existing tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler-Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. We evaluate our approach on three different tasks: (i) standard machine learning tasks, (ii) entity and document modeling, and (iii) content-based recommender systems. The evaluation shows that the proposed entity embeddings outperform existing techniques, and that pre-computed feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks. {\textcopyright} 2019 - IOS Press and the authors. All rights reserved.},
annote = {cited By 10},
author = {Ristoski, P and Rosati, J and {Di Noia}, T and {De Leone}, R and Paulheim, H},
doi = {10.3233/SW-180317},
issn = {15700844},
journal = {Semantic Web},
number = {4},
pages = {721--752},
publisher = {IOS Press},
title = {{RDF2Vec: RDF graph embeddings and their applications}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066433191&doi=10.3233%2FSW-180317&partnerID=40&md5=b00f21e937f0045b025d767fb93acaa2},
volume = {10},
year = {2019}
}
@inproceedings{10.1145/3394486.3403268,
abstract = {Cross-lingual entity alignment aims at associating semantically similar entities in knowledge graphs with different languages. It has been an essential research problem for knowledge integration and knowledge graph connection, and been studied with supervised or semi-supervised machine learning methods with the assumption of clean labeled data. However, labels from human annotations often include errors, which can largely affect the alignment results. We thus aim to formulate and explore the robust entity alignment problem, which is non-trivial, due to the deficiency of noisy labels. Our proposed method named REA (Robust Entity Alignment) consists of two components: noise detection and noise-aware entity alignment. The noise detection is designed by following the adversarial training principle. The noise-aware entity alignment is devised by leveraging graph neural network based knowledge graph encoder as the core. In order to mutually boost the performance of the two components, we propose a unified reinforced training strategy to combine them. To evaluate our REA method, we conduct extensive experiments on several real-world datasets. The experimental results demonstrate the effectiveness of our proposed method and also show that our model consistently outperforms the state-of-the-art methods with significant improvement on alignment accuracy in the noise-involved scenario.},
address = {New York, NY, USA},
author = {Pei, Shichao and Yu, Lu and Yu, Guoxian and Zhang, Xiangliang},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3394486.3403268},
isbn = {9781450379984},
keywords = {entity alignment,knowledge graph,noise detection},
pages = {2175--2184},
publisher = {Association for Computing Machinery},
series = {KDD '20},
title = {{REA: Robust Cross-Lingual Entity Alignment Between Knowledge Graphs}},
url = {https://doi.org/10.1145/3394486.3403268},
year = {2020}
}
@inproceedings{10.1145/3394486.3403223,
abstract = {Recipe representation plays an important role in food computing for perception, recognition, recommendation and other applications. Learning pretrained recipe embeddings is a challenging task, as there is a lack of high quality annotated food datasets. In this paper, we provide a joint approach for learning effective pretrained recipe embeddings using both the ingredients and cooking instructions. We present RECIPTOR, a novel set transformer-based joint model to learn recipe representations, that preserves permutation-invariance for the ingredient set and uses a novel knowledge graph (KG) derived triplet sampling approach to optimize the learned embeddings so that related recipes are closer in the latent semantic space. The embeddings are further jointly optimized by combining similarity among cooking instructions with a KG based triplet loss. We experimentally show that RECIPTOR's recipe embeddings outperform state-of-the-art baselines on two newly designed downstream classification tasks by a wide margin.},
address = {New York, NY, USA},
author = {Li, Diya and Zaki, Mohammed J},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3394486.3403223},
isbn = {9781450379984},
keywords = {food computing,food knowledge graph,recipe embedding,representation learning,set transformer},
pages = {1719--1727},
publisher = {Association for Computing Machinery},
series = {KDD '20},
title = {{RECIPTOR: An Effective Pretrained Model for Recipe Representation Learning}},
url = {https://doi.org/10.1145/3394486.3403223},
year = {2020}
}
@article{Ma2019250,
abstract = {Both recommender systems and knowledge graphs can provide overall and detailed views on datasets, and each of them has been a hot research domain by itself. However, recommending items with a pre-constructed knowledge graph or without one often limits the recommendation performance. Similarly, constructing and completing a knowledge graph without a target is insufficient for applications, such as recommendation. In this paper, we address the problems of recommendation together with knowledge graph completion by a novel model named RecKGC that generates a completed knowledge graph and recommends items for users simultaneously. Comprehensive representations of users, items and interactions/relations are learned in each respective domain, such as our attentive embeddings that integrate tuples in a knowledge graph for recommendation and our high-level interaction representations of entities and relations for knowledge graph completion. We join the tasks of recommendation and knowledge graph completion by sharing the comprehensive representations. As a result, the performance of recommendation and knowledge graph completion are mutually enhanced, which means that the recommendation is getting more effective while the knowledge graph is getting more informative. Experiments validate the effectiveness of the proposed model on both tasks. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 15th International Conference on Advanced Data Mining and Applications, ADMA 2019 ; Conference Date: 21 November 2019 Through 23 November 2019; Conference Code:234319},
author = {Ma, J and Zhong, M and Wen, J and Chen, W and Zhou, X and Li, X},
doi = {10.1007/978-3-030-35231-8_18},
editor = {{Li J. Wang S.}, Qin S Li X Wang S},
isbn = {9783030352301},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Big data; Data visualization; Flow visualization;,Data mining,High-level interactions; Knowledge graphs; Recomm},
pages = {250--265},
publisher = {Springer},
title = {{RecKGC: Integrating Recommendation with Knowledge Graph Completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076575010&doi=10.1007%2F978-3-030-35231-8_18&partnerID=40&md5=a699d4638aa093fe4388372a35d77e36},
volume = {11888 LNAI},
year = {2019}
}
@inproceedings{Czachor2018,
abstract = {Word embeddings were used for the extraction of hyponymy relation in several approaches, but also it was recently shown that they should not work, in fact. In our work we verified both claims using a very large wordnet of Polish as a gold standard for lexico-semantic relations and word embeddings extracted from a very large corpus of Polish. We showed that a hyponymy extraction method based on linear regression classifiers trained on clusters of vectors can be successfully applied on large scale. We presented also a possible explanation for contradictory findings in the literature. Moreover, in order to show the feasibility of the method we extended it to the recognition of meronymy. {\textcopyright} 2018 Global WordNet Association. All rights reserved.},
annote = {cited By 1; Conference of 9th Global WordNet Conference, GWC 2018 ; Conference Date: 8 January 2018 Through 12 January 2018; Conference Code:133484},
author = {Czachor, G and Piasecki, M and Janz, A},
booktitle = {GWC 2018 - 9th Global WordNet Conference},
keywords = {Embeddings; Extraction method; Gold standards; Hy,Extraction; Ontology; Semantics,Natural language processing systems},
publisher = {Global WordNet Association},
title = {{Recognition of hyponymy and meronymy relations in word embeddings for Polish}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043703753&partnerID=40&md5=b4d643179d230b16cdd93f5295dc703a},
volume = {2018-Janua},
year = {2018}
}
@article{Zheng2017878,
abstract = {In view of the lack of subjectivity and accuracy in the traditional micro-blog opinion leader recognition method to measure the important factors of users, a new micro-blog opinion leader recognition method is proposed. This paper used the linked data to describe the micro-blog data, used the association rule mining algorithm to quantitatively determine the important factors that affected the users' ranking, and constructed the opinion leader recognition model according to the index scoring method. Experiments show that our method using linked data identifies the opinion leaders the same as the standard leaders, is more accurate and has better feasibility than that of traditional data. {\textcopyright} 2017 Totem Publisher, Inc. All rights reserved.},
annote = {cited By 0},
author = {Zheng, Z and Li, P and Zhang, X and Li, D},
doi = {10.23940/ijpe.17.06.p9.878885},
issn = {09731318},
journal = {International Journal of Performability Engineering},
keywords = {Association rules; Data handling; Data mining,Blogs,Linked datum; Micro-blog; Opinion leaders; Recogn},
number = {6},
pages = {878--885},
publisher = {Totem Publishers Ltd},
title = {{Recognition of opinion leaders in micro-blog based on linked data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031927271&doi=10.23940%2Fijpe.17.06.p9.878885&partnerID=40&md5=9364262db3f4042f0527cd3d0b39ce81},
volume = {13},
year = {2017}
}
@inproceedings{Kang20181143,
abstract = {Recommender Systems have proliferated as general-purpose approaches to model a wide variety of consumer interaction data. Specific instances make use of signals ranging from user feedback, item relationships, geographic locality, social influence (etc.). Typically, research proceeds by showing that making use of a specific signal (within a carefully designed model) allows for higher-fidelity recommendations on a particular dataset. Of course, the real situation is more nuanced, in which a combination of many signals may be at play, or favored in different proportion by individual users. Here we seek to develop a framework that is capable of combining such heterogeneous item relationships by simultaneously modeling (a) what modality of recommendation is a user likely to be susceptible to at a particular point in time; and (b) what is the best recommendation from each modality. Our method borrows ideas from mixtures-of-experts approaches as well as knowledge graph embeddings. We find that our approach naturally yields more accurate recommendations than alternatives, while also providing intuitive 'explanations' behind the recommendations it provides. {\textcopyright} 2018 Association for Computing Machinery.},
annote = {cited By 10; Conference of 27th ACM International Conference on Information and Knowledge Management, CIKM 2018 ; Conference Date: 22 October 2018 Through 26 October 2018; Conference Code:142310},
author = {Kang, W.-C. and Wan, M and McAuley, J},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3269206.3271792},
editor = {{Paton N. Candan S.}, Wang H Allan J Agrawal R Labrinidis A Cuzzocrea A Zaki M Srivastava D Broder A Schuster A},
isbn = {9781450360142},
keywords = {Consumer interaction; Designed models; Different,Knowledge management,Mixtures},
pages = {1143--1152},
publisher = {Association for Computing Machinery},
title = {{Recommendation through mixtures of heterogeneous item relationships}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058060800&doi=10.1145%2F3269206.3271792&partnerID=40&md5=9d3466da0f8d49ea8258e47f283c900d},
year = {2018}
}
@inproceedings{Alam2017,
abstract = {The reconciled knowledge graphs are typically used for multidocument summarization, or to detect knowledge evolution across document series. This paper focuses on reconciling knowledge graphs generated from two text documents about similar events described differently. Our approach employs and extends MERGILO, a tool for reconciling knowledge graphs extracted from text, using word similarity and graph alignment. Complete semantic representation of events are generated using FRED, a semantic web machine reader, jointly with Framester, a linguistic linked data hub represented using a novel formal semantics for frames. Event-reconciliation is mainly performed via similarities based on the graph structure of frames using RDF2Vec graph embeddings, and the subsumption hierarchy of semantic roles as defined in Framester. Our approach is evaluated over a coreference resolution task. {\textcopyright} 2017 CEUR-WS. All rights reserved.},
annote = {cited By 0; Conference of 2017 Joint International Workshops on Hybrid Statistical Semantic Understanding and Emerging Semantics, and Semantic Statistics, HybridSemStats 2017 ; Conference Date: 22 October 2017; Conference Code:135717},
author = {Alam, M and Recupero, D R and Mongiovi, M and Gangemi, A and Ristoski, P},
booktitle = {CEUR Workshop Proceedings},
editor = {{Szekely P. Guha R.V.}, Kalampokis E Troncy R Cotton F Dong X L Hitzler P Kejriwal M Sivakumar D Haller A Capadisli S Lecue F Witbrock M},
issn = {16130073},
keywords = {Embeddings; Event reconciliation; Frame similarit,Formal methods; Natural language processing system,Semantic Web},
publisher = {CEUR-WS},
title = {{Reconciling event-based knowledge through RDF2Vec}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045919780&partnerID=40&md5=03b9e4e233fc66c982b6cc1f02358b2f},
volume = {1923},
year = {2017}
}
@inproceedings{Takhom201720,
abstract = {Multiple thinking of different stakeholders has to influence collabo-rative working. Some part of information exchange is fragment knowledge that is a significant challenge to complete knowledge co-creation from various do-mains. However, an effective obstacle is miscommunication among the stake-holders, particularly when ambiguous terms are mentioned in the discussion contexts. To overcome the challenge, this paper proposes an integration ap-proach of network text analysis and knowledge graph embedding. The approach is employed for understanding semantic meaning of terms from a source of knowledge, as a discussion forum. We calculate each term detected by our cross-domain codebook onto the vector space and straightforwardly investigate the relationship among questions and answers on it. To demonstrate the benefits of employing the approach, the system's functionality is implemented to mani-fest a capability of detecting and reducing miscommunication by the case study of Life Cycle Assessment's discussion board.},
annote = {cited By 2; Conference of 7th Joint International Semantic Technology Conference on Workshop and Poster, JIST-WP 2017 ; Conference Date: 10 November 2017 Through 12 November 2017; Conference Code:132027},
author = {Takhom, A and Boonkwan, P and Ikeda, M and Usanavasin, S and Supnithi, T},
booktitle = {CEUR Workshop Proceedings},
keywords = {Concept discoveries; Concepts extractions; Cross-,Knowledge management; Life cycle; Natural language,Semantic Web},
pages = {20--31},
title = {{Reducing miscommunication in cross-disciplinary concept discovery using network text analysis and semantic embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037527462&partnerID=40&md5=016e6c7cb479f24f27b2389ebef1e9b4},
volume = {2000},
year = {2017}
}
@article{Minervini2017668,
abstract = {Learning embeddings of entities and relations using neural architectures is an effective method of performing statistical learning on large-scale relational data, such as knowledge graphs. In this paper, we consider the problem of regularizing the training of neural knowledge graph embeddings by leveraging external background knowledge. We propose a principled and scalable method for leveraging equivalence and inversion axioms during the learning process, by imposing a set of model-dependent soft constraints on the predicate embeddings. The method has several advantages: (i) the number of introduced constraints does not depend on the number of entities in the knowledge base; (ii) regularities in the embedding space effectively reflect available background knowledge; (iii) it yields more accurate results in link prediction tasks over non-regularized methods; and (iv) it can be adapted to a variety of models, without affecting their scalability properties. We demonstrate the effectiveness of the proposed method on several large knowledge graphs. Our evaluation shows that it consistently improves the predictive accuracy of several neural knowledge graph embedding models (for instance, the MRR of TransE on WordNet increases by 11%) without compromising their scalability properties. {\textcopyright} 2017, Springer International Publishing AG.},
annote = {cited By 10; Conference of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2017 ; Conference Date: 18 September 2017 Through 22 September 2017; Conference Code:209269},
author = {Minervini, P and Costabello, L and Mu{\~{n}}oz, E and Nov{\'{a}}{\v{c}}ek, V and Vandenbussche, P.-Y.},
doi = {10.1007/978-3-319-71249-9_40},
editor = {{Ceci M. Dzeroski S.}, Vens C Todorovski L Hollmen J},
isbn = {9783319712482},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Artificial intelligence; Knowledge based systems;,Back-ground knowledge; Knowledge graphs; Learning,Knowledge management},
pages = {668--683},
publisher = {Springer Verlag},
title = {{Regularizing Knowledge Graph Embeddings via Equivalence and Inversion Axioms}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040231166&doi=10.1007%2F978-3-319-71249-9_40&partnerID=40&md5=b003c0b5e4d25ba27dcfaa025a6379cc},
volume = {10534 LNAI},
year = {2017}
}
@article{Wang201962,
abstract = {Relation extraction (RE) is an important task and has wide applications. Distant supervision is widely used in RE methods which can automatically construct labeled data to reduce the manual annotation effort. This method usually results in many instances with incorrect labels. In addition, most of existing relation extraction methods merely rely on the textual content of sentences to extract relation. In fact, many knowledge graphs are off-the-shelf and they can provide useful information of entities and relations, which has the potential to alleviate the noisy data problem and improve the performance of relation extraction. In this paper, we propose a knowledge-aware attention model to incorporate the knowledge graph information into relation extraction. In our approach, we first learn the representations of entities and relations from knowledge graph using graph embedding methods. Then we propose a knowledge-aware word attention model to select the informative words in sentences for relation extraction. In addition, we also propose a knowledge-aware sentence attention model to select useful sentences for RE to alleviate the problem of noisy data brought by distant supervision. We conduct experiments on a widely used dataset and the results show that our approach can effectively improve the performance of neural relation extraction. {\textcopyright} Springer Nature Singapore Pte Ltd 2019.},
annote = {cited By 0; Conference of 4th China Conference on Knowledge Graph and Semantic Computing, CCKS 2019 ; Conference Date: 24 August 2019 Through 27 August 2019; Conference Code:235759},
author = {Wang, P and Liu, H and Wu, F and Song, J and Xu, H and Wang, W},
doi = {10.1007/978-981-15-1956-7_6},
editor = {{Zhu X. Qin B.}, Liu M Zhu X Qian L},
isbn = {9789811519550},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Attention; Attention model; Graph embeddings; Kno,Data mining,Extraction; Semantics},
pages = {62--73},
publisher = {Springer},
title = {{REKA: Relation Extraction with Knowledge-Aware Attention}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078463306&doi=10.1007%2F978-981-15-1956-7_6&partnerID=40&md5=5fbb7b0d06458e8ed261ae55ffb9eadb},
volume = {1134 CCIS},
year = {2019}
}
@article{Wang202053,
abstract = {Point-of-Interest (POI) recommendation is one of the most important location-based services helping people discover interesting venues or services. However, the extreme user-POI matrix sparsity and the varying spatio-temporal context pose challenges for POI systems, which affects the quality of POI recommendations. To this end, we propose a translation-based relation embedding for POI recommendation. Our approach encodes the temporal and geographic information, as well as semantic contents effectively in a low-dimensional relation space by using Knowledge Graph Embedding techniques. To further alleviate the issue of user-POI matrix sparsity, a combined matrix factorization framework is built on a user-POI graph to enhance the inference of dynamic personal interests by exploiting the side-information. Experiments on two real-world datasets demonstrate the effectiveness of our proposed model. {\textcopyright} Springer Nature Switzerland AG 2020.},
annote = {cited By 0; Conference of 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2020 ; Conference Date: 11 May 2020 Through 14 May 2020; Conference Code:240129},
author = {Wang, X and Salim, F D and Ren, Y and Koniusz, P},
doi = {10.1007/978-3-030-47426-3_5},
editor = {{Lauw H.W. Lim E.-P.}, Wong R.C.-W. Ntoulas A Ng S.-K. Pan S J},
isbn = {9783030474256},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining; Embeddings; Factorization; Matrix alg,Geographic information; Knowledge graphs; Matrix,Location based services},
pages = {53--64},
publisher = {Springer},
title = {{Relation Embedding for Personalised Translation-Based POI Recommendation}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734138&doi=10.1007%2F978-3-030-47426-3_5&partnerID=40&md5=ac60e5dc982a3eb9b74f5a6bce22599e},
volume = {12084 LNAI},
year = {2020}
}
@inproceedings{9073093,
abstract = {Security Analysts that work in a `Security Operations Center' (SoC) play a major role in ensuring the security of the organization. The amount of background knowledge they have about the evolving and new attacks makes a significant difference in their ability to detect attacks. Open source threat intelligence sources, like text descriptions about cyber-attacks, can be stored in a structured fashion in a cybersecurity knowledge graph. A cybersecurity knowledge graph can be paramount in aiding a security analyst to detect cyber threats because it stores a vast range of cyber threat information in the form of semantic triples which can be queried. A semantic triple contains two cybersecurity entities with a relationship between them. In this work, we propose a system to create semantic triples over cybersecurity text, using deep learning approaches to extract possible relationships. We use the set of semantic triples generated through our system to assert in a cybersecurity knowledge graph. Security Analysts can retrieve this data from the knowledge graph, and use this information to form a decision about a cyber-attack.},
author = {Pingle, A and Piplai, A and Mittal, S and Joshi, A and Holt, J and Zak, R},
booktitle = {2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
doi = {10.1145/3341161.3343519},
issn = {2473-991X},
keywords = {graph theory;information retrieval;learning (artif},
month = {aug},
pages = {879--886},
title = {{RelExt: Relation Extraction using Deep Learning approaches for Cybersecurity Knowledge Graph Improvement}},
year = {2019}
}
@article{Li2020,
abstract = {Most of the existing knowledge graph embedding models are supervised methods and largely relying on the quality and quantity of obtainable labelled training data. The cost of obtaining high quality triples is high and the data sources are facing a serious problem of data sparsity, which may result in insufficient training of long-tail entities. However, unstructured text encoding entities and relational knowledge can be obtained anywhere in large quantities. Word vectors of entity names estimated from the unlabelled raw text using natural language model encode syntax and semantic properties of entities. Yet since these feature vectors are estimated through minimizing prediction error on unsupervised entity names, they may not be the best for knowledge graphs. We propose a two-phase approach to adapt unsupervised entity name embeddings to a knowledge graph subspace and jointly learn the adaptive matrix and knowledge representation. Experiments on Freebase show that our method can rely less on the labelled data and outperforms the baselines when the labelled data is relatively less. Especially, it is applicable to zero-shot scenario. {\textcopyright} 2020 Chunhua Li et al.},
annote = {cited By 0},
author = {Li, C and Xian, X and Ai, X and Cui, Z},
doi = {10.1155/2020/4741963},
issn = {10589244},
journal = {Scientific Programming},
keywords = {Embeddings; Encoding (symbols); Natural language p,Feature vectors; Knowledge graphs; Natural langua,Knowledge representation},
publisher = {Hindawi Limited},
title = {{Representation Learning of Knowledge Graphs with Embedding Subspaces}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092061749&doi=10.1155%2F2020%2F4741963&partnerID=40&md5=7a7df2a1da68d4a421df0133445fd790},
volume = {2020},
year = {2020}
}
@article{8950182,
abstract = {Most of the existing knowledge representation learning methods project the entities and relations represented by symbols in the knowledge graph into the low-dimensional vector space from the perspective of the structure and semantics of triples, and express the complex relations between entities and relations with dense low-dimensional vectors. However, triples in the knowledge graph not only contain relation triples, but also contain a large number of attribute triples. Existing knowledge representation methods often confuse these two kinds of triples and pay little attention to the semantic information contained in attributes and attribute values. In this paper, a novel representation learning method which makes use of the attribute information of entities is proposed. Specifically, deep convolutional neural network model is used to encode attribute information of entities, and both attribute information and triple structure information are utilized to learn knowledge representation, and then generate attribute-based representation of entities. The knowledge graph completion task was used to evaluate this method, and the experimental results on open data sets FB15K and FB24k showed that the attribute-embodied knowledge representation learning model outperforms the other baselines.},
author = {Zhang, Z and Cao, L and Chen, X and Tang, W and Xu, Z and Meng, Y},
doi = {10.1109/ACCESS.2020.2963990},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {convolutional neural nets;knowledge representation},
pages = {7435--7441},
title = {{Representation Learning of Knowledge Graphs With Entity Attributes}},
volume = {8},
year = {2020}
}
@article{Cheng2019282,
abstract = {Representation learning of knowledge graphs has gained wide attention in the field of natural language processing. Most existing knowledge representation models for knowledge graphs embed triples into a continuous low-dimensional vector space through a simple linear transformation. In spite of high computation efficiency, the fitting ability of these models is suboptimal. In this paper, we propose a multi-scale capsule network to model relations between embedding vectors from a deep perspective. We use convolution kernels with different sizes of windows in the convolutional layer inside a Capsule network to extract semantic features of entities and relations in triples. These semantic features are then represented as a continuous vector through a routing process algorithm in the capsule layer. The modulus of this vector is used as the score of confidence of correctness of a triple. Experiments show that the proposed model obtains better performance than state-of-the-art embedding models for the task of knowledge graph completion over two benchmarks, WN18RR and FB15k-237. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 20th International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2019 ; Conference Date: 14 November 2019 Through 16 November 2019; Conference Code:234559},
author = {Cheng, J and Yang, Z and Dang, J and Pan, C and Zhang, F},
doi = {10.1007/978-3-030-33607-3_31},
editor = {{Yin H. Allmendinger R.}, Camacho D Tino P Tallon-Ballesteros A J Menezes R},
isbn = {9783030336066},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Benchmarking; Convolution; Embeddings; Knowledge r,Convolution kernel; Dynamic routing; High computa,Vector spaces},
pages = {282--290},
publisher = {Springer},
title = {{Representation learning of knowledge graphs with multi-scale capsule network}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076639545&doi=10.1007%2F978-3-030-33607-3_31&partnerID=40&md5=5a4e052e97ade38ba311eeae52cd53e5},
volume = {11871 LNCS},
year = {2019}
}
@inproceedings{9173125,
abstract = {This paper proposes a novel model for learning the embedding representation of the given first-order logic query and then leading the task of Question Answering (QA) on knowledge graph, especially handling the complex logical queries and mining the multi-hop paths contained in the knowledge graph. In the proposed model, the node (i.e., entity) embeddings and the query embeddings are trained jointly in the same latent semantic feature-space, so that we could measure the matching degree among query (i.e., question) and candidate entities (i.e., answers) by using the semantic distances among them. Recent years have witnessed great advance of QA models based on knowledge graph, however, the main difference between these previous work and ours, is that traditional QA focuses on understanding the natural language, while we are more attentive to detail in modeling and understanding the logical form. The experiments on several knowledge graph reasoning tasks with the real-world datasets demonstrate that, the proposed logical query learning model is more effective than the state-of-the-art models.},
author = {Wang, Y and Zhang, H and Liu, Y},
booktitle = {2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)},
doi = {10.1109/ICSIDP47821.2019.9173125},
keywords = {data mining;graph theory;inference mechanisms;lear},
month = {dec},
pages = {1--6},
title = {{Representation Learning of Logical Query for Knowledge Reasoning}},
year = {2019}
}
@inproceedings{8954374,
abstract = {Graph convolutional neural networks have recently shown great potential for the task of zero-shot learning. These models are highly sample efficient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a lack of data. However, multi-layer architectures, which are required to propagate knowledge to distant nodes in the graph, dilute the knowledge by performing extensive Laplacian smoothing at each layer and thereby consequently decrease performance. In order to still enjoy the benefit brought by the graph structure while preventing dilution of knowledge from distant nodes, we propose a Dense Graph Propagation (DGP) module with carefully designed direct links among distant nodes. DGP allows us to exploit the hierarchical graph structure of the knowledge graph through additional connections. These connections are added based on a node's relationship to its ancestors and descendants. A weighting scheme is further used to weigh their contribution depending on the distance to the node to improve information propagation in the graph. Combined with finetuning of the representations in a two-stage training approach our method outperforms state-of-the-art zero-shot learning approaches.},
author = {Kampffmeyer, M and Chen, Y and Liang, X and Wang, H and Zhang, Y and Xing, E P},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2019.01175},
issn = {2575-7075},
keywords = {convolutional neural nets;graph theory;learning (a},
month = {jun},
pages = {11479--11488},
title = {{Rethinking Knowledge Graph Propagation for Zero-Shot Learning}},
year = {2019}
}
@inproceedings{Rosso20192465,
abstract = {Jointly learning embeddings from text and a Knowledge Graph benefits both word and entity/relation embeddings by taking advantage of both large-scale unstructured content (text) and high-quality structured data (the Knowledge Graph). Current techniques leverage anchors to associate entities in the Knowledge Graph to corresponding words in the text corpus; these anchors are then used to generate additional learning samples during the embedding learning process. However, we show in this paper that such techniques yield suboptimal results, as they fail to control the amount of shared information between the two data sources during the joint learning process. Moreover, the additional learning samples often incur significant computational overhead. Aiming at releasing the power of such joint embeddings, we propose JOINER, a new joint text and Knowledge Graph embedding method using regularization. JOINER not only preserves co-occurrence between words in a text corpus and relations between entities in a Knowledge Graph, it also provides the flexibility to control the amount of information shared between the two data sources via regularization. Our method does not generate additional learning samples, which makes it computationally efficient. Our extensive empirical evaluation on real datasets shows the superiority of JOINER across different evaluation tasks, including analogical reasoning, link prediction, and relation extraction. Compared to state-of-the-art techniques generating additional learning samples from a set of anchors, our method yields better results (with up to 4.3% absolute improvement) and significantly less computational overhead (76% less learning time overhead). {\textcopyright} 2019 IEEE.},
annote = {cited By 2; Conference of 2019 IEEE International Conference on Big Data, Big Data 2019 ; Conference Date: 9 December 2019 Through 12 December 2019; Conference Code:157991},
author = {Rosso, P and Yang, D and Cudre-Mauroux, P},
booktitle = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
doi = {10.1109/BigData47090.2019.9005462},
editor = {{Baru C. Huan J.}, Khan L Hu X T Ak R Tian Y Barga R Zaniolo C Lee K Ye Y F},
isbn = {9781728108582},
keywords = {Amount of information; Analogical reasoning; Comp,Anchors; Big data; Embeddings; Knowledge managemen,Learning systems},
pages = {2465--2473},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Revisiting Text and Knowledge Graph Joint Embeddings: The Amount of Shared Information Matters!}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081361492&doi=10.1109%2FBigData47090.2019.9005462&partnerID=40&md5=1e2be8d1f517d608cee279e0be549422},
year = {2019}
}
@inproceedings{Huo2019,
abstract = {As knowledge graph becomes popular in recent years, more and more attention has been paid to Knowledge Base Question-Answer (KBQA) systems. For KBQA systems, Question Understanding, as the first stage, aims to convert factual question into the interpretable form to machine just like ?-DCS. And some latest works used query subgraph to change the Question Understanding task into the Question to Subgraph(Question2Subgraph) task with which the subgraph can be simply and directly mapped to ?-DCS. In this paper, we focus on factual question to subgraph task (Qf , G) and prove that more complex questions can be easily solved based on it. Then, we propose a novel framework with Rule Inference and Sentence Schema Graph Embedding (RI-SSGE) to solve (Qf , G) task. Inspired by isomeride structures in Chemistry, we concentrate RI-SSGE on structure detection of questions to avoid the problem of poor generalization in other models, which are based on templates on various specific domain knowledge graphs. To address the problem of error propagation, RI-SSGE creatively combines the traditional rule inference method and the graph representation method together, and thus guarantees the performance of the whole framework. Having observed that human can exploit the hidden relations by joining the question and the knowledge graph structure together, we raise a novel Sentence-Schema-Graph (SSG) in the last network representation learning stage of RI-SSGE, which is designed to imitate human's way of thinking. We experimented on Geoquery-880 and AceQG[11] datasets which has 133,143 (Factual Question, Subgraph) pairs on an open academic knowledge graph and results demonstrate the advantages of RI-SSGE over other baselines. {\textcopyright} 2019 Association for Computing Machinery.},
annote = {cited By 0; Conference of 2019 ACM Turing Celebration Conference - China, ACM TURC 2019 ; Conference Date: 17 May 2019 Through 19 May 2019; Conference Code:151512},
author = {Huo, X and Wen, C and Yan, Y and Wang, R},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3321408.3321604},
isbn = {9781450371582},
keywords = {Complex questions; Error propagation; Graph embed,Embeddings; Knowledge based systems; Natural langu,Query processing},
publisher = {Association for Computing Machinery},
title = {{RI-SSGE: A framework with rule inference and sentence schema graph embedding for knowledge base query construction}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072826626&doi=10.1145%2F3321408.3321604&partnerID=40&md5=bff4b6871324b460a127445bd9e7483e},
year = {2019}
}
@article{4YDDUZZC,
abstract = {Visual question answering (VQA) that involves understanding an image and paired questions develops very quickly with the boost of deep learning in relevant research fields, such as natural language processing and computer vision. Existing works highly rely on the knowledge of the data set. However, some questions require more professional cues other than the data set knowledge to answer questions correctly. To address such an issue, we propose a novel framework named a knowledge-based augmentation network (KAN) for VQA. We introduce object-related open-domain knowledge to assist the question answering. Concretely, we extract more visual information from images and introduce a knowledge graph to provide the necessary common sense or experience for the reasoning process. For these two augmented inputs, we design an attention module that can adjust itself according to the specific questions, such that the importance of external knowledge against detected objects can be balanced adaptively. Extensive experiments show that our KAN achieves state-of-the-art performance on three challenging VQA data sets, i.e., VQA v2, VQA-CP v2, and FVQA. In addition, our open-domain knowledge is also beneficial to VQA baselines. Code is available at https://github.com/yyyanglz/KAN.},
author = {Zhang, L and Liu, S and Liu, D and Zeng, P and Li, X and Song, J and Gao, L},
issn = {2162-2388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Feature extraction;Visualization;Knowledge based s},
pages = {1--12},
title = {{Rich Visual Knowledge-Based Augmentation Network for Visual Question Answering}},
year = {2020}
}
@inproceedings{10.1145/3269206.3271739,
abstract = {To address the sparsity and cold start problem of collaborative filtering, researchers usually make use of side information, such as social networks or item attributes, to improve recommendation performance. This paper considers the knowledge graph as the source of side information. To address the limitations of existing embedding-based and path-based methods for knowledge-graph-aware recommendation, we propose RippleNet, an end-to-end framework that naturally incorporates the knowledge graph into recommender systems. Similar to actual ripples propagating on the water, RippleNet stimulates the propagation of user preferences over the set of knowledge entities by automatically and iteratively extending a user's potential interests along links in the knowledge graph. The multiple "ripples" activated by a user's historically clicked items are thus superposed to form the preference distribution of the user with respect to a candidate item, which could be used for predicting the final clicking probability. Through extensive experiments on real-world datasets, we demonstrate that RippleNet achieves substantial gains in a variety of scenarios, including movie, book and news recommendation, over several state-of-the-art baselines.},
address = {New York, NY, USA},
author = {Wang, Hongwei and Zhang, Fuzheng and Wang, Jialin and Zhao, Miao and Li, Wenjie and Xie, Xing and Guo, Minyi},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3269206.3271739},
isbn = {9781450360142},
keywords = {knowledge graph,preference propagation,recommender systems},
pages = {417--426},
publisher = {Association for Computing Machinery},
series = {CIKM '18},
title = {{RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems}},
url = {https://doi.org/10.1145/3269206.3271739},
year = {2018}
}
@inproceedings{Boytcheva2019161,
abstract = {This paper presents experiments in risk factors analysis based on clinical texts enhanced with Linked Open Data (LOD). The idea is to determine whether a patient has risk factors for a specific disease analyzing only his/her outpatient records. A semantic graph of "meta-knowledge" about a disease of interest is constructed, with integrated multilingual terms (labels) of symptoms, risk factors etc. coming from Wikidata, PubMed, Wikipedia and MESH, and linked to clinical records of individual patients via ICD-10 codes. Then a predictive model is trained to foretell whether patients are at risk to develop the disease of interest. The testing was done using outpatient records from a nation-wide repository available for the period 2011-2016. The results show improvement of the overall performance of all tested algorithms (kNN, Na{\"{i}}ve Bayes, Tree, Logistic regression, ANN), when the clinical texts are enriched with LOD resources. {\textcopyright} 2019 Association for Computational Linguistics (ACL). All rights reserved.},
annote = {cited By 0; Conference of 12th International Conference on Recent Advances in Natural Language Processing, RANLP 2019 ; Conference Date: 2 September 2019 Through 4 September 2019; Conference Code:155296},
author = {Boytcheva, S and Angelova, G and Angelov, Z},
booktitle = {International Conference Recent Advances in Natural Language Processing, RANLP},
doi = {10.26615/978-954-452-056-4_019},
editor = {{Angelova G. Mitkov R.}, Nikolova I Temnikova I Temnikova I},
isbn = {9789544520557},
issn = {13138502},
keywords = {Clinical records; Linked open data (LOD); Linked,Deep learning; Linked data; Natural language proce,Open Data},
pages = {161--167},
publisher = {Incoma Ltd},
title = {{Risk factors extraction from clinical texts based on linked open data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076495355&doi=10.26615%2F978-954-452-056-4_019&partnerID=40&md5=d573b5f2ab55d2495ef242b0594597ee},
volume = {2019-Septe},
year = {2019}
}
@inproceedings{6631179,
abstract = {There has been a recent interest in utilizing contextual knowledge to improve multi-label visual recognition for intelligent agents like robots. Natural Language Processing (NLP) can give us labels, the correlation of labels, and the ontological knowledge about them, so we can automate the acquisition of contextual knowledge. In this paper we show how to use tools from NLP in conjunction with Vision to improve visual recognition. There are two major approaches: First, different language databases organize words according to various semantic concepts. Using these, we can build special purpose databases that can predict the labels involved given a certain context. Here we build a knowledge base for the purpose of describing common daily activities. Second, statistical language tools can provide the correlations of different labels. We show a way to learn a language model from large corpus data that exploits these correlations and propose a general optimization scheme to integrate the language model into the system. Experiments conducted on three multi-label everyday recognition tasks support the effectiveness and efficiency of our approach, with significant gains in recognition accuracies when correlation information is used.},
author = {Yang, Y and Teo, C L and Ferm{\"{u}}ller, C and Aloimonos, Y},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631179},
issn = {1050-4729},
keywords = {computational linguistics;image recognition;intell},
month = {may},
pages = {4256--4262},
title = {{Robots with language: Multi-label visual recognition using NLP}},
year = {2013}
}
@inproceedings{Zwicklbauer2016425,
abstract = {Entity disambiguation is the task of mapping ambiguous terms in natural-language text to its entities in a knowledge base. It finds its application in the extraction of structured data in RDF (Resource Description Framework) from textual documents, but equally so in facilitating artificial intelligence applications, such as Semantic Search, Reasoning and Question \& Answering. We propose a new collective, graph-based disambiguation algorithm utilizing semantic entity and document embeddings for robust entity disambiguation. Robust thereby refers to the property of achieving better than state-of-the-art results over a wide range of very different data sets. Our approach is also able to abstain if no appropriate entity can be found for a specific surface form. Our evaluation shows, that our approach achieves significantly (>5%) better results than all other publicly available disambiguation algorithms on 7 of 9 datasets without data set specific tuning. Moreover, we discuss the influence of the quality of the knowledge base on the disambiguation accuracy and indicate that our algorithm achieves better results than non-publicly available state-of-the-art algorithms. {\textcopyright} 2016 ACM.},
annote = {cited By 44; Conference of 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2016 ; Conference Date: 17 July 2016 Through 21 July 2016; Conference Code:122573},
author = {Zwicklbauer, S and Seifert, C and Granitzer, M},
booktitle = {SIGIR 2016 - Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/2911451.2911535},
isbn = {9781450342902},
keywords = {Algorithms; Artificial intelligence; Graphic metho,Embeddings; Entity disambiguation; Natural langua,Semantic Web},
pages = {425--434},
publisher = {Association for Computing Machinery, Inc},
title = {{Robust and collective entity disambiguation through semantic embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980317091&doi=10.1145%2F2911451.2911535&partnerID=40&md5=c3b8636bfecbe78a6cdee5a1a9d36a18},
year = {2016}
}
@article{9178311,
abstract = {Automatic fact verification (FV) based on artificial intelligence is considered as a promising approach which can be used to identify misinformation distributed on the web. Even though previous FV using deep learning have made great achievements in single dataset (e.g., FEVER), the trained systems are unlikely to be capable of extracting evidence from heterogeneous web-sources and validating claims in accordance with evidence found on the Internet. Nevertheless, the heterogeneity covers abundant semantic information, which will help FV system identify misinformation in a more accurate way. The current work is the first attempt to make the combination of knowledge graph (KG) and graph neural network (GNN) to enhance the robustness of FV systems for heterogeneous information. As a result, it can be generalized to multi-domain datasets after training on a sufficient single one. To make information update and aggregate well on the collaborative graph, the present study proposes a double graph attention network (DGAT) framework which recursively propagates the embeddings from a node's neighbors to refine the node's embedding as well as applies an attention mechanism to classify the importance of the neighbors. We train and evaluate our system on FEVER, a single and benchmark dataset for FV, and then re-evaluate our system on UKP Snopes Corpus, a new richly annotated corpus for FV tasks on the basis of heterogeneous web sources. According to experimental results, although DGAT has no excellent advantages in a single dataset, it shows outstanding performance in more realistic and multi-domain datasets. Moreover, the current study also provides a feasible method for deep learning to have the ability to infer heterogeneous information robustly.},
author = {Wang, Y and Xia, C and Si, C and Yao, B and Wang, T},
doi = {10.1109/ACCESS.2020.3019586},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {graph theory;inference mechanisms;Internet;learnin},
pages = {157140--157150},
title = {{Robust Reasoning Over Heterogeneous Textual Information for Fact Verification}},
volume = {8},
year = {2020}
}
@article{Lyu2020145,
abstract = {A branch of question answering approaches translates natural language questions to SPARQL queries. The empty answer problem exists even when we have properly-translated ones, due to the heterogeneity and incompleteness of knowledge graphs. Existing methods use similarities, ontologies or embeddings to relax failed queries and obtain approximate answers, but they may lose efficacy in approximating simple queries with only one or two constraints because of their low accuracy and suitability for over-constrained ones. In this paper, we propose a rule-driven query expansion approach to expand failed queries for obtaining more accurate approximate answers. Specifically, we first automatically build high-quality rule sets for predicates in failed queries with rule learning techniques. Then, we use the learned rules to expand failed queries to get approximate answers and explain the reasons why we choose these answers. We develop two datasets to evaluate the effectiveness and efficiency of our approach and the results show that our approach achieves better results than several approaches based on similarities, ontologies and embeddings in approximating simple queries. {\textcopyright} Springer Nature Singapore Pte Ltd 2020.},
annote = {cited By 0; Conference of 9th Joint International Semantic Technology Conference, JIST 2019 ; Conference Date: 25 November 2019 Through 27 November 2019; Conference Code:237519},
author = {Lyu, X and Hu, W},
doi = {10.1007/978-981-15-3412-6_15},
editor = {{Wang X. Lisi F.A.}, Xiao G Botoeva E},
isbn = {9789811534119},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Effectiveness and efficiencies; Empty answer; Kno,Embeddings; Natural language processing systems; O,Query processing},
pages = {145--160},
publisher = {Springer},
title = {{RQE: Rule-driven query expansion to solve empty answers in SPARQL}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081168978&doi=10.1007%2F978-981-15-3412-6_15&partnerID=40&md5=048872743c4ddf0010ef6a551804f56d},
volume = {1157 CCIS},
year = {2020}
}
@inproceedings{Rychalska2016602,
abstract = {This paper describes our proposed solutions designed for a STS core track within the Se-mEval 2016 English Semantic Textual Similarity (STS) task. Our method of similarity detection combines recursive autoencoders with a WordNet award-penalty system that accounts for semantic relatedness, and an SVM classifier, which produces the final score from similarity matrices. This solution is further supported by an ensemble classifier, combining an aligner with a bi-directional Gated Recurrent Neural Network and additional features, which then performs Linear Support Vector Regression to determine another set of scores. {\textcopyright} 2016 Association for Computational Linguistics.},
annote = {cited By 25; Conference of 10th International Workshop on Semantic Evaluation, SemEval 2016 ; Conference Date: 16 June 2016 Through 17 June 2016; Conference Code:131705},
author = {Rychalska, B and Pakulska, K and Chodorowska, K and Walczak, W and Andruszkiewicz, P},
booktitle = {SemEval 2016 - 10th International Workshop on Semantic Evaluation, Proceedings},
doi = {10.18653/v1/s16-1091},
isbn = {9781941643952},
keywords = {Ensemble classifiers; Ensemble methods; Semantic,Natural language processing systems; Ontology; Rec,Semantics},
pages = {602--608},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Samsung Poland NLP team at SemEval-2016 task 1: Necessity for diversity; combining recursive autoencoders, WordNet and ensemble methods to measure semantic similarity}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029034070&doi=10.18653%2Fv1%2Fs16-1091&partnerID=40&md5=6bc4204e060dfb404d89be38cbab25a5},
year = {2016}
}
@inproceedings{Khalife201917,
abstract = {In this paper, we consider the named entity linking (NEL) problem. We assume a set of queries, named entities, that have to be identified within a knowledge base. This knowledge base is represented by a text database paired with a semantic graph, endowed with a classification of entities (ontology). We present state-of-the-art methods in NEL, and propose a new method for individual identification requiring few annotated data samples. We demonstrate its scalability and performance over standard datasets, for several ontology configurations. Our approach is wellmotivated for integration in real systems. Indeed, recent deep learning methods, despite their capacity to improve experimental precision, require lots of parameter tuning along with large volume of annotated data. {\textcopyright} 2019 EMNLP-IJCNLP 2019 - Graph-Based Methods for Natural Language Processing - Proceedings of the 13th Workshop. All rights reserved.},
annote = {cited By 1; Conference of 13th Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs 2019, in conjunction with the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019 ; Conference Date: 4 November 2019 Through 4 November 2019; Conference Code:159691},
author = {Khalife, S and Vazirgiannis, M},
booktitle = {EMNLP-IJCNLP 2019 - Graph-Based Methods for Natural Language Processing - Proceedings of the 13th Workshop},
isbn = {9781950737864},
keywords = {Classification (of information); Deep learning; Gr,Experimental precision; Graph-based methods; Indi,Natural language processing systems},
pages = {17--25},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Scalable graph-based method for individual named entity identification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083891730&partnerID=40&md5=aebe87ac99b64930d81d178fe74d073b},
year = {2019}
}
@inproceedings{9150954,
abstract = {In this paper, we propose a learning algorithm for training deep neural networks when there is not sufficient labeled data. To improve the generalization capabilities of the deep model, we adopt a learning scheme to train two related tasks simultaneously. One is the original task (target), and the other is an auxiliary task (source). In order to create a related auxiliary task, we leverage an available knowledge graph to query for semantically related concepts that are grounded in labeled images; hence we call our method KGAuxLearn. We jointly train the target and source tasks in a multi-task architecture. We evaluate our method on two fine-grained visual categorization benchmarks: Oxford Flowers 102 and CUB-200-2011. Our experiments demonstrate that the error rate reduced by at least 2.1% over finetuning for both datasets. We also improve the error rate by 1.36% and 2.93% over using randomly selected concepts as an auxiliary task for Oxford Flowers 102 and CUB-2002011, respectively. In addition, comparing our method with auxiliary data selection methods that do not use a knowledge graph, the error rate improves by 0.69% and 2.57% on Oxford Flowers 102 and CUB-200-2011, respectively.},
author = {Raisi, E and Bach, S H},
booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW50498.2020.00473},
issn = {2160-7516},
keywords = {image classification;knowledge based systems;learn},
month = {jun},
pages = {4026--4031},
title = {{Selecting Auxiliary Data Using Knowledge Graphs for Image Classification with Limited Labels}},
year = {2020}
}
@inproceedings{7940158,
abstract = {the amount of ontologies and semantic annotations available on the Web is constantly growing and heterogeneous data raises new challenges for the data mining community. Yet there are still many problems causing users extra problems in discovering knowledge or even failing to obtain the real and useful knowledge they need. In this paper, we survey some semantic data mining methods specifically focusing on association rules. However, there are few works that have focused in mining semantic web data itself. For extracting rules in semantic data, we present an intelligent data mining approach incorporated with domain. The paper contributes a new algorithm for discovery of new type of patterns from semantic data. This new type of patterns is appropriate for some data such as stock market. We take advantage of the knowledge encoded in the ontology and MICF measure to inference in three steps to prune the search space and generated rules to derive appropriate rules from thousands of rules. Some experiments performed on stock market data and show the usefulness and efficiency of the approach.},
author = {Asadifar, S and Kahani, M},
booktitle = {2017 2nd Conference on Swarm Intelligence and Evolutionary Computation (CSIEC)},
doi = {10.1109/CSIEC.2017.7940158},
keywords = {data mining;financial data processing;ontologies (},
month = {mar},
pages = {106--111},
title = {{Semantic association rule mining: A new approach for stock market prediction}},
year = {2017}
}
@inproceedings{Wang2020445,
abstract = {Recently, graph query is widely adopted for querying knowledge graphs. Given a query graph GQ, the graph query finds subgraphs in a knowledge graph G that exactly or approximately match GQ. We face two challenges on graph query: (1) the structural gap between GQ and the predefined schema in G causes mismatch with query graph, (2) users cannot view the answers until the graph query terminates, leading to a longer system response time (SRT). In this paper, we propose a semantic-guided and response-time-bounded graph query to return the top-k answers effectively and efficiently. We leverage a knowledge graph embedding model to build the semantic graph SGQ, and we define the path semantic similarity (pss) over SGQ as the metric to evaluate the answer's quality. Then, we propose an A∗ semantic search on SGQ to find the top-k answers with the greatest pss via a heuristic pss estimation. Furthermore, we make an approximate optimization on A∗ semantic search to allow users to trade off the effectiveness for SRT within a user- specific time bound. Extensive experiments over real datasets confirm the effectiveness and efficiency of our solution. {\textcopyright} 2020 IEEE.},
annote = {cited By 0; Conference of 36th IEEE International Conference on Data Engineering, ICDE 2020 ; Conference Date: 20 April 2020 Through 24 April 2020; Conference Code:160235},
author = {Wang, Y and Khan, A and Wu, T and Jin, J and Yan, H},
booktitle = {Proceedings - International Conference on Data Engineering},
doi = {10.1109/ICDE48307.2020.00045},
isbn = {9781728129037},
issn = {10844627},
keywords = {Approximate optimization; Effectiveness and effic,Economic and social effects; Graph theory; Knowled,Semantic Web},
pages = {445--456},
publisher = {IEEE Computer Society},
title = {{Semantic guided and response times bounded top-k similarity search over knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085864546&doi=10.1109%2FICDE48307.2020.00045&partnerID=40&md5=350bdebc45ccd32e18e531a4f11f497c},
volume = {2020-April},
year = {2020}
}
@article{Yu201924,
abstract = {Feature modeling of different modalities is a basic problem in current research of cross-modal information retrieval. Existing models typically project texts and images into one embedding space, in which semantically similar information will have a shorter distance. Semantic modeling of textural relationships is notoriously difficult. In this paper, we propose an approach to model texts using a featured graph by integrating multi-view textual relationships including semantic relationships, statistical co-occurrence, and prior relationships in knowledge base. A dual-path neural network is adopted to learn multi-modal representations of information and cross-modal similarity measure jointly. We use a Graph Convolutional Network (GCN) for generating relation-aware text representations, and use a Convolutional Neural Network (CNN) with non-linearities for image representations. The cross-modal similarity measure is learned by distance metric learning. Experimental results show that, by leveraging the rich relational semantics in texts, our model can outperform the state-of-the-art models by 3.4% on 6.3% in accuracy on two benchmark datasets. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 0; Conference of 12th International Conference on Knowledge Science, Engineering and Management, KSEM 2019 ; Conference Date: 28 August 2019 Through 30 August 2019; Conference Code:230379},
author = {Yu, J and Yang, C and Qin, Z and Yang, Z and Hu, Y and Shi, Z},
doi = {10.1007/978-3-030-29551-6_3},
editor = {{Douligeris C. Apostolou D.}, Karagiannis D},
isbn = {9783030295509},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolution; Knowledge based systems; Semantics,Convolutional networks; Cross-modal; Distance Met,Convolutional neural networks},
pages = {24--32},
publisher = {Springer},
title = {{Semantic Modeling of Textual Relationships in Cross-modal Retrieval}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081574459&doi=10.1007%2F978-3-030-29551-6_3&partnerID=40&md5=915cafa99e392ba9a0c385539e751a09},
volume = {11775 LNAI},
year = {2019}
}
@inproceedings{Yih20151321,
abstract = {We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simplifies the semantic matching problem. By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outperforms previous methods substantially, and achieves an F1 measure of 52.5% on the WEBQUESTIONS dataset. {\textcopyright} 2015 Association for Computational Linguistics.},
annote = {cited By 259; Conference of 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015 ; Conference Date: 26 July 2015 Through 31 July 2015; Conference Code:114195},
author = {Yih, W.-T. and Chang, M.-W. and He, X and Gao, J},
booktitle = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/p15-1128},
isbn = {9781941643723},
keywords = {Computational linguistics; Deep neural networks; K,Convolutional neural network; Knowledge base; Que,Natural language processing systems},
pages = {1321--1331},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Semantic parsing via staged query graph generation: Question answering with knowledge base}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943770995&doi=10.3115%2Fv1%2Fp15-1128&partnerID=40&md5=d317079b890116e6c344d73cd75dbd6d},
volume = {1},
year = {2015}
}
@inproceedings{Koltho201534,
abstract = {Semantic relation composition is a generalized approach for finding conjunctive relation paths in a knowledge base (KB). In semantic web, direct and inverse relationships between entities provide us with ample of explicit knowledge. But there is a plethora of implicit knowledge beyond these direct paths. Consider a knowledge graph, we can achieve deeper insights about a particular entity if we consider the information shared by its neighboring entities via its adjacent relation paths of arbitrary lengths. In this paper, we devise a technique to automatically discover semantically enriched conjunctive relations in a KB. Our approach is generalized for any KB and requires no additional parameter tuning. Particularly, we employ classical rule mining techniques to perform relation composition on knowledge graphs to learn first order rules. We evaluate our proposed methodology on two state of the art information extraction systems, DBpedia and Yago with promising results in terms of generating high precision rules. Furthermore, we make the rules publicly available for community usage.},
annote = {cited By 1; Conference of 3rd International Workshop on Linked Data for Information Extraction, LD4IE 2015 - co-located with the 14th International Semantic Web Conference, ISWC 2015 ; Conference Date: 12 October 2015; Conference Code:116716},
author = {Koltho, K and Dutta, A},
booktitle = {CEUR Workshop Proceedings},
editor = {{Zhang Z. Paulheim H.}, d'Amato C Gentile A L},
issn = {16130073},
keywords = {Data handling; Information analysis; Information r,Data mining,Implicit knowledge; Information extraction system},
pages = {34--47},
publisher = {CEUR-WS},
title = {{Semantic relation composition in large scale knowledge bases}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949894900&partnerID=40&md5=ff42011dd4e7b152fa358b2bd5d32001},
volume = {1467},
year = {2015}
}
@article{ISI:000549854400024,
abstract = {For many natural language processing applications, estimating similarity
and relatedness between words are key tasks that serve as the basis for
classification and generalization. Currently, vector semantic models
(VSM) have become a fundamental language modeling tool. VSMs represent
words as points in a high-dimensional space and follow the
distributional hypothesis of meaning, which assumes that semantic
similarity is related to the context. In this paper, we propose a model
whose representations are based on the semantic features associated with
a concept within the ConceptNet knowledge graph. The proposed model is
based on a vector symbolic architecture framework, which defines a set
of arithmetic operations to encode the semantic features within a single
high-dimensional vector. In addition to word distribution, these vector
representations consider several types of information. Moreover, owing
to the properties of high-dimensional spaces, they have the additional
advantage of being interpretable. We analyze the model's performance on
the SimLex-999 dataset, a dataset where commonly used distributional
models (e.g., word2vec or GloVe) perform poorly. Our results are similar
to those of other hybrid models, and they surpass several
state-of-the-art distributional and knowledge-based models.},
address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
author = {{Isaias Quiroz-Mercado}, Job and Barron-Fernandez, Ricardo and {Antonio Ramirez-Salinas}, Marco},
doi = {10.1109/ACCESS.2020.3001765},
issn = {2169-3536},
journal = {IEEE ACCESS},
keywords = {Semantics; Benchmark testing; Computational modeli},
pages = {109120--109132},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{Semantic Similarity Estimation Using Vector Symbolic Architectures}},
type = {Article},
volume = {8},
year = {2020}
}
@article{Li20121415,
abstract = {This article presents a new approach to automatically measure semantic similarity between spatial objects. It combines a description logic based knowledge base (an ontology) and a multi-layer neural network to simulate the human process of similarity perception. In the knowledge base, spatial concepts are organized hierarchically and are modelled by a set of features that best represent the spatial, temporal and descriptive attributes of the concepts, such as origin, shape and function. Water body ontology is used as a case study. The neural network was designed and human subjects' rankings on similarity of concept pairs were collected for data training, knowledge mining and result validation. The experiment shows that the proposed method achieves good performance in terms of both correlation and mean standard error analysis in measuring the similarity between neural network prediction and human subject ranking. The application of similarity measurement with respect to improving relevancy ranking of a semantic search engine is introduced at the end. {\textcopyright} 2012 Copyright Taylor and Francis Group, LLC.},
annote = {cited By 28},
author = {Li, W and Raskin, R and Goodchild, M F},
doi = {10.1080/13658816.2011.635595},
issn = {13658816},
journal = {International Journal of Geographical Information Science},
keywords = {artificial neural network; data mining; error anal},
number = {8},
pages = {1415--1435},
title = {{Semantic similarity measurement based on knowledge mining: An artificial neural net approach}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864683288&doi=10.1080%2F13658816.2011.635595&partnerID=40&md5=f5e803d6db6f3fef14cc3dc67e1678e3},
volume = {26},
year = {2012}
}
@inproceedings{Alhussien20191014,
abstract = {Commonsense knowledge is paramount to enable intelligent systems. Typically, it is characterized as being implicit and ambiguous, hindering thereby the automation of its acquisition. To address these challenges, this paper presents semantically enhanced models to enable reasoning through resolving part of commonsense ambiguity. The proposed models enhance in a knowledge graph embedding framework for knowledge base completion. Experimental results show the effectiveness of the new semantic models in commonsense reasoning. {\textcopyright} 2018 IEEE.},
annote = {cited By 0; Conference of 18th IEEE International Conference on Data Mining Workshops, ICDMW 2018 ; Conference Date: 17 November 2018 Through 20 November 2018; Conference Code:145037},
author = {Alhussien, I and Cambria, E and Nengsheng, Z},
booktitle = {IEEE International Conference on Data Mining Workshops, ICDMW},
doi = {10.1109/ICDMW.2018.00146},
editor = {{Tong H. Li Z.}, Zhu F Yu J},
isbn = {9781538692882},
issn = {23759232},
keywords = {Commonsense; Commonsense knowledge; Commonsense r,Data mining,Embeddings; Intelligent systems; Knowledge based s},
pages = {1014--1021},
publisher = {IEEE Computer Society},
title = {{Semantically enhanced models for commonsense knowledge acquisition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062839316&doi=10.1109%2FICDMW.2018.00146&partnerID=40&md5=7c608feb0692a375a35fed3e616d67da},
volume = {2018-Novem},
year = {2019}
}
@inproceedings{Guo201584,
abstract = {This paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into lowdimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper, aiming at further discovering the intrinsic geometric structure of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full advantage of additional semantic information and enforce the embedding space to be semantically smooth, i.e., entities belonging to the same semantic category will lie close to each other in the embedding space. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smoothness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. We empirically evaluate SSE in two benchmark tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-The-Art methods. Furthermore, SSE is a general framework. The smoothness assumption can be imposed to a wide variety of embedding models, and it can also be constructed using other information besides entities' semantic categories. {\textcopyright} 2015 Association for Computational Linguistics.},
annote = {cited By 62; Conference of 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015 ; Conference Date: 26 July 2015 Through 31 July 2015; Conference Code:114195},
author = {Guo, S and Wang, Q and Wang, B and Wang, L and Guo, L},
booktitle = {ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/p15-1009},
isbn = {9781941643723},
keywords = {Classification (of information); Computational lin,Geometric structure; Laplacian eigenmaps; Locally,Natural language processing systems},
pages = {84--94},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Semantically smooth knowledge graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943760568&doi=10.3115%2Fv1%2Fp15-1009&partnerID=40&md5=8364da8101b18b0d32ecdd7be5311f0f},
volume = {1},
year = {2015}
}
@inproceedings{8989264,
abstract = {In this work, our goal is to build a self-sustainable domain-specific Ontology for the purposes of creating a Knowledge Search Engine. We focused to build it in the Marathi language, which will help school-going children to explore science-related terms. For this, a method is proposed, in which ontology is learned automatically using deep learning model, Bidirectional Long Short-Term Memory (BiLSTM). This paper proposes to use learned ontology to retrieve domain-specific knowledge. The knowledge search engine, which uses constructed ontology to displays search results in Marathi with a very strict limit to the Knowledge complexity of the search results. Unlike, standard search engines, our engine attempts to provide learning resources directly to the user rather than website links. This approach enables the user to directly get information without having to spend time browsing indexed links.},
author = {Chandolikar, N and Shilaskar, S and Peddawad, D and Bhosale, S},
booktitle = {2019 International Conference on Applied Machine Learning (ICAML)},
doi = {10.1109/ICAML48257.2019.00029},
keywords = {Knowledge Web,Ontology,Relation Identification,Semantic role labeling.,information retrieval;natural language processing;},
month = {may},
pages = {108--113},
title = {{Semi-Automated Ontology Building Using Deep Learning to Provide Domain-Specific Knowledge Search in the Marathi Language}},
year = {2019}
}
@inproceedings{PWF5H6Q5,
abstract = {In many modern-day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models.},
author = {Jayawardana, V and Lakmal, D and de Silva, N and Perera, A S and Sugathadasa, K and Ayesha, B and Perera, M},
booktitle = {17th Int. Conf. on Advances in ICT for Emerging Regions (ICTer)},
issn = {2472-7598},
keywords = {learning (artificial intelligence);natural languag},
pages = {1--7},
title = {{Semi-supervised instance population of an ontology using word vector embedding}},
year = {2017}
}
@article{Razzaq2020,
abstract = {The recognition of activities of daily living (ADL) in smart environments is a well-known and an important research area, which presents the real-time state of humans in pervasive computing. The process of recognizing human activities generally involves deploying a set of obtrusive and unobtrusive sensors, pre-processing the raw data, and building classification models using machine learning (ML) algorithms. Integrating data from multiple sensors is a challenging task due to dynamic nature of data sources. This is further complicated due to semantic and syntactic differences in these data sources. These differences become even more complex if the data generated is imperfect, which ultimately has a direct impact on its usefulness in yielding an accurate classifier. In this study, we propose a semantic imputation framework to improve the quality of sensor data using ontology-based semantic similarity learning. This is achieved by identifying semantic correlations among sensor events through SPARQL queries, and by performing a time-series longitudinal imputation. Furthermore, we applied deep learning (DL) based artificial neural network (ANN) on public datasets to demonstrate the applicability and validity of the proposed approach. The results showed a higher accuracy with semantically imputed datasets using ANN. We also presented a detailed comparative analysis, comparing the results with the state-of-the-art from the literature. We found that our semantic imputed datasets improved the classification accuracy with 95.78% as a higher one thus proving the effectiveness and robustness of learned models. {\textcopyright} 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
annote = {cited By 0},
author = {Razzaq, M A and Cleland, I and Nugent, C and Lee, S},
doi = {10.3390/s20102771},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Activities of Daily Living; Classification accura,Classification (of information); Complex networks;,Deep learning},
number = {10},
publisher = {MDPI AG},
title = {{Semimput: Bridging semantic imputation with deep learning for complex human activity recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084785917&doi=10.3390%2Fs20102771&partnerID=40&md5=e6674a074fe1fb8e2889e19f2faf419f},
volume = {20},
year = {2020}
}
@article{Tymoshenko2019,
abstract = {In this article, we extensively study the use of syntactic and semantic structures obtained with shallow and full syntactic parsers for answer passage reranking. We propose several dependency and constituent-based structures, also enriched with Linked Open Data (LD) knowledge to represent pairs of questions and answer passages. We encode such tree structures in learning-to-rank (L2R) algorithms using tree kernels, which can project them in tree substructure spaces, where each dimension represents a powerful syntactic/semantic feature. Additionally, since we define links between question and passage structures, our tree kernel spaces also include relational structural features. We carried out an extensive comparative experimentation of our models for automatic answer selection benchmarks on different TREC QA corpora as well as the newer Wikipedia-based dataset, namely WikiQA, which has been widely used to test sentence rerankers. The results consistently demonstrate that our structural semantic models achieve the state of the art in passage reranking. In particular, we derived the following important findings: (i) relational syntactic structures are essential to achieve superior results; (ii) models trained with dependency trees can outperform those trained with shallow trees, e.g., in case of sentence reranking; (iii) external knowledge automatically generated with focus and question classifiers is very effective; and (iv) the semantic information derived by LD and incorporated in syntactic structures can be used to replace the knowledge provided by the above-mentioned classifiers. This is a remarkable advantage as it enables our models to increase coverage and portability over new domains. {\textcopyright} 2018 ACM},
annote = {cited By 2},
author = {Tymoshenko, K and Moschitti, A},
doi = {10.1145/3233772},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
keywords = {Automatically generated; Kernel methods; Learning,Classification (of information); Forestry; Linked,Syntactics},
number = {1},
publisher = {Association for Computing Machinery},
title = {{Shallow and deep syntactic/semantic structures for passage reranking in question-answering systems}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061246074&doi=10.1145%2F3233772&partnerID=40&md5=9bc8face218e0ac9dd03ca691e072e55},
volume = {37},
year = {2019}
}
@inproceedings{10.1145/3269206.3271704,
abstract = {Knowledge Graphs (KGs) have facilitated many real-world applications (e.g., vertical search and intelligent question answering). However, they are usually incomplete, which affects the performance of such KG based applications. To alleviate this problem, a number of Knowledge Graph Completion (KGC) methods have been developed to predict those implicit triples. Tensor/matrix based methods and translation based methods have attracted great attention for a long time. Recently, neural network has been introduced into KGC due to its extensive superiority in many fields (e.g., natural language processing and computer vision), and achieves promising results. In this paper, we propose a Shared Embedding based Neural Network (SENN) model for KGC. It integrates the prediction tasks of head entities, relations and tail entities into a neural network based framework with shared embeddings of entities and relations, while explicitly considering the differences among these prediction tasks. Moreover, we propose an adaptively weighted loss mechanism, which dynamically adjusts the weights of losses according to the mapping properties of relations, and the prediction tasks. Since relation prediction usually performs better than head and tail entity predictions, we further extend SENN to SENN+ by employing it to assist head and tail entity predictions. Experiments on benchmark datasets validate the effectiveness and merits of the proposed SENN and SENN+ methods. The shared embeddings and the adaptively weighted loss mechanism are also testified to be effective.},
address = {New York, NY, USA},
author = {Guan, Saiping and Jin, Xiaolong and Wang, Yuanzhuo and Cheng, Xueqi},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
doi = {10.1145/3269206.3271704},
isbn = {9781450360142},
keywords = {knowledge graph completion,neural network,shared embedding},
pages = {247--256},
publisher = {Association for Computing Machinery},
series = {CIKM '18},
title = {{Shared Embedding Based Neural Networks for Knowledge Graph Completion}},
url = {https://doi.org/10.1145/3269206.3271704},
year = {2018}
}
@article{Wang201967,
abstract = {Knowledge graphs (KG) contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Knowledge Graph Completion (KGC) task aims to findin missing or errant relationships with the goal of improving the general quality of KGs. Recent years have witnessed great advance of represent learning (RL) based KGC models, which represent entities and relations as elements of a continuous vector space. However, with the deepening of the research, the scale of parameters and the complexity of KGC models become larger and larger, resulting in a serious imbalance between accuracy and computational complexity. Finally, not only the efficiency is not satisfactory, but also the training cost becomes high, which seriously restricts the flexibility and scalability of the KGC model. Therefore, this paper investigates how to enhance the simplicity of KGC model and achieve a reasonable balance between accuracy and complexity. Extensive experiments show that the proposed framework improves the performance of the current represent learning models for KGC task. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 1; Conference of 25th China Conference on Information Retrieval, CCIR 2019 ; Conference Date: 20 September 2019 Through 22 September 2019; Conference Code:232279},
author = {Wang, Y and Zhang, H and Li, Y and Xie, H},
doi = {10.1007/978-3-030-31624-2_6},
editor = {{Zhang Q. Liao X.}, Ren Z},
isbn = {9783030316235},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Information retrieval; Vector spaces,Knowledge graphs; Learning models; Parameter shar,Learning systems},
pages = {67--78},
publisher = {Springer},
title = {{Simplified representation learning model based on parameter-sharing for knowledge graph completion}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075604747&doi=10.1007%2F978-3-030-31624-2_6&partnerID=40&md5=c4933a830ff95474c6bae76c6c31ef70},
volume = {11772 LNCS},
year = {2019}
}
@article{Nechaev2018251,
abstract = {SocialLink is a project designed to match social media profiles on Twitter to corresponding entities in DBpedia. Built to bridge the vibrant Twitter social media world and the Linked Open Data cloud, SocialLink enables knowledge transfer between the two, both assisting Semantic Web practitioners in better harvesting the vast amounts of information available on Twitter and allowing leveraging of DBpedia data for social media analysis tasks. In this paper, we further extend the original SocialLink approach by exploiting graph-based features based on both DBpedia and Twitter, represented as graph embeddings learned from vast amounts of unlabeled data. The introduction of such new features required to redesign our deep neural network-based candidate selection algorithm and, as a result, we experimentally demonstrate a significant improvement of the performances of SocialLink. {\textcopyright} 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
annote = {cited By 5},
author = {Nechaev, Y and Corcoglioniti, F and Giuliano, C},
doi = {10.1007/s13748-018-0160-x},
issn = {21926352},
journal = {Progress in Artificial Intelligence},
keywords = {Candidate selection; Dbpedia; Graph embeddings; G,Deep neural networks; Graphic methods; Knowledge m,Social networking (online)},
number = {4},
pages = {251--272},
publisher = {Springer Verlag},
title = {{SocialLink: exploiting graph embeddings to link DBpedia entities to Twitter profiles}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054635727&doi=10.1007%2Fs13748-018-0160-x&partnerID=40&md5=3ab44429451fe059421740d7f8b7a0c0},
volume = {7},
year = {2018}
}
@inproceedings{Soru2018,
abstract = {Recently, the Linked Data Cloud has achieved a size of more than 100 billion facts pertaining to a multitude of domains. However, accessing this information has been significantly challenging for lay users. Approaches to problems such as Question Answering on Linked Data and Link Discovery have notably played a role in increasing information access. These approaches are often based on handcrafted and/or statistical models derived from data observation. Recently, Deep Learning architectures based on Neural Networks called seq2seq have shown to achieve the state-of-the-art results at translating sequences into sequences. In this direction, we propose Neural SPARQL Machines, end-to-end deep architectures to translate any natural language expression into sentences encoding SPARQL queries. Our preliminary results, restricted on selected DBpedia classes, show that Neural SPARQL Machines are a promising approach for Question Answering on Linked Data, as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition. � 2018 CEUR-WS. All rights reserved.},
annote = {cited By 0; Conference of Posters and Demos Track of the 13th International Conference on Semantic Systems SEMANTiCS, SEMPDS 2017 ; Conference Date: 11 September 2017 Through 14 September 2017; Conference Code:135792},
author = {Soru, T and Marx, E and Moussallem, D and Publio, G and Valdestilhas, A and Esteves, D and Neto, C B},
booktitle = {CEUR Workshop Proceedings},
editor = {{Hellmann S.}, Fernandez J D},
issn = {16130073},
keywords = {Data handling; Linked data; Natural language proce,Deep architectures; Foreign language; Information,Deep learning},
publisher = {CEUR-WS},
title = {{SPARQL as a foreign language}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045919177&partnerID=40&md5=0d373223e96121a27ecb8550eb1001b4},
volume = {2044},
year = {2018}
}
@inproceedings{10.1145/3336191.3371812,
abstract = {Knowledge Graph Question Answering aims to automatically answer natural language questions via well-structured relation information between entities stored in knowledge graphs. When faced with a multi-relation question, existing embedding-based approaches take the whole topic-entity-centric subgraph into account, resulting in high time complexity. Meanwhile, due to the high cost for data annotations, it is impractical to exactly show how to answer a complex question step by step, and only the final answer is labeled, as weak supervision. To address these challenges, this paper proposes a neural method based on reinforcement learning, namely Stepwise Reasoning Network, which formulates multi-relation question answering as a sequential decision problem. The proposed model performs effective path search over the knowledge graph to obtain the answer, and leverages beam search to reduce the number of candidates significantly. Meanwhile, based on the attention mechanism and neural networks, the policy network can enhance the unique impact of different parts of a given question over triple selection. Moreover, to alleviate the delayed and sparse reward problem caused by weak supervision, we propose a potential-based reward shaping strategy, which can accelerate the convergence of the training algorithm and help the model perform better. Extensive experiments conducted over three benchmark datasets well demonstrate the effectiveness of the proposed model, which outperforms the state-of-the-art approaches.},
address = {New York, NY, USA},
author = {Qiu, Yunqi and Wang, Yuanzhuo and Jin, Xiaolong and Zhang, Kun},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
doi = {10.1145/3336191.3371812},
isbn = {9781450368223},
keywords = {knowledge graph,question answering,reinforcement learning,weak supervision},
pages = {474--482},
publisher = {Association for Computing Machinery},
series = {WSDM '20},
title = {{Stepwise Reasoning for Multi-Relation Question Answering over Knowledge Graph with Weak Supervision}},
url = {https://doi.org/10.1145/3336191.3371812},
year = {2020}
}
@inproceedings{Fu201825,
abstract = {Interactive prediction of financial instrument returns is important. It is needed for asset managers to generate trading strategies as well as for stock exchange regulators to discover pricing anomalies. In this paper, we introduce an integrated stochastic optimization technique, namely genetic programming (GP) with generalized crowding (GC), GP+GC. GP+GC is as an integrated method for market return prediction, using a financial knowledge graph (KG). On the one hand, using time-series data for twenty-nine component stocks of the Dow Jones industrial average, we show that our stochastic optimization method can give strong prediction performance by providing a comparison of its return performances with two traditional benchmarks, namely a Buy \& Hold strategy and the Moving Average Convergence Divergence (MACD) technical indicator. On the other hand, we use features extracted from a time-evolving knowledge graph constructed from fifty component stocks of the Shanghai Stock Exchange SSE50 index. These features are used by our GP+GC variant and then expression learnt by GP+GC are extracted into a KG. Overall, this work demonstrates how to integrate GP+GC with KGs in a powerful manner. {\textcopyright}2018 IEEE},
annote = {cited By 4; Conference of 9th IEEE International Conference on Big Knowledge, ICBK 2018 ; Conference Date: 17 November 2018 Through 18 November 2018; Conference Code:144043},
author = {Fu, X and Ren, X and Mengshoel, O J and Wu, X},
booktitle = {Proceedings - 9th IEEE International Conference on Big Knowledge, ICBK 2018},
doi = {10.1109/ICBK.2018.00012},
editor = {{Soon O.Y. Chen H.}, Wu X Aggarwal C},
isbn = {9781538691243},
keywords = {Benchmarking; Commerce; Finance; Forecasting; Gene,Dow Jones Industrial averages; Generalized crowdi,Financial markets},
pages = {25--32},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Stochastic optimization for market return prediction using financial knowledge graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061352372&doi=10.1109%2FICBK.2018.00012&partnerID=40&md5=060babef88514b02890fd83c89961225},
year = {2018}
}
@inproceedings{10.1145/3132847.3133152,
abstract = {With the aid of recently proposed word embedding algorithms, the study of semantic relatedness has progressed and advanced rapidly. In this research, we propose a novel structural-fitting method that utilizes the linguistic ontology into vector space representations. The ontological information is applied in two ways. The fine2coarse approach refines the word vectors from fine-grained to coarse-grained terms (word types), while the coarse2fine approach refines the word vectors from coarse-grained to fine-grained terms. In the experiments, we show that our proposed methods outperform previous approaches in seven publicly available benchmark datasets.},
address = {New York, NY, USA},
author = {Lee, Yang-Yin and Yen, Ting-Yu and Huang, Hen-Hsen and Chen, Hsin-Hsi},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
doi = {10.1145/3132847.3133152},
isbn = {9781450349185},
keywords = {Word embedding,linguistic ontology,retrofitting,semantic relatedness,structural-fitting},
pages = {2151--2154},
publisher = {Association for Computing Machinery},
series = {CIKM '17},
title = {{Structural-Fitting Word Vectors to Linguistic Ontology for Semantic Relatedness Measurement}},
url = {https://doi.org/10.1145/3132847.3133152},
year = {2017}
}
@article{Kejriwal2018233,
abstract = {In domains such as humanitarian assistance and disaster relief (HADR), events, rather than named entities, are the primary focus of analysts and aid officials. An important problem that must be solved to provide situational awareness to aid providers is automatic clustering of sub-events that refer to the same underlying event. An effective solution to the problem requires judicious use of both domain-specific and semantic information, as well as statistical methods like deep neural embeddings. In this paper, we present an approach, AugSEER (Augmented feature sets for Structured Event Entity Resolution), that combines advances in deep neural embeddings both on text and graph data with minimally supervised inputs from domain experts. AugSEER can operate in both online and batch scenarios. On five real-world HADR datasets, AugSEER is found, on average, to outperform the next best baseline result by almost 15% on the cluster purity metric and by 3% on the F1-Measure metric. In contrast, text-based approaches are found to perform poorly, demonstrating the importance of semantic information in devising a good solution. We also use sub-event clustering visualizations to illustrate the qualitative potential of AugSEER. {\textcopyright} Springer Nature Switzerland AG 2018.},
annote = {cited By 0; Conference of 17th International Semantic Web Conference, ISWC 2018 ; Conference Date: 8 October 2018 Through 12 October 2018; Conference Code:219319},
author = {Kejriwal, M and Peng, J and Zhang, H and Szekely, P},
doi = {10.1007/978-3-030-00671-6_14},
editor = {{Suarez-Figueroa M.C. Presutti V.}, Kaffee L Simperl E Sabou M Vrandecic D Celino I Bontcheva K},
isbn = {9783030006709},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Clustering; Crisis informatics; Disaster relief;,Disaster prevention; Emergency services; Natural l,Semantic Web},
pages = {233--249},
publisher = {Springer Verlag},
title = {{Structured event entity resolution in humanitarian domains}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054811011&doi=10.1007%2F978-3-030-00671-6_14&partnerID=40&md5=91891867657008d6af5c279ad5ed4845},
volume = {11136 LNCS},
year = {2018}
}
@inproceedings{Abedjan20131,
abstract = {Linked Open Data brings new challenges and opportunities for the data mining community. Its underlying data model RDF is heterogeneous and contains machine readable semantic relations. The amount of available open data requires profiling and integration for desired applications. One of the promising underlying techniques is association rule mining. However there has been only limited application of association rules on semantic web data. We introduce the concept of mining configurations that allows us to mine RDF data on statement level. We described elaborated use cases such as ontology engineering, data imputation that are based on configurations in the context of RDF subjects. A novel application that is based on mining configurations is synonym discovery. Synonym discovery is useful for discovering globally valid synonyms for a thesauros as well supporting the completeness of SPARQL query results by including results that are connected to synonym predicates. We show that synonym discovery in RDF data can be done efficiently using the proposed techniques based on association rule mining.},
annote = {cited By 0; Conference of 6th Ph.D. Retreat of the HPI Research School on Service-Oriented Systems Engineering ; Conference Code:109503},
author = {Abedjan, Z},
booktitle = {Technische Berichte des Hasso-Plattner-Instituts fur Softwaresystemtechnik an der Universitat Potsdam},
editor = {{Naumann F. Meinel C.}, Polze A Giese H Dollner J Baudisch P Weske M Hirschfeld R Plattner H},
isbn = {9783869562568},
issn = {21911665},
keywords = {Association rules; Data mining; Semantics; Systems,Data imputation; Data mining community; Linked op,Semantic Web},
pages = {1--10},
publisher = {Universitatsverlag Potsdam},
title = {{Synonym discovery in RDF data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939536201&partnerID=40&md5=e6ae8666f5df2a21910529f31f5cf387},
volume = {76},
year = {2013}
}
@article{Goo201688,
abstract = {We propose a novel convolutional network architecture that abstracts and differentiates the categories based on a given class hierarchy. We exploit grouped and discriminative information provided by the taxonomy, by focusing on the general and specific components that comprise each category, through the min- and difference-pooling operations. Without using any additional parameters or substantial increase in time complexity, our model is able to learn the features that are discriminative for classifying often confused sub-classes belonging to the same superclass, and thus improve the overall classification performance. We validate our method on CIFAR-100, Places-205, and ImageNet Animal datasets, on which our model obtains significant improvements over the base convolutional networks. {\textcopyright} Springer International Publishing AG 2016.},
annote = {cited By 7; Conference of 14th European Conference on Computer Vision, ECCV 2016 ; Conference Date: 8 October 2016 Through 16 October 2016; Conference Code:183959},
author = {Goo, W and Kim, J and Kim, G and Hwang, S J},
doi = {10.1007/978-3-319-46475-6_6},
editor = {{Leibe B. Sebe N.}, Welling M Matas J},
isbn = {9783319464749},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Class hierarchies; Classification performance; Co,Complex networks,Computer vision; Convolution; Network architecture},
pages = {88--101},
publisher = {Springer Verlag},
title = {{Taxonomy-regularized semantic deep convolutional neural networks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990845581&doi=10.1007%2F978-3-319-46475-6_6&partnerID=40&md5=623c141946c5dee4540388efb2c68932},
volume = {9906 LNCS},
year = {2016}
}
@inproceedings{9194506,
abstract = {As an effective and novel knowledge management technology, knowledge graph can provide a new way for the inheritance and development of traditional Chinese medicine (TCM). However, the construction of the knowledge graph of TCM is still mainly based on structured data at present. With the accumulation of literatures and electronic medical records, a large amount of knowledge is stored in unstructured texts which urgently needs to be extracted for learning. In this study, we extract TCM core concepts and build ontology layer by analyzing the process of TCM diagnosis and treatment. Then we use deep learning to extract entities and their relations for building TCM knowledge graph from unstructured data. Finally, we build an end-to-end platform TCMKG based on knowledge graph, which can provide functions such as knowledge retrieval, visualization and data management for helping the learning and sharing of TCM knowledge.},
author = {Zheng, Z and Liu, Y and Zhang, Y and Wen, C},
booktitle = {2020 IEEE International Conference on Knowledge Graph (ICKG)},
doi = {10.1109/ICBK50248.2020.00084},
keywords = {data visualisation;electronic health records;graph},
month = {aug},
pages = {560--564},
title = {{TCMKG: A Deep Learning Based Traditional Chinese Medicine Knowledge Graph Platform}},
year = {2020}
}
@inproceedings{Wu201879,
abstract = {People are flooded with massive semi-structured and unstructured texts in their daily work life. The fast-paced lifestyle has forced us to get more focused information from these large amounts of text more quickly. So people urgently need a technology that can automatically extract abstracts from text. The traditional extractive automatic abstract method can only extract keywords or key sentences. Although the current popular sequence-To-sequence extraction methods have greatly improved compared with the traditional methods, they cannot be combined with the background information to obtain higher level abstraction. Therefore, we propose a method based on knowledge graph technology to automatically extract abstract texts. This method can not only obtain higher-level extraction from the text, but also can select template and question and answer to obtain a personalized abstract. We experimented on the CNN DAILYMAIL dataset. The results show that the abstract obtained by this method can reflect more textual information, and more in line with human reading habits, and can achieve personalized extraction, and can obtain close to the best ROUGE index results. {\textcopyright} 2018 IEEE.},
annote = {cited By 3; Conference of 6th International Conference on Audio, Language and Image Processing, ICALIP 2018 ; Conference Date: 16 July 2018 Through 17 July 2018; Conference Code:139354},
author = {Wu, P and Zhou, Q and Lei, Z and Qiu, W and Li, X},
booktitle = {ICALIP 2018 - 6th International Conference on Audio, Language and Image Processing},
doi = {10.1109/ICALIP.2018.8455241},
isbn = {9781538651957},
keywords = {Abstracting; Extraction; Information retrieval; Ne,Background information; Entity extractions; Extra,Image processing},
pages = {79--83},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Template Oriented Text Summarization via Knowledge Graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063526219&doi=10.1109%2FICALIP.2018.8455241&partnerID=40&md5=eca8a978e382bf44d115e9704d2092fc},
year = {2018}
}
@article{Li2012765,
abstract = {In this paper, a corpus-based thesaurus and WordNet were used to improve text categorization performance. We employed the k-NN algorithm and the back propagation neural network (BPNN) algorithms as the classifiers. The k-NN is a simple and famous approach for categorization, and the BPNNs has been widely used in the categorization and pattern recognition fields. However the standard BPNN has some generally acknowledged limitations, such as a slow training speed and can be easily trapped into a local minimum. To alleviate the problems of the standard BPNN, two modified versions, Morbidity neurons Rectified BPNN (MRBP) and Learning Phase Evaluation BPNN (LPEBP), were considered and applied to the text categorization. We conducted the experiments on both the standard reuter-21578 data set and the 20 Newsgroups data set. Experimental results showed that our proposed methods achieved high categorization effectiveness as measured by the precision, recall and F-measure protocols. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
annote = {cited By 34},
author = {Li, C H and Yang, J C and Park, S C},
doi = {10.1016/j.eswa.2011.07.070},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Algorithms; Ontology; Pattern recognition; Semant,Back-propagation neural networks; BPNN; Data sets;,Neural networks},
number = {1},
pages = {765--772},
title = {{Text categorization algorithms using semantic approaches, corpus-based thesaurus and WordNet}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855175872&doi=10.1016%2Fj.eswa.2011.07.070&partnerID=40&md5=bee62ef597e96fa1bcd4398eb914013c},
volume = {39},
year = {2012}
}
@inproceedings{8995179,
abstract = {Knowledge representation learning (KRL), which transforms both the entities and relations into continuous low dimensional continuous vector space, has attracted considerable research. Most of existing knowledge graph (KG) completion models only considers the structural representation of triples, but do not consider the important text information about entity descriptions in the knowledge base. We propose a text-enhanced KG model based on gated convolution network (GConvTE), which can learn entity descriptions and symbol triples jointly by feature fusion. Specifically, each triple (head entity, relation, tail entity) is represented as a 3-column structural embedding matrix, a 3-column textual embedding matrix and a 3-column joint embedding matrix where each column vector represents a triple element. Textual embeddings are obtained by bidirectional gated recurrent unit with attention (A-BGRU) encoding entity descriptions and joint embeddings are obtained by the combination of textual embeddings and structural embeddings. Extending feature dimension in embedding layer, these three matrixs are concatenated into 3-channel feature block to be fed into convolution layer, where the gated unit is added to selectively output the joint features maps. These feature maps are concatenated and then multiplied with a weight vector via a dot product to return a score. The experimental results show that our model GConvTE achieves better link performance than previous state-of-art embedding models on two benchmark datasets.},
author = {Liu, C and Zhang, Y and Yu, M and Li, X and Zhao, M and Xu, T and Yu, J and Yu, R},
booktitle = {2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)},
doi = {10.1109/ICTAI.2019.00051},
issn = {2375-0197},
keywords = {convolutional neural nets;feature extraction;graph},
month = {nov},
pages = {308--315},
title = {{Text-Enhanced Knowledge Representation Learning Based on Gated Convolutional Networks}},
year = {2019}
}
@inproceedings{Kim201568,
abstract = {Wikipedia categories are a useful source of knowledge that is usually expressed in a noun-phrase that contains information about concepts of entities or relations among entities. In DBpedia KBs, they categorize their entities into Wikipedia categories using RDF triples. The RDF triples represent only categories of entities, but not concepts of entities or relations among entities despite the fact that expression of Wikipedia categories contain a wealth of those types of information. In this regard, We propose a method that extracts RDF triples encoding concepts of entities or relations among entities from RDF triples encoding Wikipedia categories of each DBpedia entities using association rule mining techniques that mainly utilize lexical patterns in category expression and a hierarchy of categories. Our extensive experiments show that our approach can mine association rules with more high quality than those of state-of-the-art approaches in this problem.},
annote = {cited By 2; Conference of 3rd NLP and DBpedia Workshop, NLP and DBpedia 2015 ; Conference Date: 11 October 2015; Conference Code:122142},
author = {Kim, J and Kim, E.-K. and Won, Y and Nam, S and Choi, K.-S.},
booktitle = {CEUR Workshop Proceedings},
editor = {{Paulheim H. Brummer M.}, van Erp M Filipowska A Mendes P N},
issn = {16130073},
keywords = {Association rules; Encoding (symbols); Knowledge b,Dbpedia; High quality; Knowledge base; Lexical pa,Semantic Web},
pages = {68--80},
publisher = {CEUR-WS},
title = {{The association rule mining system for acquiring knowledge of DBpedia from wikipedia categories}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977570552&partnerID=40&md5=1d441a6818d7f00d2c8a4d58ab0661a5},
volume = {1581},
year = {2015}
}
@inproceedings{8904496,
abstract = {As the public medical resources becomes more and more scarce due to the aging of a large population in China, online medical consultation become a popular alternative. However, existing online medical consultation providers or forums are either labor-intensive, thus expensive, or free but unreliable with respect to the quality of answers. With the aim of providing a convenient, instant and reliable tool for people in need of simple medication related consultations, we developed a question and answering system (Dr-KGQA) to automatically answer the questions from users in an interactive manner, with the help of our powerful Chinese medical knowledge graph (Med-KG). Our system employs a pipeline of deep learning models including named entity recognition (NER) and relation matching. We conduct a series of experiments to demonstrate the performance of our system, on a human annotated dataset we collect. We have piloted our system on a family doctor platform in Chongqing, China, and the results show that our system is promising for real-world applications.},
author = {Zhu, W and Ni, Y and Xie, G and Zhou, X and Chen, C},
booktitle = {2019 IEEE International Conference on Healthcare Informatics (ICHI)},
doi = {10.1109/ICHI.2019.8904496},
issn = {2575-2634},
keywords = {data mining;learning (artificial intelligence);med},
month = {jun},
pages = {1--6},
title = {{The Dr-KGQA System for Automatically Answering Medication Related Questions in Chinese}},
year = {2019}
}
@article{Kastrati20191618,
abstract = {This paper presents a semantically rich document representation model for automatically classifying financial documents into predefined categories utilizing deep learning. The model architecture consists of two main modules including document representation and document classification. In the first module, a document is enriched with semantics using background knowledge provided by an ontology and through the acquisition of its relevant terminology. Acquisition of terminology integrated to the ontology extends the capabilities of semantically rich document representations with an in depth-coverage of concepts, thereby capturing the whole conceptualization involved in documents. Semantically rich representations obtained from the first module will serve as input to the document classification module which aims at finding the most appropriate category for that document through deep learning. Three different deep learning networks each belonging to a different category of machine learning techniques for ontological document classification using a real-life ontology are used. Multiple simulations are carried out with various deep neural networks configurations, and our findings reveal that a three hidden layer feedforward network with 1024 neurons obtain the highest document classification performance on the INFUSE dataset. The performance in terms of F1 score is further increased by almost five percentage points to 78.10% for the same network configuration when the relevant terminology integrated to the ontology is applied to enrich document representation. Furthermore, we conducted a comparative performance evaluation using various state-of-the-art document representation approaches and classification techniques including shallow and conventional machine learning classifiers. {\textcopyright} 2019 Elsevier Ltd},
annote = {cited By 18},
author = {Kastrati, Z and Imran, A S and Yayilgan, S Y},
doi = {10.1016/j.ipm.2019.05.003},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Back-ground knowledge; Classification technique;,Classification (of information); Deep learning; De,Information retrieval systems},
number = {5},
pages = {1618--1632},
publisher = {Elsevier Ltd},
title = {{The impact of deep learning on document classification using semantically rich representations}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065664667&doi=10.1016%2Fj.ipm.2019.05.003&partnerID=40&md5=7ffe5524ad87d96fb32c8a8119423952},
volume = {56},
year = {2019}
}
@inproceedings{Marino201720,
abstract = {One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification. {\textcopyright} 2017 IEEE.},
annote = {cited By 62; Conference of 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017 ; Conference Date: 21 July 2017 Through 26 July 2017; Conference Code:132417},
author = {Marino, K and Salakhutdinov, R and Gupta, A},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.10},
isbn = {9781538604571},
keywords = {Classification (of information); Computer vision;,Computer vision algorithms; Graph search; Knowled,Image classification},
pages = {20--28},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The more you know: using knowledge graphs for image classification}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041905615&doi=10.1109%2FCVPR.2017.10&partnerID=40&md5=1dcf315038c8106f478df472f18870bf},
volume = {2017-Janua},
year = {2017}
}
@article{Zhu2019122,
abstract = {The traditional collaborative filtering recommendation algorithm only uses the item-user rating matrix without considering the semantic information of the item itself, resulting in a problem that the recommendation accuracy is not high. This paper proposes a Top-N collaborative filtering recommendation algorithm based on knowledge graph embedding. The knowledge graph embedding is used to learn a low-dimensional vector for each entity and relationship in the knowledge graph, while maintaining the structure and semantic information of the original graph in the vector. By calculating the semantic similarity between items, the semantic information of the item itself is incorporated into the collaborative filtering recommendation. The algorithm makes up for the defect that the collaborative filtering recommendation algorithm does not consider the knowledge information of the item itself, and enhances the effect of collaborative filtering recommendation on the semantic level. The experimental results on the MovieLens dataset show that the algorithm can get higher values on precision, recall and F1 measure. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 3; Conference of 14th International Conference on Knowledge Management in Organizations, KMO 2019 ; Conference Date: 15 July 2019 Through 18 July 2019; Conference Code:227229},
author = {Zhu, M and Zhen, D.-S. and Tao, R and Shi, Y.-Q. and Feng, X.-Y. and Wang, Q},
doi = {10.1007/978-3-030-21451-7_11},
editor = {{Uden L. Ting I.-H.}, Corchado J M},
isbn = {9783030214500},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Collaborative filtering,Collaborative filtering recommendations; Knowledg,Embeddings; Knowledge management; Semantics},
pages = {122--134},
publisher = {Springer Verlag},
title = {{Top-N collaborative filtering recommendation algorithm based on knowledge graph embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067640231&doi=10.1007%2F978-3-030-21451-7_11&partnerID=40&md5=a79ac0447f95203de906c816d672205f},
volume = {1027},
year = {2019}
}
@article{Xu2020178,
abstract = {This paper aims to analyze and adopt the term clustering method for building a modular ontology according to its core ontology. The acquisition of semantic knowledge focuses on noun phrase appearing with the same syntactic roles in relation to a verb or its preposition combination in a sentence. The construction of this co-occurrence matrix from context helps to build feature space of noun phrases, which is then transformed to several encoding representations including feature selection and dimensionality reduction. In addition, word embedding techniques are also presented as feature representation. These representations are clustered respectively with K-Means, K-Medoids, Affinity Propagation, DBscan and co-clustering algorithms. The feature representation and clustering methods constitute the major sections of term clustering frameworks. Due to the randomness of clustering approaches, iteration efforts are adopted to find the optimal parameter and provide convinced value for evaluation. The DBscan and affinity propagation show their outstanding effectiveness for term clustering and NMF encoding technique and word embedding representation are salient by its promising facilities in feature compression. {\textcopyright} 2020, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2018 ; Conference Date: 18 September 2018 Through 20 September 2018; Conference Code:241649},
author = {Xu, Z and Harzallah, M and Guillet, F and Ichise, R},
doi = {10.1007/978-3-030-49559-6_9},
editor = {{Fred A. Salgado A.}, Aveiro D Dietz J Bernardino J Filipe J},
isbn = {9783030495589},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Affinity propagation; Clustering approach; Cluste,Cluster analysis; Dimensionality reduction; Embedd,K-means clustering},
pages = {178--201},
publisher = {Springer},
title = {{Towards a Term Clustering Framework for Modular Ontology Learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088253474&doi=10.1007%2F978-3-030-49559-6_9&partnerID=40&md5=d292058735ebfcfa51fde8ca706425b4},
volume = {1222 CCIS},
year = {2020}
}
@article{Catling201850,
abstract = {Background: Patients' encounters with healthcare services must undergo clinical coding. These codes are typically derived from free-text notes. Manual clinical coding is expensive, time-consuming and prone to error. Automated clinical coding systems have great potential to save resources, and realtime availability of codes would improve oversight of patient care and accelerate research. Automated coding is made challenging by the idiosyncrasies of clinical text, the large number of disease codes and their unbalanced distribution. Methods: We explore methods for representing clinical text and the labels in hierarchical clinical coding ontologies. Text is represented as term frequency-inverse document frequency counts and then as word embeddings, which we use as input to recurrent neural networks. Labels are represented atomically, and then by learning representations of each node in a coding ontology and composing a representation for each label from its respective node path. We consider different strategies for initialisation of the node representations. We evaluate our methods using the publicly-available Medical Information Mart for Intensive Care III dataset: we extract the history of presenting illness section from each discharge summary in the dataset, then predicting the International Classification of Diseases, ninth revision, Clinical Modification codes associated with these. Results: Composing the label representations from the clinical-coding-ontology nodes increased weighted F1 for prediction of the 17,561 disease labels to 0.264–0.281 from 0.232–0.249 for atomic representations. Recurrent neural network text representation improved weighted F1 for prediction of the 19 disease-category labels to 0.682–0.701 from 0.662–0.682 using term frequency-inverse document frequency. However, term frequency-inverse document frequency outperformed recurrent neural networks for prediction of the 17,561 disease labels. Conclusions: This study demonstrates that hierarchically-structured medical knowledge can be incorporated into statistical models, and produces improved performance during automated clinical coding. This performance improvement results primarily from improved representation of rarer diseases. We also show that recurrent neural networks improve representation of medical text in some settings. Learning good representations of the very rare diseases in clinical coding ontologies from data alone remains challenging, and alternative means of representing these diseases will form a major focus of future work on automated clinical coding. {\textcopyright} 2018},
annote = {cited By 4},
author = {Catling, F and Spithourakis, G P and Riedel, S},
doi = {10.1016/j.ijmedinf.2018.09.021},
issn = {13865056},
journal = {International Journal of Medical Informatics},
keywords = {Article; artificial neural network; automation; c,Automated,Automation; Classification (of information); Clini,Clinical Coding; Electronic Health Records; Human,Clinical coding; Hierarchical representation; Int,Recurrent neural networks},
pages = {50--61},
publisher = {Elsevier Ireland Ltd},
title = {{Towards automated clinical coding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054423133&doi=10.1016%2Fj.ijmedinf.2018.09.021&partnerID=40&md5=a0b7e46dfd76d8f212e722c64572b240},
volume = {120},
year = {2018}
}
@inproceedings{Singh2019,
abstract = {Drug repositioning offers an economical and efficient alternative to traditional drug discovery. It means that, a drug approved for effect against a particular disease is considered and its applications for novel pharmaceutical purposes are explored in shorter development timelines. Unlike conventional approaches, this work attempts to explore the network of existing drugs and its unmapped indications by treating drug repositioning as a classification problem. The proposed classification model attempts estimation of the relevance of a drug with an unmapped indication. An enhanced word representation model is used for this purpose by integrating knowledge obtained from a structured biological knowledge graph and medical literature. To harvest the structured biological data, we have leveraged multiple biological ontologies to achieve a formal framework in the form of a semantic knowledge graph. Our novelty lies in that we have exploited knowledge from biological knowledge graph and medical corpora to complement each other. This makes our method competent with well established drug repositioning techniques. {\textcopyright} 2019 IEEE.},
annote = {cited By 0; Conference of 16th IEEE India Council International Conference, INDICON 2019 ; Conference Date: 13 December 2019 Through 15 December 2019; Conference Code:158465},
author = {Singh, A V and Negi, A},
booktitle = {2019 IEEE 16th India Council International Conference, INDICON 2019 - Symposium Proceedings},
doi = {10.1109/INDICON47234.2019.9029004},
isbn = {9781728123271},
keywords = {Biological ontologies; Classification models; Con,Knowledge management,Semantics},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Towards better drug repositioning using joint learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083040077&doi=10.1109%2FINDICON47234.2019.9029004&partnerID=40&md5=2dadb7174dcc31a3c9a4722e72099e22},
year = {2019}
}
@inproceedings{Shigarov20191,
abstract = {The paper is devoted to the problem of an end-to-end table transformation from untagged portable documents (PDF) to linked data. It covers the issues of the table extraction from documents, the reconstruction of logical table structure, the conceptualization of their natural-language content, and the linking of extracted data with external vocabularies. We consider some perspective approaches for the deep-learning-based table detection, heuristic-based table structure recognition, rule-based table analysis, and knowledge-based table interpretation. They can be used as a basis to develop a consistent solution for this problem. Our application experience confirms that such solutions are demanded for populating databases and generating ontologies with tabular data being extracted from weakly and semi-structured documents. Copyright {\textcopyright} 2019 for this paper by its authors.},
annote = {cited By 0; Conference of 2nd Scientific-Practical Workshop Information Technologies: Algorithms, Models, Systems, ITAMS 2019 ; Conference Date: 20 September 2019; Conference Code:152357},
author = {Shigarov, A and Cherepanov, I and Cherkashin, E and Dorodnykh, N and Khristyuk, V and Mikhailov, A and Paramonov, V and Rozhkow, E and Yurin, A},
booktitle = {CEUR Workshop Proceedings},
editor = {{Bychkov I.V.}, Karastoyanov D},
issn = {16130073},
keywords = {Application experiences; Data transformation; Doc,Data handling; Deep learning; Extraction; Knowledg,Data mining},
pages = {1--12},
publisher = {CEUR-WS},
title = {{Towards end-to-end transformation of arbitrary tables from untagged portable documents (PDF) to linked data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073540721&partnerID=40&md5=25549f2cdb1a76353088505254d72f47},
volume = {2463},
year = {2019}
}
@article{Thoma2017694,
abstract = {Knowledge Graphs (KGs) effectively capture explicit relational knowledge about individual entities. However, visual attributes of those entities, like their shape and color and pragmatic aspects concerning their usage in natural language are not covered. Recent approaches encode such knowledge by learning latent representations (‘embeddings') separately: In computer vision, visual object features are learned from large image collections and in computational linguistics, word embeddings are extracted from huge text corpora which capture their distributional semantics. We investigate the potential of complementing the relational knowledge captured in KG embeddings with knowledge from text documents and images by learning a shared latent representation that integrates information across those modalities. Our empirical results show that a joined concept representation provides measurable benefits for (i) semantic similarity benchmarks, since it shows a higher correlation with the human notion of similarity than uni- or bi-modal representations, and (ii) entity-type prediction tasks, since it clearly outperforms plain KG embeddings. These findings encourage further research towards capturing types of knowledge that go beyond today's KGs. {\textcopyright} Springer International Publishing AG 2017.},
annote = {cited By 9; Conference of 16th International Semantic Web Conference, ISWC 2017 ; Conference Date: 21 October 2017 Through 25 October 2017; Conference Code:200299},
author = {Thoma, S and Rettinger, A and Both, F},
doi = {10.1007/978-3-319-68288-4_41},
editor = {{Cudre-Mauroux P. Lange C.}, d'Amato C Fernandez M Heflin J Lecue F Tamma V Sequeda J},
isbn = {9783319682877},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Distributional semantics; Embeddings; Entity-type,Knowledge management; Natural language processing,Semantic Web},
pages = {694--710},
publisher = {Springer Verlag},
title = {{Towards holistic concept representations: Embedding relational knowledge, visual attributes, and distributional word semantics}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032216290&doi=10.1007%2F978-3-319-68288-4_41&partnerID=40&md5=e6019b231eb1199aeed8bc0be97f9987},
volume = {10587 LNCS},
year = {2017}
}
@inproceedings{Simov2017679,
abstract = {Word vectors with varying dimensionalities and produced by different algorithms have been extensively used in NLP. The corpora that the algorithms are trained on can contain either natural language text (e.g. Wikipedia or newswire articles) or artificially-generated pseudo corpora due to natural data sparseness. We exploit Lexical Chain based templates over Knowledge Graph for generating pseudo-corpora with controlled linguistic value. These corpora are then used for learning word embeddings. A number of experiments have been conducted over the following test sets: WordSim353 Similarity, WordSim353 Relatedness and SimLex-999. Lhe results show that, on the one hand, the incorporation of many-relation lexical chains improves results, but on the other hand, unrestricted-length chains remain difficult to handle with respect to their huge quantity. {\textcopyright} 2018 Association for Computational Linguistics (ACL). All rights reserved.},
annote = {cited By 2; Conference of 11th International Conference on Recent Advances in Natural Language Processing, RANLP 2017 ; Conference Date: 2 September 2017 Through 8 September 2017; Conference Code:135740},
author = {Simov, K and Boytcheva, S and Osenova, P},
booktitle = {International Conference Recent Advances in Natural Language Processing, RANLP},
doi = {10.26615/978-954-452-049-6-087},
editor = {{Mitkov R. Temnikova I.}, Bontcheva K Nikolova I Angelova G},
isbn = {9789544520489},
issn = {13138502},
keywords = {Chains; Deep learning; Graphic methods; Linguistic,Data sparseness; Embeddings; Knowledge graphs; Le,Natural language processing systems},
pages = {679--685},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Towards lexical chains for knowledge-graph-based word embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045766451&doi=10.26615%2F978-954-452-049-6-087&partnerID=40&md5=b184f7213d9a24aeb09144fb1a85109e},
volume = {2017-Septe},
year = {2017}
}
@inproceedings{Venant2019188,
abstract = {The evaluation of text complexity is an important topic in education. While this objective has been addressed by approaches using lexical and syntactic analysis for decades, semantic complexity is less common, and the recent research works that tackle this question rely on machine learning algorithms that are hardly explainable and are not specifically designed to measure this variable. To address this issue, we explore in this paper the engineering of novel features to evaluate conceptual complexity. Through the construction of a knowledge graph that captures the concepts present in a text and their generalized forms, we measure different graph-based metrics to express such a complexity. Eventually, early-stage evaluations based on a well-known public corpus of students' productions show that the use of these metrics significantly improves performance compared to a state-of-the-art binary neural network classifier. {\textcopyright} EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining. All rights reserved.},
annote = {cited By 0; Conference of 12th International Conference on Educational Data Mining, EDM 2019 ; Conference Date: 2 July 2019 Through 5 July 2019; Conference Code:159954},
author = {Venant, R and D'Aquin, M},
booktitle = {EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining},
editor = {{Lynch C.F. Merceron A.}, Desmarais M Nkambou R},
isbn = {9781733673600},
keywords = {Binary neural networks; Complexity based; Concept,Complex networks; Graphic methods; Learning algori,Data mining},
pages = {188--197},
publisher = {International Educational Data Mining Society},
title = {{Towards the prediction of semantic complexity based on concept graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086001765&partnerID=40&md5=974f9b255290255554b319e7135dcb8b},
year = {2019}
}
@inproceedings{Rastogi201725,
abstract = {We present ways of incorporating logical rules into the construction of embedding based Knowledge Base Completion (KBC) systems. Enforcing "logical consistency" in the predictions of a KBC system guarantees that the predictions comply with logical rules such as symmetry, implication and generalized transitivity. Our method encodes logical rules about entities and relations as convex constraints in the embedding space to enforce the condition that the score of a logically entailed fact must never be less than the minimum score of an antecedent fact. Such constraints provide a weak guarantee that the predictions made by our KBC model will match the output of a logical knowledge base for many types of logical inferences. We validate our method via experiments on a knowledge graph derived fromWordNet. {\textcopyright} Copyright by the paper's authors.},
annote = {cited By 0; Conference of 1st Workshop on Knowledge Graphs and Semantics for Text Retrieval and Analysis, KG4IR 2017 ; Conference Date: 11 August 2017; Conference Code:129401},
author = {Rastogi, P and Poliak, A and {Van Durme}, B},
booktitle = {CEUR Workshop Proceedings},
editor = {{Dietz L. Xiong C.}, Meij E},
issn = {16130073},
keywords = {Convex constraints; Embeddings; Knowledge base; K,Forecasting; Knowledge based systems; Semantics,Information retrieval},
pages = {25--31},
publisher = {CEUR-WS},
title = {{Training relation embeddings under logical constraints}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027878117&partnerID=40&md5=b0d264e505a9e88ead37f173912bd583},
volume = {1883},
year = {2017}
}
@inproceedings{Ammanabrolu20191,
abstract = {Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy learning. In this paper, we explore the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents. Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster. {\textcopyright} 2019 EMNLP-IJCNLP 2019 - Graph-Based Methods for Natural Language Processing - Proceedings of the 13th Workshop. All rights reserved.},
annote = {cited By 1; Conference of 13th Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs 2019, in conjunction with the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019 ; Conference Date: 4 November 2019 Through 4 November 2019; Conference Code:159691},
author = {Ammanabrolu, P and Riedl, M O},
booktitle = {EMNLP-IJCNLP 2019 - Graph-Based Methods for Natural Language Processing - Proceedings of the 13th Workshop},
isbn = {9781950737864},
keywords = {Adventure games; Domain knowledge; Knowledge grap,Computer games; Graphic methods; Intelligent agent,Deep learning},
pages = {1--10},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Transfer in deep reinforcement learning using knowledge graphs}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085045262&partnerID=40&md5=c7c708325d80bd2488a7a6e8129a96f0},
year = {2019}
}
@article{Jang2019296,
abstract = {In this paper, we propose a way to transform traditional Q\&As into conversational Q\&As for an efficient information retrieval in special knowledge. Special knowledge involves difficult words. It requires users to raise a series of questions and get the answers to them to pinpoint the desired information. And, conversational Q\&A is appropriate than the traditional Q\&A because it allows a user to narrow down searches in a solution space. To transform a given set of Q\&As to conversational Q\&A system for special knowledge search, we first explore not only the present traditional Q\&A systems and conversational Q\&A systems for general knowledge search, but also those for special knowledge search. From this, we induce an appropriate search process in conversational Q\&A systems for special knowledge. Secondly, we build an ontology with the help of machine learning to support the navigation in special knowledge. Finally, we give a way to evaluate performance after embedding the ontology on our search process of conversational Q\&A. We apply this procedure to the case of Korean simplified taxation in a Korean Q\&A system, Naver Jisik-In Q\&A. We found that searching through Jisik-In Q\&A with ontology has better usability than using Jisik-In Q\&A only. Therefore, this study aims to improve the usability of special knowledge search, lower the threshold of special knowledge, and develop special knowledge as general as common knowledge using conversational Q\&A based on ontology. However, as the number of user experimented is limited and the classifier for the extracted words from existing Q\&A system should be reviewed by tax expert, so the future work is demanded. {\textcopyright} 2019, Springer Nature Switzerland AG.},
annote = {cited By 0; Conference of 21st International Conference on Human Computer Interaction, HCII 2019 ; Conference Date: 26 July 2019 Through 31 July 2019; Conference Code:232579},
author = {Jang, J and Lee, K},
doi = {10.1007/978-3-030-30712-7_38},
editor = {{Stephanidis C.}, Antona M},
isbn = {9783030307110},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Chatbot; Common knowledge; General knowledge; Sea,Human computer interaction; Taxation,Ontology},
pages = {296--308},
publisher = {Springer},
title = {{Transforming a Specialized Q\&A System to a Chatbot System: A Case of a Simplified Taxation in Korea}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075837901&doi=10.1007%2F978-3-030-30712-7_38&partnerID=40&md5=5207281cce76ae8f8c24362f7cd13f9d},
volume = {1088},
year = {2019}
}
@inproceedings{Fan2014328,
abstract = {Many knowledge repositories nowadays contain billions of triplets, i.e. (head-entity, relationship, tail-entity), as relation instances. These triplets form a directed graph with entities as nodes and relationships as edges. However, this kind of symbolic and discrete storage structure makes it difficult for us to exploit the knowledge to enhance other intelligenceacquired applications (e.g. The Question-Answering System), as many AI-related algorithms prefer conducting computation on continuous data. Therefore, a series of emerging approaches have been proposed to facilitate knowledge computing via encoding the knowledge graph into a low-dimensional embedding space. TransE is the latest and most promising approach among them, and can achieve a higher performance with fewer parameters by modeling the relationship as a transitional vector from the head entity to the tail entity. Unfortunately, it is not flexible enough to tackle well with the various mapping properties of triplets, even though its authors spot the harm on performance. In this paper, we thus propose a superior model called TransM to leverage the structure of the knowledge graph via pre-calculating the distinct weight for each training triplet according to its relational mapping property. In this way, the optimal function deals with each triplet depending on its own weight. We carry out extensive experiments to compare TransM with the state-of-the-art method TransE and other prior arts. The performance of each approach is evaluated within two different application scenarios on several benchmark datasets. Results show that the model we proposed significantly outperforms the former ones with lower parameter complexity as TransE. Copyright 2014 by Miao Fan, Qiang Zhou, Emily Chang, and Thomas Fang Zheng.},
annote = {cited By 49; Conference of 28th Pacific Asia Conference on Language, Information and Computation, PACLIC 2014 ; Conference Date: 12 December 2014 Through 14 December 2014; Conference Code:124040},
author = {Fan, M and Zhou, Q and Chang, E and Zheng, T F},
booktitle = {Proceedings of the 28th Pacific Asia Conference on Language, Information and Computation, PACLIC 2014},
editor = {{Boonkwan P. Aroonmanakun W.}, Supnithi T},
isbn = {9786165518871},
keywords = {Application scenario; Benchmark datasets; Knowled,Artificial intelligence; Benchmarking; Digital sto,Knowledge management},
pages = {328--337},
publisher = {Faculty of Pharmaceutical Sciences, Chulalongkorn University},
title = {{Transition-based knowledge graph embedding with relational mapping properties}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994120919&partnerID=40&md5=7ddcf01d9be7a92815e5a9d2606a76ee},
year = {2014}
}
@inproceedings{Wang2018917,
abstract = {Knowledge graph completion is a critical issue because many applications benefit from their structural and rich resources. In this paper, we propose a method named TransN, which consid- ers the dependencies between triples and incorporates neighbor information dynamically. In experiments, we evaluate our model by link prediction and also conduct several qualitative analyses to prove effectiveness. Experimental results show that our model could integrate neighbor information effectively and outperform state-of-the-art models. {\textcopyright} 2018 ACM.},
annote = {cited By 3; Conference of 41st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2018 ; Conference Date: 8 July 2018 Through 12 July 2018; Conference Code:138121},
author = {Wang, C.-C. and Cheng, P.-J.},
booktitle = {41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2018},
doi = {10.1145/3209978.3210085},
isbn = {9781450356572},
keywords = {Critical issues; Knowledge graphs; Link predictio,Information retrieval,Natural language processing systems},
pages = {917--920},
publisher = {Association for Computing Machinery, Inc},
title = {{Translating representations of knowledge graphs with neighbors}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051461827&doi=10.1145%2F3209978.3210085&partnerID=40&md5=4a364f7a0973073f8b322f9a5ddc6787},
year = {2018}
}
@inproceedings{9194503,
abstract = {Embedding knowledge graph into continuous space(s) is attracting more and more research attention, and lots of novel methods have been proposed. Among them, translation based methods achieved state-of-the-art experimental results. However, most of existing work ignore following two facts. First, once a relation is fixed, its linked head and tail entities will be fixed to a certain extent. Second, in a triplet, if one of its entities and the relation are fixed, the other entity's candidates will also be fixed to a certain extent. Taking these two facts into consideration, we propose a new knowledge graph embedding model named TransP, which defines a head entity space and a tail entity space for each relation. During embedding, TransP first projects entities into these two position spaces. Then the entities in these two position spaces are further projected into a common transformation space, in which the relation is converted into two transformation matrices. A symmetrical score function is designed to connect a correct triplet's head and tail entity in the common space. The basic idea behind this score function is that if a correct triplet holds, its head (tail) entity should be able to be converted into its tail (head) entity when taking the relation's transformation matrix as an intermediate bridge. Viewing the transformation matrices as decoders, this process is just like a common translation process. We evaluate TransP on triplet classification task and link prediction task. Extensive experiments show that TransP achieves much better performance than other baseline models.},
author = {Ren, F and Li, J and Zhang, H and Yang, X},
booktitle = {2020 IEEE International Conference on Knowledge Graph (ICKG)},
doi = {10.1109/ICBK50248.2020.00056},
keywords = {graph theory;learning (artificial intelligence);ma},
month = {aug},
pages = {344--351},
title = {{TransP: A New Knowledge Graph Embedding Model by Translating on Positions*}},
year = {2020}
}
@article{8338385,
abstract = {In this paper, we propose a novel network representation learning model TransPath to encode heterogeneous information networks (HINs). Traditional network representation learning models aim to learn the embeddings of a homogeneous network. TransPath is able to capture the rich semantic and structure information of a HIN via meta-paths. We take advantage of the concept of translation mechanism in knowledge graph which regards a meta-path, instead of an edge, as a translating operation from the first node to the last node. Moreover, we propose a user-guided meta-path sampling strategy which takes users' preference as a guidance, which could explore the semantics of a path more precisely, and meanwhile improve model efficiency via the avoidance of other noisy and meaningless meta-paths. We evaluate our model on two large-scale real-world data sets database systems and logic programming (DBLP) and YELP, and two benchmark tasks similarity search and node classification. We observe that TransPath outperforms other state-of-the-art baselines consistently and significantly.},
author = {Fang, Y and Zhao, X and Tan, Z and Xiao, W},
doi = {10.1109/ACCESS.2018.2827121},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {information networks;learning (artificial intellig},
pages = {20712--20721},
title = {{TransPath: Representation Learning for Heterogeneous Information Networks via Translation Mechanism}},
volume = {6},
year = {2018}
}
@article{Ali201927,
abstract = {Social networks play a key role in providing a new approach to collecting information regarding mobility and transportation services. To study this information, sentiment analysis can make decent observations to support intelligent transportation systems (ITSs) in examining traffic control and management systems. However, sentiment analysis faces technical challenges: extracting meaningful information from social network platforms, and the transformation of extracted data into valuable information. In addition, accurate topic modeling and document representation are other challenging tasks in sentiment analysis. We propose an ontology and latent Dirichlet allocation (OLDA)-based topic modeling and word embedding approach for sentiment classification. The proposed system retrieves transportation content from social networks, removes irrelevant content to extract meaningful information, and generates topics and features from extracted data using OLDA. It also represents documents using word embedding techniques, and then employs lexicon-based approaches to enhance the accuracy of the word embedding model. The proposed ontology and the intelligent model are developed using Web Ontology Language and Java, respectively. Machine learning classifiers are used to evaluate the proposed word embedding system. The method achieves accuracy of 93%, which shows that the proposed approach is effective for sentiment classification. {\textcopyright} 2019 Elsevier B.V.},
annote = {cited By 26},
author = {Ali, F and Kwak, D and Khan, P and El-Sappagh, S and Ali, A and Ullah, S and Kim, K H and Kwak, K.-S.},
doi = {10.1016/j.knosys.2019.02.033},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Control and management; Document Representation;,Data mining,Embeddings; Information management; Intelligent sy},
pages = {27--42},
publisher = {Elsevier B.V.},
title = {{Transportation sentiment analysis using word embedding and ontology-based topic modeling}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062661528&doi=10.1016%2Fj.knosys.2019.02.033&partnerID=40&md5=7daf7dcd9114eb179798c7ecb95afa3b},
volume = {174},
year = {2019}
}
@inproceedings{Guu2015318,
abstract = {Path queries on a knowledge graph can be used to answer compositional questions such as "What languages are spoken by people living in Lisbon?". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new "compositional" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results. {\textcopyright} 2015 Association for Computational Linguistics.},
annote = {cited By 104; Conference of Conference on Empirical Methods in Natural Language Processing, EMNLP 2015 ; Conference Date: 17 September 2015 Through 21 September 2015; Conference Code:116677},
author = {Guu, K and Miller, J and Liang, P},
booktitle = {Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/d15-1038},
isbn = {9781941643327},
keywords = {Base models; Improving performance; Knowledge bas,Graph theory,Knowledge based systems; Natural language processi},
pages = {318--327},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Traversing knowledge graphs in vector space}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959890267&doi=10.18653%2Fv1%2Fd15-1038&partnerID=40&md5=7a1d1c9c8591ae93ff6183b24eb3adc7},
year = {2015}
}
@article{8490812,
abstract = {Knowledge graph embedding aims to represent entities and relations of a knowledge graph in continuous vector spaces. It has increasingly drawn attention for its ability to encode semantics in low dimensional vectors as well as its outstanding performance on many applications, such as question answering systems and information retrieval tasks. Existing methods often handle each triple independently, without considering context information of a triple in the knowledge graph, such an information can be useful for inference of new knowledge. Moreover, the relations and paths between an entity pair also provide information for inference. In this paper, we define a novel context-dependent knowledge graph representation model named triple-context-based knowledge embedding, which is based on the notion of triple context used for embedding entities and relations. For each triple, the triple context is composed of two kinds of graph structured information: one is a set of neighboring entities along with their outgoing relations, the other is a set of relation paths which contain a pair of target entities. Our embedding method is designed to utilize the triple context of each triple while learning embeddings of entities and relations. The method is evaluated on multiple tasks in the paper. Experimental results reveal that our method achieves significant improvements over the state-of-the-art methods.},
author = {Gao, H and Shi, J and Qi, G and Wang, M},
doi = {10.1109/ACCESS.2018.2875066},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Context modeling;Semantics;Task analysis;Computer},
pages = {58978--58989},
title = {{Triple Context-Based Knowledge Graph Embedding}},
volume = {6},
year = {2018}
}
@article{Ebisu20173001,
abstract = {Knowledge graphs have been shown to be useful to many tasks in artificial intelligence. Triples of knowledge graphs are traditionally structured by human editors or extracted from semi-structured information; however, editing is expensive, and semi-structured information is not common. On the other hand, most such information is stored as text. Hence, it is necessary to develop a method that can extract knowledge from texts and then construct or populate a knowledge graph; this has been attempted in various ways. Currently, there are two approaches to constructing a knowledge graph. One is open information extraction (Open IE), and the other is knowledge graph embedding; however, neither is without problems. Stanford Open IE, the current best such system, requires labeled sentences as training data, and knowledge graph embedding systems require numerous triples. Recently, distributed representations of words have become a hot topic in the field of natural language processing, since this approach does not require labeled data for training. These require only plain text, but Mikolov showed that it can perform well with the word analogy task, answering questions such as, “a is to b as c is to ?.” This can be considered as a knowledge extraction task from a text for finding the missing entity of a triple. However, the accuracy is not sufficiently high when applied in a straightforward manner to relations in knowledge graphs, since the method uses only one triple as a positive example. In this paper, we analyze why distributed representations perform such tasks well; we also propose a new method for extracting knowledge from texts that requires much less annotated data. Experiments show that the proposed method achieves considerable improvement compared with the baseline; in particular, the improvement in HITS@10 was more than doubled for some relations. Copyright {\textcopyright} 2017 The Institute of Electronics, Information and Communication Engineers.},
annote = {cited By 2},
author = {Ebisu, T and Ichise, R},
doi = {10.1587/transinf.2017EDP7112},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Artificial intelligence; Natural language processi,Data mining,Distributed representation; Hot topics; Knowledge},
number = {12},
pages = {3001--3009},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{Triple prediction from texts by using distributed representations of words}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038370232&doi=10.1587%2Ftransinf.2017EDP7112&partnerID=40&md5=abde520637a26b4a98203e0abf597760},
volume = {E100D},
year = {2017}
}
@inproceedings{Zhu2019455,
abstract = {Knowledge base is one of the main forms to represent information in a structured way. A knowledge base typically consists of Resource Description Frameworks (RDF) triples which describe the entities and their relations. Generating natural language description of the knowledge base is an important task in NLP, which has been formulated as a conditional language generation task and tackled using the sequence-to-sequence framework. Current works mostly train the language models by maximum likelihood estimation, which tends to generate lousy sentences. In this paper, we argue that such a problem of maximum likelihood estimation is intrinsic, which is generally irrevocable via changing network structures. Accordingly, we propose a novel Triple-to-Text (T2T) framework, which approximately optimizes the inverse Kullback-Leibler (KL) divergence between the distributions of the real and generated sentences. Due to the nature that inverse KL imposes large penalty on fake-looking samples, the proposed method can significantly reduce the probability of generating low-quality sentences. Our experiments on three real-world datasets demonstrate that T2T can generate higher-quality sentences and outperform baseline models in several evaluation metrics. {\textcopyright} 2019 Association for Computing Machinery.},
annote = {cited By 2; Conference of 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019 ; Conference Date: 21 July 2019 Through 25 July 2019; Conference Code:149777},
author = {Zhu, Y and Wan, J and Zhou, Z and Chen, L and Qiu, L and Zhang, W and Jiang, X and Yu, Y},
booktitle = {SIGIR 2019 - Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/3331184.3331232},
isbn = {9781450361729},
keywords = {Evaluation metrics; Knowledge basis; Kullback-Lei,Information retrieval; Inverse problems; Knowledge,Maximum likelihood estimation},
pages = {455--464},
publisher = {Association for Computing Machinery, Inc},
title = {{Triple-to-text: Converting RDF triples into high-quality natural languages via optimizing an inverse KL divergence}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073802101&doi=10.1145%2F3331184.3331232&partnerID=40&md5=4a06548e645363e040c187c8c8943420},
year = {2019}
}
@inproceedings{Bianchi201872,
abstract = {Knowledge Graphs (KGs) are abstractions used to represent knowledge in which real-world entities are organized using a type system where types are organized using a sub-type relation: the ontology. A key factor in many applications is to evaluate the similarity between the types of the ontology. Classical measures to evaluate the semantic similarity between types are often based on the structured organization of the sub-type system. In this work, we show that it is possible to use methods coming from Natural Language Processing to embed types in a vector space starting from textual documents. We show that in this representation some of the properties of the hierarchy are still present and that the similarity in this space captures also characteristics that are close to human behavior. {\textcopyright} 2018 CEUR Workshop Proceedings. All rights reserved.},
annote = {cited By 1; Conference of 1st Workshop on Deep Learning for Knowledge Graphs and Semantic Technologies, DL4KGS 2018 ; Conference Date: 4 June 2018; Conference Code:136761},
author = {Bianchi, F and Soto, M and Palmonari, M and Cutrona, V},
booktitle = {CEUR Workshop Proceedings},
editor = {{Palumbo E. Sack H.}, Koutraki M Cochez M Declerck T Lecue F Gromann D de Melo G Fetahu B Anke L E Kejriwal M},
issn = {16130073},
keywords = {Behavioral research; Natural language processing s,Deep learning,Empirical analysis; Human behaviors; Knowledge gr},
pages = {72--83},
publisher = {CEUR-WS},
title = {{Type vector representations from text: An empirical analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048323758&partnerID=40&md5=2f762b27f030ed6f9c0d42bfaf4877cd},
volume = {2106},
year = {2018}
}
@inproceedings{Rahman201865,
abstract = {Understanding large, structured documents like scholarly articles, requests for proposals or business reports is a complex and difficult task. It involves discovering a document's overall purpose and subject(s), understanding the function and meaning of its sections and subsections, and extracting low level entities and facts about them. In this research, we present a deep learning based document ontology to capture the general purpose semantic structure and domain specific semantic concepts from a large number of academic articles and business documents. The ontology is able to describe different functional parts of a document, which can be used to enhance semantic indexing for a better understanding by human beings and machines. We evaluate our models through extensive experiments on datasets of scholarly articles from arxiv and Request for Proposal documents. {\textcopyright} 2018 CEUR-WS. All Rights Reserved.},
annote = {cited By 1; Conference of Joint 4th Workshop on Semantic Deep Learning: Natural Language Interfaces for the Web of Data and 9th Question Answering over Linked Data Challenge, SemDeep-4_NLIWOD-4 2018 ; Conference Date: 8 October 2018 Through 9 October 2018; Conference Code:141651},
author = {Rahman, M M and Finin, T},
booktitle = {CEUR Workshop Proceedings},
editor = {{Usbeck R. Choi K.-S.}, Ngomo A.-C.N. Saleem M Anke L E Kim J.-D. Declerck T Gromann D},
issn = {16130073},
keywords = {Business documents; Domain specific semantics; Re,Data handling; Linked data; Natural language proce,Deep learning},
pages = {65--76},
publisher = {CEUR-WS},
title = {{Understanding and representing the semantics of large structured documents}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056889853&partnerID=40&md5=13974d8e84aa9eb9e5687c8f88cb8825},
volume = {2241},
year = {2018}
}
@article{Kambau20194751,
abstract = {The amount of digital data is growing at a staggering pace and mostly in the form of text. The growth of data, including multimedia (text, images, video, and audio) poses challenges in developing Multimedia Information Retrieval Systems (MIRS). Today, MIRS uses one or two media as a query input, for example, Google using text and image. There are comprehensive information needs in multimedia, including video and audio media as query input that can increase the amount and variety of information in retrieval result. Also, it is difficult to organize the relationship between the query input and the retrieval result in the same context or semantically related. This paper proposes a Unified Concept-based MIRS using deep learning with Ontology to tackle these problems. There are three main processes of this research; the first is Indexing Process which consist of collecting multimedia data, creating the multimedia dataset, extracting multimedia features, identifying and classifying objects and media format with Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), then storing object indexed as the concepts. The second is Query Processing which consists of inputting multimedia query or object to identifying and classifying object becomes a concept, and then the third is Retrieval and Rank Process of concept or object indexed and concept of query input. The ontology organizes the relationship between the concepts. Resource of this research uses the Cultural Heritage domain. The Unified Concept-based MIRS shows the capability of the system to extract features of four media (text, image, audio, and video) as concept representation to identify the multimedia object from query input and retrieve the object in four types of media at once. The retrieval results performance with deep learning increase about 30%-40% than support vector machine technique, and the usage of ontology increases the retrieval relevant results about 30 % is compared with MIRS without Ontology. {\textcopyright} 2005 – ongoing JATIT \& LLS},
annote = {cited By 0},
author = {Kambau, R A and {Octaviano Pratama}, M},
issn = {19928645},
journal = {Journal of Theoretical and Applied Information Technology},
number = {18},
pages = {4751--4767},
publisher = {Little Lion Scientific},
title = {{Unified concept-based multimedia information retrieval system using deep learning process with ontology}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075500310&partnerID=40&md5=ff545169654cfc3bbc6eb78f7d192af0},
volume = {97},
year = {2019}
}
@article{Xu20201510,
abstract = {OBJECTIVE: Concept normalization, the task of linking phrases in text to concepts in an ontology, is useful for many downstream tasks including relation extraction, information retrieval, etc. We present a generate-and-rank concept normalization system based on our participation in the 2019 National NLP Clinical Challenges Shared Task Track 3 Concept Normalization. MATERIALS AND METHODS: The shared task provided 13 609 concept mentions drawn from 100 discharge summaries. We first design a sieve-based system that uses Lucene indices over the training data, Unified Medical Language System (UMLS) preferred terms, and UMLS synonyms to generate a list of possible concepts for each mention. We then design a listwise classifier based on the BERT (Bidirectional Encoder Representations from Transformers) neural network to rank the candidate concepts, integrating UMLS semantic types through a regularizer. RESULTS: Our generate-and-rank system was third of 33 in the competition, outperforming the candidate generator alone (81.66% vs 79.44%) and the previous state of the art (76.35%). During postevaluation, the model's accuracy was increased to 83.56% via improvements to how training data are generated from UMLS and incorporation of our UMLS semantic type regularizer. DISCUSSION: Analysis of the model shows that prioritizing UMLS preferred terms yields better performance, that the UMLS semantic type regularizer results in qualitatively better concept predictions, and that the model performs well even on concepts not seen during training. CONCLUSIONS: Our generate-and-rank framework for UMLS concept normalization integrates key UMLS features like preferred terms and semantic types with a neural network-based ranking model to accurately link phrases in text to UMLS concepts. {\textcopyright} The Author(s) 2020. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved. For permissions, please email: journals.permissions@oup.com.},
annote = {cited By 0},
author = {Xu, D and Gopale, M and Zhang, J and Brown, K and Begoli, E and Bethard, S},
doi = {10.1093/jamia/ocaa080},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association : JAMIA},
keywords = {article; classifier; competition; deep learning; h},
number = {10},
pages = {1510--1519},
publisher = {NLM (Medline)},
title = {{Unified Medical Language System resources improve sieve-based generation and Bidirectional Encoder Representations from Transformers (BERT)-based ranking for concept normalization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093539184&doi=10.1093%2Fjamia%2Focaa080&partnerID=40&md5=7cc010fce56a2c58d56bb8618925a383},
volume = {27},
year = {2020}
}
@inproceedings{10.1145/3292500.3330838,
abstract = {Many large-scale knowledge bases simultaneously represent two views of knowledge graphs (KGs): an ontology view for abstract and commonsense concepts, and an instance view for specific entities that are instantiated from ontological concepts. Existing KG embedding models, however, merely focus on representing one of the two views alone. In this paper, we propose a novel two-view KG embedding model, JOIE, with the goal to produce better knowledge embedding and enable new applications that rely on multi-view knowledge. JOIE employs both cross-view and intra-view modeling that learn on multiple facets of the knowledge base. The cross-view association model is learned to bridge the embeddings of ontological concepts and their corresponding instance-view entities. The intra-view models are trained to capture the structured knowledge of instance and ontology views in separate embedding spaces, with a hierarchy-aware encoding technique enabled for ontologies with hierarchies. We explore multiple representation techniques for the two model components and investigate with nine variants of JOIE. Our model is trained on large-scale knowledge bases that consist of massive instances and their corresponding ontological concepts connected via a (small) set of cross-view links. Experimental results on public datasets show that the best variant of JOIE significantly outperforms previous models on instance-view triple prediction task as well as ontology population on ontology-view KG. In addition, our model successfully extends the use of KG embeddings to entity typing with promising performance.},
address = {New York, NY, USA},
author = {Hao, Junheng and Chen, Muhao and Yu, Wenchao and Sun, Yizhou and Wang, Wei},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
doi = {10.1145/3292500.3330838},
isbn = {9781450362016},
keywords = {knowledge graph,ontology learning,relational embeddings},
pages = {1709--1719},
publisher = {Association for Computing Machinery},
series = {KDD '19},
title = {{Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts}},
url = {https://doi.org/10.1145/3292500.3330838},
year = {2019}
}
@inproceedings{8983253,
abstract = {The extraction of phenotype information which is naturally contained in electronic health records (EHRs) has been found to be useful in various clinical informatics applications such as disease diagnosis. However, due to imprecise descriptions, lack of gold standards and the demand for efficiency, annotating phenotypic abnormalities on millions of EHR narratives is still challenging. In this work, we propose a novel unsupervised deep learning framework to annotate the phenotypic abnormalities from EHRs via semantic latent representations. The proposed framework takes the advantage of Human Phenotype Ontology (HPO), which is a knowledge base of phenotypic abnormalities, to standardize the annotation results. Experiments have been conducted on 52,722 EHRs from MIMIC-III dataset. Quantitative and qualitative analysis have shown the proposed framework achieves state-of-the-art annotation performance and computational efficiency compared with other methods.},
author = {Zhang, J and Zhang, X and Sun, K and Yang, X and Dai, C and Guo, Y},
booktitle = {2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
doi = {10.1109/BIBM47256.2019.8983253},
keywords = {diseases;electronic health records;ontologies (art},
month = {nov},
pages = {598--603},
title = {{Unsupervised Annotation of Phenotypic Abnormalities via Semantic Latent Representations on Electronic Health Records}},
year = {2019}
}
@inproceedings{Bhingardive20151238,
abstract = {An acid test for any new Word Sense Disambiguation (WSD) algorithm is its performance against the Most Frequent Sense (MFS). The field of WSD has found the MFS baseline very hard to beat. Clearly, if WSD researchers had access to MFS values, their striving to better this heuristic will push the WSD frontier. However, getting MFS values requires sense annotated corpus in enormous amounts, which is out of bounds for most languages, even if their WordNets are available. In this paper, we propose an unsupervised method for MFS detection from the untagged corpora, which exploits word embeddings. We compare the word embedding of a word with all its sense embeddings and obtain the predominant sense with the highest similarity. We observe significant performance gain for Hindi WSD over the WordNet First Sense (WFS) baseline. As for English, the SemCor baseline is bettered for those words whose frequency is greater than 2. Our approach is language and domain independent. {\textcopyright} 2015 Association for Computational Linguistics.},
annote = {cited By 21; Conference of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2015 ; Conference Date: 31 May 2015 Through 5 June 2015; Conference Code:116756},
author = {Bhingardive, S and Singh, D and Rudra, M V and Redkar, H and Bhattacharyya, P},
booktitle = {NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference},
doi = {10.3115/v1/n15-1132},
isbn = {9781941643495},
keywords = {Computational linguistics; Natural language proces,Domain independents; Performance Gain; Unsupervis,Embeddings},
pages = {1238--1243},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Unsupervised most frequent sense detection usingword embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960172274&doi=10.3115%2Fv1%2Fn15-1132&partnerID=40&md5=fc1dbc501d0ef40c2956f5c8489a252c},
year = {2015}
}
@article{8412214,
abstract = {This paper presents a cloud-based building energy management system, underpinned by semantic middleware, that integrates an enhanced sensor network with advanced analytics, accessible through an intuitive Web-based user interface. The proposed solution is described in terms of its three key layers: 1) user interface; 2) intelligence; and 3) interoperability. The system's intelligence is derived from simulation-based optimized rules, historical sensor data mining, and a fuzzy reasoner. The solution enables interoperability through a semantic knowledge base, which also contributes intelligence through reasoning and inference abilities, and which are enhanced through intelligent rules. Finally, building energy performance monitoring is delivered alongside optimized rule suggestions and a negotiation process in a 3-D Web-based interface using WebGL. The solution has been validated in a real pilot building to illustrate the strength of the approach, where it has shown over 25% energy savings. The relevance of this paper in the field is discussed, and it is argued that the proposed solution is mature enough for testing across further buildings.},
author = {Howell, S K and Wicaksono, H and Yuce, B and McGlinn, K and Rezgui, Y},
doi = {10.1109/TCYB.2018.2839700},
issn = {2168-2275},
journal = {IEEE Transactions on Cybernetics},
keywords = {building management systems;cloud computing;data m},
number = {9},
pages = {3278--3292},
title = {{User Centered Neuro-Fuzzy Energy Management Through Semantic-Based Optimization}},
volume = {49},
year = {2019}
}
@article{Wilcke2019,
abstract = {In recent years, there has been a growing interest from the digital humanities in knowledge graphs as data modelling paradigm. Already, this has led to the creation of many such knowledge graphs, many of which are now available as part of the Linked Open Data cloud. This presents new opportunities for data mining. In this work, we develop, implement, and evaluate (both data-driven and user-driven) an end-to-end pipeline for user-centric pattern mining on knowledge graphs in the humanities. This pipeline combines constrained generalized association rule mining with natural language output and facet rule browsing to allow for transparency and interpretability—two key domain requirements. Experiments in the archaeological domain show that domain experts were positively surprised by the range of patterns that were discovered and were overall optimistic about the future potential of this approach. {\textcopyright} 2018 Elsevier B.V.},
annote = {cited By 2},
author = {Wilcke, W X and de Boer, V and de Kleijn, M T M and van Harmelen, F A H and Scholten, H J},
doi = {10.1016/j.websem.2018.12.004},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Archaeology; Digital humanities; Generalized asso,Association rules; Graphic methods; Pipelines,Data mining},
publisher = {Elsevier B.V.},
title = {{User-centric pattern mining on knowledge graphs: An archaeological case study}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058640501&doi=10.1016%2Fj.websem.2018.12.004&partnerID=40&md5=2b46863d0e03860ab2e18d0cebce23fd},
volume = {59},
year = {2019}
}
@inproceedings{Jacinto2012184,
abstract = {The automatic acquisition of models to represent existing domain knowledge is a key step to further develop domain driven data mining. Ontology Learning has been mostly focused on unstructured data sources, as text, leaving structured data almost ignored. This is probably due to the existence of a model behind that kind of data, that without being an ontology, reveals some data semantics. This paper extends the work by Borgida [1], giving to the user the possibility to choose the level of detail of a domain ontology learnt from a relational database. Beside the full exploration of relational model premises, we apply association rules mining to discover basic axioms, which describe the hidden assertions underlying the domain. {\textcopyright} 2012 IEEE.},
address = {Shanghai},
annote = {cited By 2; Conference of 2012 IEEE/ACIS 11th International Conference on Computer and Information Science, ICIS 2012 ; Conference Date: 30 May 2012 Through 1 June 2012; Conference Code:91164},
author = {Jacinto, C and Antunes, C},
booktitle = {Proceedings - 2012 IEEE/ACIS 11th International Conference on Computer and Information Science, ICIS 2012},
doi = {10.1109/ICIS.2012.115},
isbn = {9780769546940},
keywords = {Association rules mining; Automatic acquisition; D,Data mining; Semantics,Information science},
pages = {184--189},
title = {{User-driven ontology learning from structured data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864075322&doi=10.1109%2FICIS.2012.115&partnerID=40&md5=d6b9c41871a5ec5113b005a0f31c58d5},
year = {2012}
}
@article{Martínez-Romero2019,
abstract = {Metadata-the machine-readable descriptions of the data-are increasingly seen as crucial for describing the vast array of biomedical datasets that are currently being deposited in public repositories. While most public repositories have firm requirements that metadata must accompany submitted datasets, the quality of those metadata is generally very poor. A key problem is that the typical metadata acquisition process is onerous and time consuming, with little interactive guidance or assistance provided to users. Secondary problems include the lack of validation and sparse use of standardized terms or ontologies when authoring metadata. There is a pressing need for improvements to the metadata acquisition process that will help users to enter metadata quickly and accurately. In this paper, we outline a recommendation system for metadata that aims to address this challenge. Our approach uses association rule mining to uncover hidden associations among metadata values and to represent them in the form of association rules. These rules are then used to present users with real-time recommendations when authoring metadata. The novelties of our method are that it is able to combine analyses of metadata from multiple repositories when generating recommendations and can enhance those recommendations by aligning them with ontology terms. We implemented our approach as a service integrated into the CEDAR Workbench metadata authoring platform, and evaluated it using metadata from two public biomedical repositories: US-based National Center for Biotechnology Information BioSample and European Bioinformatics Institute BioSamples. The results show that our approach is able to use analyses of previously entered metadata coupled with ontology-based mappings to present users with accurate recommendations when authoring metadata. {\textcopyright} 2019 The Author(s) 2019. Published by Oxford University Press.},
annote = {cited By 2},
author = {Mart{\'{i}}nez-Romero, M and O'Connor, M J and Egyedi, A L and Willrett, D and Hardi, J and Graybeal, J and Musen, M A},
doi = {10.1093/database/baz059},
issn = {17580463},
journal = {Database},
keywords = {Computational Biology; Data Mining; Databases,Factual; Metadata,biology; data mining; factual database; metadata;},
number = {1},
publisher = {Oxford University Press},
title = {{Using association rule mining and ontologies to generate metadata recommendations from multiple biomedical databases}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068404255&doi=10.1093%2Fdatabase%2Fbaz059&partnerID=40&md5=46b29f27c3c30b7ea12aad760557d76d},
volume = {2019},
year = {2019}
}
@inproceedings{ELQZ65E8,
abstract = {Semantic taxonomies are powerful tools that provide structured knowledge to Natural Language Processing (NLP), Information Retreval (IR), and general Artificial Intelligence (AI) systems. These taxonomies are extensively used for solving knowledge rich problems such as textual entailment and question answering. In this paper, we present a taxonomy induction system and evaluate it using the benchmarks provided in the Taxonomy Extraction Evaluation (TExEval2) Task. The task is to identify hyponym-hypernym relations and to construct a taxonomy from a given domain specific list. Our approach is based on a word embedding, trained from a large corpus and string-matching approaches. The overall approach is semi-supervised. We propose a generic algorithm that utilizes the vectors from the embedding effectively, to identify hyponym-hypernym relations and to induce the taxonomy. The system generated taxonomies on English language for three different domains (environment, food and science) which are evaluated against gold standard taxonomies. The system achieved good results for hyponym-hypernym identification and taxonomy induction, especially when compared to other tools using similar background knowledge.},
author = {Zafar, B and Cochez, M and Qamar, U},
booktitle = {2016 International Conference on Frontiers of Information Technology (FIT)},
keywords = {information retrieval;natural language processing;},
month = {dec},
pages = {348--353},
title = {{Using Distributional Semantics for Automatic Taxonomy Induction}},
year = {2016}
}
@article{43JBXLQQ,
abstract = {The Gene Ontology (GO) is a structured repository of concepts (GO terms) that are associated to one or more gene products. The process of association is referred to as annotation. The relevance and the specificity of both GO terms and annotations are evaluated by a measure defined as information content (IC). The analysis of annotated data is thus an important challenge for bioinformatics. There exist different approaches of analysis. From those, the use of association rules (AR) may provide useful knowledge, and it has been used in some applications, e.g. improving the quality of annotations. Nevertheless classical association rules algorithms do not take into account the source of annotation nor the importance yielding to the generation of candidate rules with low IC. This paper presents GO-WAR (Gene Ontology-based Weighted Association Rules) a methodology for extracting weighted association rules. GO-WAR can extract association rules with a high level of IC without loss of support and confidence from a dataset of annotated data. A case study on using of GO-WAR on publicly available GO annotation datasets is used to demonstrate that our method outperforms current state of the art approaches. {\textcopyright} 2015 Elsevier Ireland Ltd.},
annote = {cited By 8},
author = {Agapito, Giuseppe and Cannataro, Mario and Guzzi, Pietro Hiram and Milano, Marianna},
issn = {01692607},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Algorithms; Data Mining; Molecular Sequence Annot,Association rules; Bioinformatics; Genes,Data mining,Gene ontology; Gene products; GO terms; Informati,algorithm; Article; bioinformatics; computer prog},
number = {2},
pages = {113--122},
publisher = {Elsevier Ireland Ltd},
title = {{Using GO-WAR for mining cross-ontology weighted association rules}},
volume = {120},
year = {2015}
}
@inproceedings{Wang20172351,
abstract = {Modern Knowledge Graphs such as DBPedia contain significant information regarding Named Entities and the logical relationships which exist between them. Twitter on the other hand, contains important information on the popularity and frequency with which these entities are mentioned and discussed in combination with one another. In this paper we investigate whether these two sources of information can be used to complement and explain one another. In particular, we would like to know whether the logical relationships (a.k.a. semantic paths) which exist between pairs of known entities can help to explain the frequency with which those entities co-occur with one another in Twitter. To do this we train attranking function over semantic paths between pairs of entities. The aim of the ranker is to identify the path that most likely explains why a particular pair of entities have appeared together in a particular tweet. We train the ranking model using a number of lexical, graph-embedding and popularity-based features over semantic paths containing a single intermediate entity and demonstrate the efficacy of the model for determining why pairs of entities occur together in tweets. {\textcopyright} 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.},
annote = {cited By 1; Conference of 26th ACM International Conference on Information and Knowledge Management, CIKM 2017 ; Conference Date: 6 November 2017 Through 10 November 2017; Conference Code:131841},
author = {Wang, Y and Carman, M J and Li, Y.-F.},
booktitle = {International Conference on Information and Knowledge Management, Proceedings},
doi = {10.1145/3132847.3133161},
isbn = {9781450349185},
keywords = {Dbpedia; Importance ranking; Knowledge graphs; Mi,Information retrieval; Knowledge management; Learn,Social networking (online)},
pages = {2351--2354},
publisher = {Association for Computing Machinery},
title = {{Using knowledge graphs to explain entity co-occurrence in twitter}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037341536&doi=10.1145%2F3132847.3133161&partnerID=40&md5=1723ed519a4d35f9b9181130ee66ee27},
year = {2017}
}
@article{ISI:000531089000019,
abstract = {Background: Health education emerged as an important intervention for
improving the awareness and self-management abilities of chronic disease
patients. The development of information technologies has changed the
form of patient educational materials from traditional paper materials
to electronic materials. To date, the amount of patient educational
materials on the internet is tremendous, with variable quality, which
makes it hard to identify the most valuable materials by individuals
lacking medical backgrounds.
Objective: The aim of this study was to develop a health recommender
system to provide appropriate educational materials for chronic disease
patients in China and evaluate the effect of this system.
Methods: A knowledge-based recommender system was implemented using
ontology and several natural language processing (NLP) techniques. The
development process was divided into 3 stages. In stage 1, an ontology
was constructed to describe patient characteristics contained in the
data. In stage 2, an algorithm was designed and implemented to generate
recommendations based on the ontology. Patient data and educational
materials were mapped to the ontology and converted into vectors of the
same length, and then recommendations were generated according to
similarity between these vectors. In stage 3, the ontology and algorithm
were incorporated into an mHealth system for practical use. Keyword
extraction algorithms and pretrained word embeddings were used to
preprocess educational materials. Three strategies were proposed to
improve the performance of keyword extraction. System evaluation was
based on a manually assembled test collection for 50 patients and 100
educational documents. Recommendation performance was assessed using the
macro precision of top-ranked documents and the overall mean average
precision (MAP).
Results: The constructed ontology contained 40 classes, 31 object
properties, 67 data properties, and 32 individuals. A total of 80 SWRL
rules were defined to implement the semantic logic of mapping patient
original data to the ontology vector space. The recommender system was
implemented as a separate Web service connected with patients'
smartphones. According to the evaluation results, our system can achieve
a macro precision up to 0.970 for the top 1 recommendation and an
overall MAP score up to 0.628.
Conclusions: This study demonstrated that a knowledge-based health
recommender system has the potential to accurately recommend educational
materials to chronic disease patients. Traditional NLP techniques
combined with improvement strategies for specific language and domain
proved to be effective for improving system performance. One direction
for future work is to explore the effect of such systems from the
perspective of patients in a practical setting.},
address = {130 QUEENS QUAY E, STE 1102, TORONTO, ON M5A 0P6, CANADA},
author = {Wang, Zheyu and Huang, Haoce and Cui, Liping and Chen, Juan and An, Jiye and Duan, Huilong and Ge, Huiqing and Deng, Ning},
doi = {10.2196/17642},
journal = {JMIR MEDICAL INFORMATICS},
keywords = {health education; ontology; natural language proce},
month = {apr},
number = {4},
publisher = {JMIR PUBLICATIONS, INC},
title = {{Using Natural Language Processing Techniques to Provide Personalized Educational Materials for Chronic Disease Patients in China: Development and Assessment of a Knowledge-Based Health Recommender System}},
type = {Article},
volume = {8},
year = {2020}
}
@article{10.1016/j.ins.2010.09.027,
abstract = {Data mining is used to discover hidden patterns or structures in large databases. Association rule induction extracts frequently occurring patterns in the form of association rules. However, this technique has a drawback as it typically generates a large number of association rules. Several methods have been proposed to prune the set of extracted rules in order to present only those which are of interest to the domain experts. Some of these methods involve subjective analysis based on prior domain knowledge, while others can be considered to involve objective, data-driven analysis based on numerical measures that provide a partial description of the interestingness of the extracted association rules. Recently it has been proposed that ontologies could be used to guide the data mining process. In this paper, we propose a hybrid pruning method that involve the use of objective analysis and subjective analysis, with the latter involving the use of an ontology. We demonstrate the applicability of this hybrid method using a medical database.},
address = {USA},
author = {Mansingh, Gunjan and Osei-Bryson, Kweku-Muata and Reichgelt, Han},
doi = {10.1016/j.ins.2010.09.027},
issn = {0020-0255},
journal = {Inf. Sci.},
keywords = {Association rule induction,Data mining process,Healthcare,Interestingness,Objective measures,Ontologies,Subjective measures},
month = {feb},
number = {3},
pages = {419--434},
publisher = {Elsevier Science Inc.},
title = {{Using Ontologies to Facilitate Post-Processing of Association Rules by Domain Experts}},
url = {https://doi.org/10.1016/j.ins.2010.09.027},
volume = {181},
year = {2011}
}
@article{Reynaud2019241,
abstract = {In this article, we compare the use of Redescription Mining (RM) and Association Rule Mining (ARM) for discovering class definitions in Linked Open Data (LOD). RM is aimed at mining alternate descriptions from two datasets related to the same set of individuals. We reuse RM for providing category definitions in DBpedia in terms of necessary and sufficient conditions (NSC). Implications and AR can be jointly used for mining category definitions still in terms of NSC. In this paper, we firstly, recall the basics of redescription mining and make precise the principles of definition discovery. Then we detail a series of experiments carried out on datasets extracted from DBpedia. We analyze the different outputs related to RM and ARM applications, and we discuss the strengths and limitations of both approaches. Finally, we point out possible improvements of the approaches. {\textcopyright} Springer Nature Switzerland AG 2019.},
annote = {cited By 0; Conference of 15th International Conference on Formal Concept Analysis, ICFCA 2019 ; Conference Date: 25 June 2019 Through 28 June 2019; Conference Code:227309},
author = {Reynaud, J and Toussaint, Y and Napoli, A},
doi = {10.1007/978-3-030-21462-3_16},
editor = {{Cristea D. Le Ber F.}, Sertkaya B},
isbn = {9783030214616},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Association rule minings (ARM); Concept analysis;,Association rules; Data mining; Linked data; Open,Formal concept analysis},
pages = {241--256},
publisher = {Springer Verlag},
title = {{Using redescriptions and formal concept analysis for mining definitions in linked data}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068180768&doi=10.1007%2F978-3-030-21462-3_16&partnerID=40&md5=a32b04cce47a9d99582a9d20b074f481},
volume = {11511 LNAI},
year = {2019}
}
@article{Voogd201978,
abstract = {In decision support systems, information from many different sources must be integrated and interpreted to aid the process of gaining situational understanding. These systems assist users in making the right decisions, for example when under time pressure. In this work, we discuss a controlled automated support tool for gaining situational understanding, where multiple sources of information are integrated. In the domain of operational safety and security, available data is often limited and insufficient for sub-symbolic approaches such as neural networks. Experts generally have high level (symbolic) knowledge but may lack the ability to adapt and apply that knowledge to the current situation. In this work, we combine sub-symbolic information and technologies (machine learning) with symbolic knowledge and technologies (from experts or ontologies). This combination offers the potential to steer the interpretation of the little data available with the knowledge of the expert. We created a framework that consists of concepts and relations between those concepts, for which the exact relational importance is not necessarily specified. A machine-learning approach is used to determine the relations that fit the available data. The use of symbolic concepts allows for properties such as explainability and controllability. The framework was tested with expert rules on an attribute dataset of vehicles. The performance with incomplete inputs or smaller training sets was compared to a traditional fully-connected neural network. The results show it as a viable alternative when data is limited or incomplete, and that more semantic meaning can be extracted from the activations of concepts. {\textcopyright} IFIP International Federation for Information Processing 2019.},
annote = {cited By 0; Conference of 3rd IFIP Cross Domain Conference for Machine Learning and Knowledge Extraction, CD-MAKE 2019 ; Conference Date: 26 August 2019 Through 29 August 2019; Conference Code:231589},
author = {Voogd, J and de Heer, P and Veltman, K and Hanckmann, P and van Lith, J},
doi = {10.1007/978-3-030-29726-8_6},
editor = {{Holzinger A. Kieseberg P.}, Tjoa A M Weippl E},
isbn = {9783030297251},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining,Decision support systems; Engineering education; E,Decision supports; Explainability; Fully connecte},
pages = {78--93},
publisher = {Springer Verlag},
title = {{Using Relational Concept Networks for Explainable Decision Support}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072856897&doi=10.1007%2F978-3-030-29726-8_6&partnerID=40&md5=a9f6cd48578f76e4b1a9be28caa8f2dd},
volume = {11713 LNCS},
year = {2019}
}
@inproceedings{10.1007/978-3-642-37899-7_16,
abstract = {Association rule (AR) mining has been widely used on the electronic medical records (EMR) for discovering hidden knowledge and medical patterns and also for improving the information retrieval performance via query expansion. A major obstacle in association rule mining is that often a huge number of rules are generated even with very reasonable support and confidence. The main challenge of using AR in information retrieval (IR) is to select the rules that are related to the query, since many of them are trivial, redundant or semantically wrong. In this paper, we propose a novel approach to modeling medical query contexts based on mining semantic-based AR for improving clinical text retrieval. We semantically index the EMR with concepts of UMLS ontology. First, the concepts in the query context are derived from the rules that cover the query and then weighted according to their semantic relatedness to the query concepts. The query context is then exploited to re-rank patients records for improving clinical retrieval performance. We evaluate our approach on the medical TREC dataset. Results show that our proposed approach allows performing better retrieval performance than the probabilistic BM25 model.},
address = {Berlin, Heidelberg},
author = {Babashzadeh, Atanaz and Daoud, Mariam and Huang, Jimmy},
booktitle = {Proceedings of the Second International Conference on Health Information Science},
doi = {10.1007/978-3-642-37899-7_16},
isbn = {9783642378980},
pages = {186--197},
publisher = {Springer-Verlag},
series = {HIS'13},
title = {{Using Semantic-Based Association Rule Mining for Improving Clinical Text Retrieval}},
url = {https://doi.org/10.1007/978-3-642-37899-7_16},
year = {2013}
}
@inproceedings{DeKok2020834,
abstract = {Nowadays, the Web is the main platform to gather information. The growing amount of freely available unstructured data has increased the interest in sentiment analysis, where the goal is to extract opinions from text. In this paper we focus on review-level aspect-based sentiment analysis, where we predict the sentiment of a certain aspect in a review. We propose a two-stage sentiment analysis algorithm. In the first stage a domain ontology is utilized to predict the sentiment. If the domain ontology stage is inconclusive, a back-up stage based on an SVM bag-of-words model is employed. Furthermore, the use of word embeddings to improve the domain ontology coverage in the first stage by finding semantically similar words is investigated. We find that the two-stage approach significantly outperforms two baseline methods and achieves competitive results for the SemEval-2016 data. Furthermore, by not employing the back-up stage, we still perform significantly better than the baselines. Lastly, we find that employing word embeddings improves the accuracy when the domain ontology size is relatively small. {\textcopyright} 2020 ACM.},
annote = {cited By 0; Conference of 35th Annual ACM Symposium on Applied Computing, SAC 2020 ; Conference Date: 30 March 2020 Through 3 April 2020; Conference Code:158685},
author = {{De Kok}, S and Frasincar, F},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/3341105.3373848},
isbn = {9781450368667},
keywords = {Back up; Bag-of-words models; Baseline methods; D,Embeddings; Information retrieval; Sentiment analy,Ontology},
pages = {834--842},
publisher = {Association for Computing Machinery},
title = {{Using word embeddings for ontology-driven aspect-based sentiment analysis}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083034302&doi=10.1145%2F3341105.3373848&partnerID=40&md5=e977c673328e12ae02888d7389c50703},
year = {2020}
}
@inproceedings{Zhang20186069,
abstract = {Knowledge graph (KG) is known to be helpful for the task of question answering (QA), since it provides well-structured relational information between entities, and allows one to further infer indirect facts. However, it is challenging to build QA systems which can learn to reason over knowledge graphs based on question-answer pairs alone. First, when people ask questions, their expressions are noisy (for example, typos in texts, or variations in pronunciations), which is non-trivial for the QA system to match those mentioned entities to the knowledge graph. Second, many questions require multi-hop logic reasoning over the knowledge graph to retrieve the answers. To address these challenges, we propose a novel and unified deep learning architecture, and an end-to-end variational learning algorithm which can handle noise in questions, and learn multi-hop reasoning simultaneously. Our method achieves state-of-the-art performance on a recent benchmark dataset in the literature. We also derive a series of new benchmark datasets, including questions for multi-hop reasoning, questions paraphrased by neural translation model, and questions in human voice. Our method yields very promising results on all these challenging datasets. Copyright {\textcopyright} 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
annote = {cited By 39; Conference of 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference Date: 2 February 2018 Through 7 February 2018; Conference Code:143510},
author = {Zhang, Y and Dai, H and Kozareva, Z and Smola, A J and Song, L},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
isbn = {9781577358008},
keywords = {Artificial intelligence; Benchmarking; Learning al,Benchmark datasets; Knowledge graphs; Learning ar,Deep learning},
pages = {6069--6076},
publisher = {AAAI press},
title = {{Variational reasoning for question answering with knowledge graph}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060464851&partnerID=40&md5=e20549798b3f7ebbb7a6ede335984a21},
year = {2018}
}
@article{Denaux2019881,
abstract = {The proliferation of knowledge graphs and recent advances in Artificial Intelligence have raised great expectations related to the combination of symbolic and distributional semantics in cognitive tasks. This is particularly the case of knowledge-based approaches to natural language processing as near-human symbolic understanding relies on expressive, structured knowledge representations. Engineered by humans, such knowledge graphs are frequently well curated and of high quality, but at the same time can be labor-intensive, brittle or biased. The work reported in this paper aims to address such limitations, bringing together bottom-up, corpus-based knowledge and top-down, structured knowledge graphs by capturing as embeddings in a joint space the semantics of both words and concepts from large document corpora. To evaluate our results, we perform the largest and most comprehensive empirical study around this topic that we are aware of, analyzing and comparing the quality of the resulting embeddings over competing approaches. We include a detailed ablation study on the different strategies and components our approach comprises and show that our method outperforms the previous state of the art according to standard benchmarks. {\textcopyright} 2019 - IOS Press and the authors. All rights reserved.},
annote = {cited By 0},
author = {Denaux, R and Gomez-Perez, J M},
doi = {10.3233/SW-180361},
issn = {15700844},
journal = {Semantic Web},
number = {5},
pages = {881--908},
publisher = {IOS Press},
title = {{Vecsigrafo: Corpus-based word-concept embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072398040&doi=10.3233%2FSW-180361&partnerID=40&md5=b07dd887d40e1c98745603b3382444c1},
volume = {10},
year = {2019}
}
@article{Ballan201080,
abstract = {An approach for automatic annotation and retrieval of video content uses semantic concept classifiers and ontologies to permit expanded queries to synonyms and concept specializations. {\textcopyright} 2010 IEEE.},
annote = {cited By 43},
author = {Ballan, L and Bertini, M and {Del Bimbo}, A and Serra, G},
doi = {10.1109/MMUL.2010.4},
issn = {1070986X},
journal = {IEEE Multimedia},
keywords = {Automatic annotation; Automatic video annotation;,Information retrieval; Ontology,Semantic Web},
number = {4},
pages = {80--88},
title = {{Video annotation and retrieval using ontologies and rule learning}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149312458&doi=10.1109%2FMMUL.2010.4&partnerID=40&md5=dc38334a3b5b2e12b3bf7039afa7ef19},
volume = {17},
year = {2010}
}
@inproceedings{Diao20202507,
abstract = {Homographic puns have a long history in human writing, widely used in written and spoken literature, which usually occur in a certain syntactic or stylistic structure. How to recognize homographic puns is an important research. However, homographic pun recognition does not solve very well in existing work. In this work, we first use WordNet to understand and expand word embedding for settling the polysemy of homographic puns, and then propose a WordNet-Encoded Collocation-Attention network model (WECA) which combined with the context weights for recognizing the puns. Our experiments on the SemEval2017 Task7 and Pun of the Day demonstrate that the proposed model is able to distinguish between homographic pun and non-homographic pun texts. We show the effectiveness of the model to present the capability of choosing qualitatively informative words. The results show that our model achieves the state-of-the-art performance on homographic puns recognition. {\textcopyright} 2018 Association for Computational Linguistics},
annote = {cited By 4; Conference of 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018 ; Conference Date: 31 October 2018 Through 4 November 2018; Conference Code:158085},
author = {Diao, Y and Lin, H and Wu, D and Yang, L and Xu, K and Yang, Z and Wang, J and Zhang, S and Xu, B and Zhang, D},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
editor = {{Riloff E. Chiang D.}, Hockenmaier J Tsujii J},
isbn = {9781948087841},
keywords = {Natural language processing systems; Ontology,Network coding,Network modeling; State-of-the-art performance; W},
pages = {2507--2516},
publisher = {Association for Computational Linguistics},
title = {{WECA: A wordnet-encoded collocation-attention network for homographic pun recognition}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079828394&partnerID=40&md5=f25ec9f67dd16896ccbe3b30480dba90},
year = {2020}
}
@inproceedings{Belth20201115,
abstract = {Knowledge graphs (KGs) store highly heterogeneous information about the world in the structure of a graph, and are useful for tasks such as question answering and reasoning. However, they often contain errors and are missing information. Vibrant research in KG refinement has worked to resolve these issues, tailoring techniques to either detect specific types of errors or complete a KG. In this work, we introduce a unified solution to KG characterization by formulating the problem as unsupervised KG summarization with a set of inductive, soft rules, which describe what is normal in a KG, and thus can be used to identify what is abnormal, whether it be strange or missing. Unlike first-order logic rules, our rules are labeled, rooted graphs, i.e., patterns that describe the expected neighborhood around a (seen or unseen) node, based on its type, and information in the KG. Stepping away from the traditional support/confidence-based rule mining techniques, we propose KGist, Knowledge Graph Inductive SummarizaTion, which learns a summary of inductive rules that best compress the KG according to the Minimum Description Length principle - a formulation that we are the first to use in the context of KG rule mining. We apply our rules to three large KGs (NELL, DBpedia, and Yago), and tasks such as compression, various types of error detection, and identification of incomplete information. We show that KGist outperforms task-specific, supervised and unsupervised baselines in error detection and incompleteness identification, (identifying the location of up to 93% of missing entities - over 10% more than baselines), while also being efficient for large knowledge graphs. {\textcopyright} 2020 ACM.},
annote = {cited By 1; Conference of 29th International World Wide Web Conference, WWW 2020 ; Conference Date: 20 April 2020 Through 24 April 2020; Conference Code:160505},
author = {Belth, C and Zheng, X and Vreeken, J and Koutra, D},
booktitle = {The Web Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020},
doi = {10.1145/3366423.3380189},
isbn = {9781450370233},
keywords = {Error detection; Formal logic; World Wide Web,First order logic; Heterogeneous information; Inc,Natural language processing systems},
pages = {1115--1126},
publisher = {Association for Computing Machinery, Inc},
title = {{What is Normal, What is Strange, and What is Missing in a Knowledge Graph: Unified Characterization via Inductive Summarization}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086577940&doi=10.1145%2F3366423.3380189&partnerID=40&md5=d4eefadd00669156997ad77649a0ad46},
year = {2020}
}
@article{Cifariello20191,
abstract = {We present WISER, a new semantic search engine for expert finding in academia. Our system is unsupervised and it jointly combines classical language modeling techniques, based on text evidences, with the Wikipedia Knowledge Graph, via entity linking. WISER indexes each academic author through a novel profiling technique which models her expertise with a small, labeled and weighted graph drawn from Wikipedia. Nodes in this graph are the Wikipedia entities mentioned in the author's publications, whereas the weighted edges express the semantic relatedness among these entities computed via textual and graph-based relatedness functions. Every node is also labeled with a relevance score which models the pertinence of the corresponding entity to author's expertise, and is computed by means of a proper random-walk calculation over that graph; and with a latent vector representation which is learned via entity and other kinds of structural embeddings derived from Wikipedia. At query time, experts are retrieved by combining classic document-centric approaches, which exploit the occurrences of query terms in the author's documents, with a novel set of profile-centric scoring strategies, which compute the semantic relatedness between the author's expertise and the query topic via the above graph-based profiles. The effectiveness of our system is established over a large-scale experimental test on a standard dataset for this task. We show that WISER achieves better performance than all the other competitors, thus proving the effectiveness of modeling author's profile via our “semantic” graph of entities. Finally, we comment on the use of WISER for indexing and profiling the whole research community within the University of Pisa, and its application to technology transfer in our University. {\textcopyright} 2018},
annote = {cited By 18},
author = {Cifariello, P and Ferragina, P and Ponza, M},
doi = {10.1016/j.is.2018.12.003},
issn = {03064379},
journal = {Information Systems},
keywords = {Entity linking; Expert finding; Expert profiling;,Graph theory,Graphic methods; Information retrieval; Modeling l},
pages = {1--16},
publisher = {Elsevier Ltd},
title = {{WISER: A semantic approach for expert finding in academia based on entity linking}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059823000&doi=10.1016%2Fj.is.2018.12.003&partnerID=40&md5=b714b3ee406f9447f801907083869434},
volume = {82},
year = {2019}
}
@inproceedings{Surkova2019,
abstract = {The paper considers two linguistic models, analyzed the possibility of their use for the text data classification as well as their associations in the integrated texts presentation. A cognitive approach for the text classification issues is presented. An algorithm to identify the words basic level using WordNet is considered. A model for text classification based on the pre-trained word embeddings is presented. The model consists of three layers: embedding layer Long-Short Term Memory (LSTM) layer, and softmax layer. The model was trained and evaluated on the 20 Newsgroups dataset. The classification quality was assessed by F-measure, precision and recall. The obtained results analysis is carried out. Both described models show good results, low scores for some texts are explained. The advantages and limitations of the linguistic models are shown. In future works the authors are going to combine proposed models and modify them. Thus, for model based on word embedding there are pretty vast opportunities for extension: from experimenting with different word embeddings and various distance metrics to more complicated architecture of layers and even promising state of the art artificial neural network models, activation functions and their modifications. In addition, there is research area of proper ensemble strategy selection. {\textcopyright} 2019 Association for Computing Machinery.},
annote = {cited By 0; Conference of 11th International Scientific and Theoretical Conference ""Communicative strategies of Information Society"", CSIS 2019 ; Conference Date: 25 October 2019 Through 26 October 2019; Conference Code:157195},
author = {Surkova, A and Skorynin, S and Chernobaev, I},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3373722.3373778},
editor = {{Nordmann A. Moccozet L.}, Volkova V Shipunova O},
isbn = {9781450376709},
keywords = {Artificial neural network models; Classification,Classification (of information),Data mining; Embeddings; Long short-term memory; M},
publisher = {Association for Computing Machinery},
title = {{Word embedding and cognitive linguistic models in text classification tasks}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079281324&doi=10.1145%2F3373722.3373778&partnerID=40&md5=8f66648f8b448dcfee7443fb078d83ac},
year = {2019}
}
@inproceedings{Alkhatlan201850,
abstract = {Word Sense Disambiguation (WSD) is a task which aims to identify the meaning of a word given its context. This problem has been investigated and analyzed in depth in English. However, work in Arabic has been limited despite the fact that there are half a billion native Arabic speakers. In this work, we present multiple approaches for the problem of WSD in Arabic utilizing recent developments and successes in learning word embeddings with approaches such as GloVe, and Word2vec. The primary shortcoming of word embeddings is the single vector representation of a word's meaning, although many words are polysemous. Our main contribution in this work is to computationally obtain an embedding for each sense, using an Arabic WordNet (AWN) to overcome the problem of WSD. We also compute word semantic similarity giving thought to multiple Arabic stemming algorithms. Finally, we make available a large pre-processed corpus that is ready to be used for further experiments and a WSD test data based on AWN,1 seeking to fill gaps in Arabic NLP (ANLP) compared to English. {\textcopyright} 2018 The Authors. Published by Elsevier B.V.},
annote = {cited By 4; Conference of 4th Arabic Computational Linguistics, ACLing 2018 ; Conference Date: 17 November 2018 Through 19 November 2018; Conference Code:147807},
author = {Alkhatlan, A and Kalita, J and Alhaddad, A},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2018.10.460},
editor = {{El-Beltagy S.R.}, Shaalan K},
issn = {18770509},
keywords = {Arabic nlp; Arabic stemming; Arabic wordnet; Sema,Computational linguistics; Embeddings; Neural netw,Natural language processing systems},
pages = {50--60},
publisher = {Elsevier B.V.},
title = {{Word Sense Disambiguation for Arabic Exploiting Arabic WordNet and Word Embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065723027&doi=10.1016%2Fj.procs.2018.10.460&partnerID=40&md5=b51f4286d58d9b6000f6690de006c562},
volume = {142},
year = {2018}
}
@article{Bartusiak2019141,
abstract = {The complex nature of big data resources requires new structuring methods, especially for textual content. WordNet is a good knowledge source for the comprehensive abstraction of natural language as it offers good implementation for many languages. Since WordNet embeds natural language in the form of a complex network, a transformation mechanism, WordNet2Vec, is proposed in this paper. This creates vectors for each word from WordNet. These vectors encapsulate a general position — the role of a given word related to all other words in the given natural language. Any list or set of such vectors contains knowledge about the context of its components within the whole language. This type of word representation can be easily applied to many analytic tasks such as classification or clustering. The usefulness of the WordNet2Vec method is demonstrated in sentiment analysis including the classification of an Amazon opinion text dataset with transfer learning. {\textcopyright} 2017 Elsevier B.V.},
annote = {cited By 4},
author = {Bartusiak, R and Augustyniak, {\L} and Kajdanowicz, T and Kazienko, P and Piasecki, M},
doi = {10.1016/j.neucom.2017.01.121},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Big data,Classification (of information); Complex networks;,Natural languages; Network transformation; Transf,algorithm; Article; artificial intelligence; arti},
pages = {141--150},
publisher = {Elsevier B.V.},
title = {{WordNet2Vec: Corpora agnostic word vectorization method}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031117090&doi=10.1016%2Fj.neucom.2017.01.121&partnerID=40&md5=39e9ddc87fc878151263f6cf5a2af97e},
volume = {326-327},
year = {2019}
}
@inproceedings{Li2015879,
abstract = {Given the difficulty of acquiring labeled examples for many fine-grained visual classes, there is an increasing interest in zero-shot image tagging, aiming to tag images with novel labels that have no training examples present. Using a semantic space trained by a neural language model, the current state-of-the-art embeds both images and labels into the space, wherein cross-media similarity is computed. However, for labels of relatively low occurrence, its similarity to images and other labels can be unreliable. This paper proposes Hierarchical Semantic Embedding (HierSE), a simple model that exploits the WordNet hierarchy to improve label embedding and consequently image embedding. Moreover, we identify two good tricks, namely training the neural language model using Flickr tags instead of web documents, and using partial match instead of full match for vectorizing a WordNet node. All this lets us outperform the state-ofthe-art. On a test set of over 1,500 visual object classes and 1.3 million images, the proposed model beats the current best results (18.3% versus 9.4% in hit@1). {\textcopyright} 2015 ACM.},
annote = {cited By 40; Conference of 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2015 ; Conference Date: 9 August 2015 Through 13 August 2015; Conference Code:117001},
author = {Li, X and Liao, S and Lan, W and Du, X and Yang, G},
booktitle = {SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/2766462.2767773},
isbn = {9781450336215},
keywords = {Computational linguistics; Information retrieval;,Image embedding; Image tagging; Partial matches;,Semantics},
pages = {879--882},
publisher = {Association for Computing Machinery, Inc},
title = {{Zero-shot image tagging by Hierarchical semantic embedding}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953712089&doi=10.1145%2F2766462.2767773&partnerID=40&md5=56529e0269ba3f7885dbf25928567707},
year = {2015}
}
@inproceedings{8578815,
abstract = {We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 $\sim$ 3% on some metrics to whopping 20% on a few).},
author = {Wang, X and Ye, Y and Gupta, A},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00717},
issn = {2575-7075},
keywords = {graph theory;image representation;learning (artifi},
month = {jun},
pages = {6857--6866},
title = {{Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs}},
year = {2018}
}
@inproceedings{Kumar20205670,
abstract = {Word Sense Disambiguation (WSD) is a longstanding but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance. {\textcopyright} 2019 Association for Computational Linguistics},
annote = {cited By 2; Conference of 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019 ; Conference Date: 28 July 2019 Through 2 August 2019; Conference Code:159206},
author = {Kumar, S and Jat, S and Saxena, K and Talukdar, P},
booktitle = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
isbn = {9781950737482},
keywords = {Computational linguistics; Embeddings; Signal enco,Knowledge graphs; Label space; Large corpora; NAt,Natural language processing systems},
pages = {5670--5681},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Zero-shot word sense disambiguation using sense definition embeddings}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075859601&partnerID=40&md5=df7fa442077c5b74e0de03860cb4219b},
year = {2020}
}
